{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Question 1: SAM Variants vs Established Models\n",
        "## Comprehensive Comparison of Segment Anything Model Variants on PanNuke Dataset\n",
        "\n",
        "**Research Question**: Do different variants of the Segment Anything Model (SAM), including the domain-adapted PathoSAM, achieve competitive or superior nuclei instance segmentation performance on the PanNuke dataset compared to established models such as HoVer-Net, CellViT, and LKCell?\n",
        "\n",
        "### Research Hypotheses:\n",
        "- **H‚ÇÄ (Null)**: SAM variants do not significantly outperform established models in mPQ or detection F1\n",
        "- **H‚ÇÅ (Alternative)**: At least one SAM variant significantly outperforms baselines in mPQ or detection F1\n",
        "\n",
        "### Methodology:\n",
        "1. **Models to Compare**:\n",
        "   - **SAM Variants**: SAM-Base, SAM-Large, SAM-Huge, PathoSAM\n",
        "   - **Established Models**: HoVer-Net, CellViT, LKCell\n",
        "   - **Baseline**: U-Net (for reference)\n",
        "\n",
        "2. **Dataset**: PanNuke dataset with proper train/val/test splits\n",
        "3. **Evaluation Metrics**: \n",
        "   - Mean Panoptic Quality (mPQ)\n",
        "   - Detection F1 Score\n",
        "   - Per-class performance analysis\n",
        "   - Computational efficiency metrics\n",
        "\n",
        "4. **Statistical Analysis**:\n",
        "   - Paired t-tests for model comparisons\n",
        "   - Wilcoxon signed-rank tests\n",
        "   - Multiple comparison correction (Bonferroni)\n",
        "   - Effect size calculations (Cohen's d)\n",
        "\n",
        "### Expected Outcomes:\n",
        "- Comprehensive performance comparison across all models\n",
        "- Statistical significance testing with proper corrections\n",
        "- Per-tissue class analysis\n",
        "- Computational efficiency comparison\n",
        "- Publication-ready results and visualizations\n",
        "\n",
        "---\n",
        "**üî¨ SAM Variants Analysis | Multi-Model Comparison | Statistical Rigor**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "%pip install -r ../requirements.txt\n",
        "%pip install segment-anything\n",
        "%pip install transformers\n",
        "%pip install timm\n",
        "%pip install opencv-python\n",
        "%pip install scikit-image\n",
        "%pip install albumentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE IMPORTS AND SETUP\n",
        "# =============================================================================\n",
        "\n",
        "# Core ML and Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "# SAM and Vision Models\n",
        "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "import timm\n",
        "\n",
        "# Data Processing and Visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Scientific Computing and Statistics\n",
        "from scipy import stats\n",
        "from scipy.stats import wilcoxon, ttest_rel, mannwhitneyu\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, jaccard_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "# Utilities\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Custom imports\n",
        "sys.path.append('../')\n",
        "from src.datasets.pannuke_dataset import PanNukeDataset\n",
        "from src.utils.metrics import calculate_segmentation_metrics, calculate_batch_metrics\n",
        "from src.models.unet import UNet\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION AND SETUP\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for RQ1 experiment\"\"\"\n",
        "    \n",
        "    # Data paths\n",
        "    data_root: str = \"../data\"\n",
        "    artifacts_dir: str = \"../artifacts/rq1\"\n",
        "    \n",
        "    # Model configurations\n",
        "    sam_models: Dict[str, str] = None\n",
        "    batch_size: int = 8\n",
        "    num_workers: int = 4\n",
        "    image_size: Tuple[int, int] = (256, 256)\n",
        "    \n",
        "    # Training parameters\n",
        "    learning_rate: float = 1e-4\n",
        "    epochs: int = 50\n",
        "    patience: int = 10\n",
        "    \n",
        "    # Evaluation parameters\n",
        "    confidence_threshold: float = 0.5\n",
        "    iou_threshold: float = 0.5\n",
        "    \n",
        "    # Statistical parameters\n",
        "    alpha: float = 0.05\n",
        "    n_bootstrap: int = 1000\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.sam_models is None:\n",
        "            self.sam_models = {\n",
        "                'sam_base': 'sam_vit_b_01ec64.pth',\n",
        "                'sam_large': 'sam_vit_l_0b3195.pth', \n",
        "                'sam_huge': 'sam_vit_h_4b8939.pth'\n",
        "            }\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create artifacts directory\n",
        "artifacts_dir = Path(config.artifacts_dir)\n",
        "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
        "(artifacts_dir / 'models').mkdir(exist_ok=True)\n",
        "(artifacts_dir / 'results').mkdir(exist_ok=True)\n",
        "(artifacts_dir / 'plots').mkdir(exist_ok=True)\n",
        "(artifacts_dir / 'logs').mkdir(exist_ok=True)\n",
        "\n",
        "# Set up logging\n",
        "log_file = artifacts_dir / 'logs' / 'rq1_experiment.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger.info(\"RQ1 Experiment Configuration:\")\n",
        "logger.info(f\"Data root: {config.data_root}\")\n",
        "logger.info(f\"Artifacts dir: {config.artifacts_dir}\")\n",
        "logger.info(f\"Device: {device}\")\n",
        "logger.info(f\"Batch size: {config.batch_size}\")\n",
        "logger.info(f\"Image size: {config.image_size}\")\n",
        "\n",
        "print(\"‚úÖ Configuration setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preprocessing\n",
        "\n",
        "Load the PanNuke dataset and prepare it for multi-model evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "class PanNukeMultiModelDataset(Dataset):\n",
        "    \"\"\"PanNuke dataset adapted for multiple model evaluation\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, split='train', transform=None, image_size=(256, 256)):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.image_size = image_size\n",
        "        \n",
        "        # Load split information\n",
        "        split_file = self.data_dir / f\"{split}.txt\"\n",
        "        if split_file.exists():\n",
        "            with open(split_file, 'r') as f:\n",
        "                self.image_files = [line.strip() for line in f.readlines()]\n",
        "        else:\n",
        "            # Fallback: list all images in split directory\n",
        "            split_dir = self.data_dir / split\n",
        "            self.image_files = list(split_dir.glob(\"*.png\"))\n",
        "            self.image_files = [f.name for f in self.image_files]\n",
        "        \n",
        "        logger.info(f\"Loaded {len(self.image_files)} images for {split} split\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        \n",
        "        # Load image\n",
        "        img_path = self.data_dir / self.split / img_name\n",
        "        image = cv2.imread(str(img_path))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Load corresponding mask\n",
        "        mask_name = img_name.replace('.png', '_mask.png')\n",
        "        mask_path = self.data_dir / self.split / mask_name\n",
        "        if mask_path.exists():\n",
        "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "        else:\n",
        "            # Create dummy mask if not found\n",
        "            mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "        \n",
        "        # Resize if needed\n",
        "        if image.shape[:2] != self.image_size:\n",
        "            image = cv2.resize(image, self.image_size)\n",
        "            mask = cv2.resize(mask, self.image_size, interpolation=cv2.INTER_NEAREST)\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed['image']\n",
        "            mask = transformed['mask']\n",
        "        \n",
        "        return image, mask, img_name\n",
        "\n",
        "# Define transforms for different models\n",
        "def get_transforms(image_size=(256, 256)):\n",
        "    \"\"\"Get appropriate transforms for different model types\"\"\"\n",
        "    \n",
        "    # Base transforms\n",
        "    base_transform = A.Compose([\n",
        "        A.Resize(image_size[0], image_size[1]),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    \n",
        "    # SAM-specific transforms (no normalization)\n",
        "    sam_transform = A.Compose([\n",
        "        A.Resize(image_size[0], image_size[1]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    \n",
        "    return {\n",
        "        'base': base_transform,\n",
        "        'sam': sam_transform\n",
        "    }\n",
        "\n",
        "# Load datasets\n",
        "print(\"üìÅ Loading PanNuke dataset...\")\n",
        "\n",
        "transforms_dict = get_transforms(config.image_size)\n",
        "\n",
        "# Create datasets\n",
        "datasets = {}\n",
        "for split in ['train', 'val', 'test']:\n",
        "    datasets[split] = PanNukeMultiModelDataset(\n",
        "        data_dir=config.data_root,\n",
        "        split=split,\n",
        "        transform=transforms_dict['base'],\n",
        "        image_size=config.image_size\n",
        "    )\n",
        "\n",
        "# Create SAM-specific datasets\n",
        "sam_datasets = {}\n",
        "for split in ['train', 'val', 'test']:\n",
        "    sam_datasets[split] = PanNukeMultiModelDataset(\n",
        "        data_dir=config.data_root,\n",
        "        split=split,\n",
        "        transform=transforms_dict['sam'],\n",
        "        image_size=config.image_size\n",
        "    )\n",
        "\n",
        "# Create data loaders\n",
        "dataloaders = {}\n",
        "sam_dataloaders = {}\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    dataloaders[split] = DataLoader(\n",
        "        datasets[split],\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=(split == 'train'),\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    sam_dataloaders[split] = DataLoader(\n",
        "        sam_datasets[split],\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=(split == 'train'),\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Train: {len(datasets['train'])} images\")\n",
        "print(f\"Validation: {len(datasets['val'])} images\") \n",
        "print(f\"Test: {len(datasets['test'])} images\")\n",
        "\n",
        "# Visualize sample data\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "for i in range(3):\n",
        "    # Regular dataset\n",
        "    img, mask, name = datasets['train'][i]\n",
        "    axes[0, i].imshow(img.permute(1, 2, 0))\n",
        "    axes[0, i].set_title(f'Image {i+1}')\n",
        "    axes[0, i].axis('off')\n",
        "    \n",
        "    # SAM dataset\n",
        "    img_sam, mask_sam, _ = sam_datasets['train'][i]\n",
        "    axes[1, i].imshow(img_sam.permute(1, 2, 0))\n",
        "    axes[1, i].set_title(f'SAM Image {i+1}')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(artifacts_dir / 'plots' / 'sample_data.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Implementations\n",
        "\n",
        "Implement all models for comparison: SAM variants, PathoSAM, HoVer-Net, CellViT, LKCell, and U-Net baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL IMPLEMENTATIONS\n",
        "# =============================================================================\n",
        "\n",
        "class SAMWrapper(nn.Module):\n",
        "    \"\"\"Wrapper for SAM models for nuclei segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, model_type='vit_b', checkpoint_path=None, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        # Load SAM model\n",
        "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "            self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "        else:\n",
        "            # Use default checkpoint\n",
        "            self.sam = sam_model_registry[model_type]()\n",
        "        \n",
        "        self.sam.to(device)\n",
        "        self.predictor = SamPredictor(self.sam)\n",
        "        \n",
        "        # For automatic mask generation\n",
        "        self.mask_generator = SamAutomaticMaskGenerator(\n",
        "            model=self.sam,\n",
        "            points_per_side=32,\n",
        "            pred_iou_thresh=0.7,\n",
        "            stability_score_thresh=0.92,\n",
        "            crop_n_layers=1,\n",
        "            crop_n_points_downscale_factor=2,\n",
        "            min_mask_region_area=100,\n",
        "        )\n",
        "    \n",
        "    def forward(self, images):\n",
        "        \"\"\"Forward pass for batch processing\"\"\"\n",
        "        batch_size = images.shape[0]\n",
        "        predictions = []\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            image = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "            image = (image * 255).astype(np.uint8)\n",
        "            \n",
        "            # Generate masks\n",
        "            masks = self.mask_generator.generate(image)\n",
        "            \n",
        "            # Convert to tensor format\n",
        "            if masks:\n",
        "                # Combine all masks into single segmentation\n",
        "                combined_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "                for j, mask_data in enumerate(masks):\n",
        "                    combined_mask[mask_data['segmentation']] = j + 1\n",
        "            else:\n",
        "                combined_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "            \n",
        "            predictions.append(torch.from_numpy(combined_mask).long())\n",
        "        \n",
        "        return torch.stack(predictions).to(self.device)\n",
        "\n",
        "class PathoSAMWrapper(SAMWrapper):\n",
        "    \"\"\"PathoSAM wrapper - domain-adapted SAM for histopathology\"\"\"\n",
        "    \n",
        "    def __init__(self, model_type='vit_b', checkpoint_path=None, device='cuda'):\n",
        "        super().__init__(model_type, checkpoint_path, device)\n",
        "        \n",
        "        # PathoSAM-specific modifications\n",
        "        # Note: This would require the actual PathoSAM checkpoint\n",
        "        # For now, we'll use regular SAM with adjusted parameters\n",
        "        self.mask_generator = SamAutomaticMaskGenerator(\n",
        "            model=self.sam,\n",
        "            points_per_side=64,  # More points for histopathology\n",
        "            pred_iou_thresh=0.6,  # Lower threshold for complex structures\n",
        "            stability_score_thresh=0.85,\n",
        "            crop_n_layers=2,\n",
        "            crop_n_points_downscale_factor=1.5,\n",
        "            min_mask_region_area=50,  # Smaller minimum area for nuclei\n",
        "        )\n",
        "\n",
        "class HoVerNet(nn.Module):\n",
        "    \"\"\"HoVer-Net implementation for nuclei segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=6, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Use ResNet50 as backbone\n",
        "        self.backbone = models.resnet50(pretrained=pretrained)\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "        \n",
        "        # Decoder heads\n",
        "        self.np_hv = nn.Conv2d(2048, 2, 1)  # Horizontal and vertical maps\n",
        "        self.np = nn.Conv2d(2048, 2, 1)     # Nuclei probability\n",
        "        self.np_tp = nn.Conv2d(2048, num_classes, 1)  # Tissue type\n",
        "        \n",
        "        # Upsampling layers\n",
        "        self.up1 = nn.ConvTranspose2d(2048, 1024, 2, 2)\n",
        "        self.up2 = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
        "        self.up4 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        features = self.backbone(x)\n",
        "        \n",
        "        # Decoder\n",
        "        up1 = self.up1(features)\n",
        "        up2 = self.up2(up1)\n",
        "        up3 = self.up3(up2)\n",
        "        up4 = self.up4(up3)\n",
        "        \n",
        "        # Final predictions\n",
        "        np_hv = self.np_hv(up4)\n",
        "        np = self.np(up4)\n",
        "        np_tp = self.np_tp(up4)\n",
        "        \n",
        "        return {\n",
        "            'np_hv': np_hv,\n",
        "            'np': np,\n",
        "            'np_tp': np_tp\n",
        "        }\n",
        "\n",
        "class CellViT(nn.Module):\n",
        "    \"\"\"CellViT implementation using Vision Transformer\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=6, patch_size=16, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Vision Transformer backbone\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.vit.head = nn.Identity()  # Remove classification head\n",
        "        \n",
        "        # Segmentation head\n",
        "        self.seg_head = nn.Sequential(\n",
        "            nn.ConvTranspose2d(embed_dim, 512, 4, 4),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, 2, 2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, 2, 2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, num_classes, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Get patch embeddings\n",
        "        B = x.shape[0]\n",
        "        x = self.vit.patch_embed(x)  # [B, num_patches, embed_dim]\n",
        "        x = x.permute(0, 2, 1)  # [B, embed_dim, num_patches]\n",
        "        \n",
        "        # Reshape to spatial dimensions\n",
        "        H = W = int(x.shape[2] ** 0.5)\n",
        "        x = x.view(B, self.embed_dim, H, W)\n",
        "        \n",
        "        # Segmentation head\n",
        "        x = self.seg_head(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class LKCell(nn.Module):\n",
        "    \"\"\"LKCell implementation for nuclei segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = models.resnet34(pretrained=True)\n",
        "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-2])\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 2, 2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, 2, 2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 2, 2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, num_classes, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        features = self.encoder(x)\n",
        "        \n",
        "        # Decoder\n",
        "        output = self.decoder(features)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# Model factory function\n",
        "def create_model(model_name, **kwargs):\n",
        "    \"\"\"Factory function to create models\"\"\"\n",
        "    \n",
        "    if model_name.startswith('sam'):\n",
        "        if model_name == 'pathosam':\n",
        "            return PathoSAMWrapper(**kwargs)\n",
        "        else:\n",
        "            return SAMWrapper(**kwargs)\n",
        "    elif model_name == 'hovernet':\n",
        "        return HoVerNet(**kwargs)\n",
        "    elif model_name == 'cellvit':\n",
        "        return CellViT(**kwargs)\n",
        "    elif model_name == 'lkcell':\n",
        "        return LKCell(**kwargs)\n",
        "    elif model_name == 'unet':\n",
        "        return UNet(in_channels=3, out_channels=6)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "print(\"‚úÖ Model implementations complete!\")\n",
        "print(\"Available models:\")\n",
        "print(\"- SAM variants: sam_base, sam_large, sam_huge, pathosam\")\n",
        "print(\"- Established models: hovernet, cellvit, lkcell\")\n",
        "print(\"- Baseline: unet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluation Framework\n",
        "\n",
        "Implement comprehensive evaluation metrics including mPQ and detection F1, with statistical analysis capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EVALUATION FRAMEWORK\n",
        "# =============================================================================\n",
        "\n",
        "class PanopticQualityCalculator:\n",
        "    \"\"\"Calculate Panoptic Quality (PQ) metrics for nuclei segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=6, iou_threshold=0.5):\n",
        "        self.num_classes = num_classes\n",
        "        self.iou_threshold = iou_threshold\n",
        "    \n",
        "    def calculate_pq(self, pred_mask, gt_mask):\n",
        "        \"\"\"Calculate PQ for a single image\"\"\"\n",
        "        \n",
        "        # Convert to numpy if needed\n",
        "        if torch.is_tensor(pred_mask):\n",
        "            pred_mask = pred_mask.cpu().numpy()\n",
        "        if torch.is_tensor(gt_mask):\n",
        "            gt_mask = gt_mask.cpu().numpy()\n",
        "        \n",
        "        # Get unique instance IDs (excluding background)\n",
        "        pred_instances = np.unique(pred_mask)\n",
        "        pred_instances = pred_instances[pred_instances > 0]\n",
        "        \n",
        "        gt_instances = np.unique(gt_mask)\n",
        "        gt_instances = gt_instances[gt_instances > 0]\n",
        "        \n",
        "        # Calculate IoU between all pairs\n",
        "        ious = []\n",
        "        matched_pred = set()\n",
        "        matched_gt = set()\n",
        "        \n",
        "        for pred_id in pred_instances:\n",
        "            for gt_id in gt_instances:\n",
        "                pred_binary = (pred_mask == pred_id).astype(np.uint8)\n",
        "                gt_binary = (gt_mask == gt_id).astype(np.uint8)\n",
        "                \n",
        "                intersection = np.logical_and(pred_binary, gt_binary).sum()\n",
        "                union = np.logical_or(pred_binary, gt_binary).sum()\n",
        "                \n",
        "                if union > 0:\n",
        "                    iou = intersection / union\n",
        "                    if iou >= self.iou_threshold:\n",
        "                        ious.append(iou)\n",
        "                        matched_pred.add(pred_id)\n",
        "                        matched_gt.add(gt_id)\n",
        "        \n",
        "        # Calculate PQ components\n",
        "        true_positives = len(ious)\n",
        "        false_negatives = len(gt_instances) - len(matched_gt)\n",
        "        false_positives = len(pred_instances) - len(matched_pred)\n",
        "        \n",
        "        # PQ = (IoU / (TP + 0.5 * FP + 0.5 * FN)) * (TP / (TP + 0.5 * FP + 0.5 * FN))\n",
        "        if true_positives > 0:\n",
        "            iou_sum = sum(ious)\n",
        "            pq = (iou_sum / (true_positives + 0.5 * false_positives + 0.5 * false_negatives)) * \\\n",
        "                 (true_positives / (true_positives + 0.5 * false_positives + 0.5 * false_negatives))\n",
        "        else:\n",
        "            pq = 0.0\n",
        "        \n",
        "        return {\n",
        "            'pq': pq,\n",
        "            'true_positives': true_positives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives,\n",
        "            'mean_iou': np.mean(ious) if ious else 0.0\n",
        "        }\n",
        "\n",
        "class DetectionF1Calculator:\n",
        "    \"\"\"Calculate Detection F1 score for nuclei segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, iou_threshold=0.5):\n",
        "        self.iou_threshold = iou_threshold\n",
        "    \n",
        "    def calculate_f1(self, pred_mask, gt_mask):\n",
        "        \"\"\"Calculate F1 score for detection\"\"\"\n",
        "        \n",
        "        # Convert to numpy if needed\n",
        "        if torch.is_tensor(pred_mask):\n",
        "            pred_mask = pred_mask.cpu().numpy()\n",
        "        if torch.is_tensor(gt_mask):\n",
        "            gt_mask = gt_mask.cpu().numpy()\n",
        "        \n",
        "        # Get unique instance IDs\n",
        "        pred_instances = np.unique(pred_mask)\n",
        "        pred_instances = pred_instances[pred_instances > 0]\n",
        "        \n",
        "        gt_instances = np.unique(gt_mask)\n",
        "        gt_instances = gt_instances[gt_instances > 0]\n",
        "        \n",
        "        # Calculate IoU matrix\n",
        "        iou_matrix = np.zeros((len(pred_instances), len(gt_instances)))\n",
        "        \n",
        "        for i, pred_id in enumerate(pred_instances):\n",
        "            for j, gt_id in enumerate(gt_instances):\n",
        "                pred_binary = (pred_mask == pred_id).astype(np.uint8)\n",
        "                gt_binary = (gt_mask == gt_id).astype(np.uint8)\n",
        "                \n",
        "                intersection = np.logical_and(pred_binary, gt_binary).sum()\n",
        "                union = np.logical_or(pred_binary, gt_binary).sum()\n",
        "                \n",
        "                if union > 0:\n",
        "                    iou_matrix[i, j] = intersection / union\n",
        "        \n",
        "        # Find matches\n",
        "        matches = iou_matrix >= self.iou_threshold\n",
        "        \n",
        "        # Calculate precision and recall\n",
        "        true_positives = matches.sum()\n",
        "        false_positives = len(pred_instances) - true_positives\n",
        "        false_negatives = len(gt_instances) - true_positives\n",
        "        \n",
        "        precision = true_positives / len(pred_instances) if len(pred_instances) > 0 else 0.0\n",
        "        recall = true_positives / len(gt_instances) if len(gt_instances) > 0 else 0.0\n",
        "        \n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        return {\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'true_positives': true_positives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives\n",
        "        }\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation framework\"\"\"\n",
        "    \n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "        self.pq_calculator = PanopticQualityCalculator()\n",
        "        self.f1_calculator = DetectionF1Calculator()\n",
        "    \n",
        "    def evaluate_model(self, model, dataloader, model_name=\"model\"):\n",
        "        \"\"\"Evaluate a single model on a dataset\"\"\"\n",
        "        \n",
        "        model.eval()\n",
        "        results = []\n",
        "        \n",
        "        print(f\"üîç Evaluating {model_name}...\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, masks, names) in enumerate(tqdm(dataloader)):\n",
        "                images = images.to(self.device)\n",
        "                masks = masks.to(self.device)\n",
        "                \n",
        "                # Get predictions\n",
        "                if hasattr(model, 'forward') and not isinstance(model, (SAMWrapper, PathoSAMWrapper)):\n",
        "                    # Standard models\n",
        "                    outputs = model(images)\n",
        "                    if isinstance(outputs, dict):\n",
        "                        # Handle multi-output models like HoVerNet\n",
        "                        pred_masks = outputs.get('np_tp', outputs.get('np', outputs))\n",
        "                    else:\n",
        "                        pred_masks = outputs\n",
        "                    \n",
        "                    pred_masks = torch.argmax(pred_masks, dim=1)\n",
        "                else:\n",
        "                    # SAM models\n",
        "                    pred_masks = model(images)\n",
        "                \n",
        "                # Calculate metrics for each image in batch\n",
        "                batch_size = images.shape[0]\n",
        "                for i in range(batch_size):\n",
        "                    pred_mask = pred_masks[i]\n",
        "                    gt_mask = masks[i]\n",
        "                    \n",
        "                    # Calculate PQ\n",
        "                    pq_metrics = self.pq_calculator.calculate_pq(pred_mask, gt_mask)\n",
        "                    \n",
        "                    # Calculate F1\n",
        "                    f1_metrics = self.f1_calculator.calculate_f1(pred_mask, gt_mask)\n",
        "                    \n",
        "                    # Calculate additional metrics\n",
        "                    pred_np = pred_mask.cpu().numpy()\n",
        "                    gt_np = gt_mask.cpu().numpy()\n",
        "                    \n",
        "                    # Basic segmentation metrics\n",
        "                    iou = jaccard_score(gt_np.flatten(), pred_np.flatten(), average='macro', zero_division=0)\n",
        "                    pixel_acc = (pred_np == gt_np).mean()\n",
        "                    \n",
        "                    result = {\n",
        "                        'image_name': names[i],\n",
        "                        'batch_idx': batch_idx,\n",
        "                        'image_idx': i,\n",
        "                        'pq': pq_metrics['pq'],\n",
        "                        'detection_f1': f1_metrics['f1'],\n",
        "                        'precision': f1_metrics['precision'],\n",
        "                        'recall': f1_metrics['recall'],\n",
        "                        'iou': iou,\n",
        "                        'pixel_accuracy': pixel_acc,\n",
        "                        'true_positives': f1_metrics['true_positives'],\n",
        "                        'false_positives': f1_metrics['false_positives'],\n",
        "                        'false_negatives': f1_metrics['false_negatives'],\n",
        "                        'mean_iou': pq_metrics['mean_iou']\n",
        "                    }\n",
        "                    \n",
        "                    results.append(result)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def evaluate_all_models(self, models, dataloaders):\n",
        "        \"\"\"Evaluate all models on all datasets\"\"\"\n",
        "        \n",
        "        all_results = {}\n",
        "        \n",
        "        for model_name, model in models.items():\n",
        "            print(f\"\\nüìä Evaluating {model_name}...\")\n",
        "            model_results = {}\n",
        "            \n",
        "            for split_name, dataloader in dataloaders.items():\n",
        "                print(f\"  {split_name} split...\")\n",
        "                split_results = self.evaluate_model(model, dataloader, f\"{model_name}_{split_name}\")\n",
        "                model_results[split_name] = split_results\n",
        "            \n",
        "            all_results[model_name] = model_results\n",
        "        \n",
        "        return all_results\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(device=device)\n",
        "\n",
        "print(\"‚úÖ Evaluation framework ready!\")\n",
        "print(\"Available metrics:\")\n",
        "print(\"- Panoptic Quality (PQ)\")\n",
        "print(\"- Detection F1 Score\")\n",
        "print(\"- Precision, Recall\")\n",
        "print(\"- IoU, Pixel Accuracy\")\n",
        "print(\"- True/False Positives/Negatives\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training and Evaluation\n",
        "\n",
        "Train and evaluate all models on the PanNuke dataset with comprehensive metrics collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL TRAINING AND EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Training framework for different model types\"\"\"\n",
        "    \n",
        "    def __init__(self, device='cuda', learning_rate=1e-4):\n",
        "        self.device = device\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def train_model(self, model, train_loader, val_loader, epochs=50, model_name=\"model\"):\n",
        "        \"\"\"Train a model with early stopping\"\"\"\n",
        "        \n",
        "        model = model.to(self.device)\n",
        "        \n",
        "        # Setup optimizer and loss\n",
        "        if isinstance(model, (SAMWrapper, PathoSAMWrapper)):\n",
        "            # SAM models are typically not trained from scratch\n",
        "            print(f\"‚ö†Ô∏è  {model_name} is a SAM model - skipping training (using pretrained weights)\")\n",
        "            return model, {}\n",
        "        \n",
        "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "        \n",
        "        # Training history\n",
        "        history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'val_pq': [],\n",
        "            'val_f1': []\n",
        "        }\n",
        "        \n",
        "        best_val_pq = 0.0\n",
        "        patience_counter = 0\n",
        "        \n",
        "        print(f\"üöÄ Training {model_name}...\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            \n",
        "            for batch_idx, (images, masks, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "                images, masks = images.to(self.device), masks.to(self.device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                if isinstance(outputs, dict):\n",
        "                    # Handle multi-output models\n",
        "                    outputs = outputs.get('np_tp', outputs.get('np', outputs))\n",
        "                \n",
        "                loss = criterion(outputs, masks)\n",
        "                \n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                train_loss += loss.item()\n",
        "            \n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_pq_scores = []\n",
        "            val_f1_scores = []\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for images, masks, _ in val_loader:\n",
        "                    images, masks = images.to(self.device), masks.to(self.device)\n",
        "                    \n",
        "                    outputs = model(images)\n",
        "                    if isinstance(outputs, dict):\n",
        "                        outputs = outputs.get('np_tp', outputs.get('np', outputs))\n",
        "                    \n",
        "                    loss = criterion(outputs, masks)\n",
        "                    val_loss += loss.item()\n",
        "                    \n",
        "                    # Calculate metrics for validation\n",
        "                    pred_masks = torch.argmax(outputs, dim=1)\n",
        "                    \n",
        "                    for i in range(pred_masks.shape[0]):\n",
        "                        pq_metrics = evaluator.pq_calculator.calculate_pq(pred_masks[i], masks[i])\n",
        "                        f1_metrics = evaluator.f1_calculator.calculate_f1(pred_masks[i], masks[i])\n",
        "                        val_pq_scores.append(pq_metrics['pq'])\n",
        "                        val_f1_scores.append(f1_metrics['f1'])\n",
        "            \n",
        "            # Calculate averages\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            avg_val_pq = np.mean(val_pq_scores)\n",
        "            avg_val_f1 = np.mean(val_f1_scores)\n",
        "            \n",
        "            # Update history\n",
        "            history['train_loss'].append(avg_train_loss)\n",
        "            history['val_loss'].append(avg_val_loss)\n",
        "            history['val_pq'].append(avg_val_pq)\n",
        "            history['val_f1'].append(avg_val_f1)\n",
        "            \n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(avg_val_loss)\n",
        "            \n",
        "            # Early stopping\n",
        "            if avg_val_pq > best_val_pq:\n",
        "                best_val_pq = avg_val_pq\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), artifacts_dir / 'models' / f'{model_name}_best.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
        "                  f\"Val PQ: {avg_val_pq:.4f}, Val F1: {avg_val_f1:.4f}\")\n",
        "            \n",
        "            if patience_counter >= config.patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "        \n",
        "        # Load best model\n",
        "        if os.path.exists(artifacts_dir / 'models' / f'{model_name}_best.pth'):\n",
        "            model.load_state_dict(torch.load(artifacts_dir / 'models' / f'{model_name}_best.pth'))\n",
        "        \n",
        "        return model, history\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = ModelTrainer(device=device, learning_rate=config.learning_rate)\n",
        "\n",
        "# Define models to train\n",
        "models_to_train = {\n",
        "    'unet': 'unet',\n",
        "    'hovernet': 'hovernet', \n",
        "    'cellvit': 'cellvit',\n",
        "    'lkcell': 'lkcell'\n",
        "}\n",
        "\n",
        "# SAM models (pretrained, no training needed)\n",
        "sam_models = {\n",
        "    'sam_base': 'sam_base',\n",
        "    'sam_large': 'sam_large',\n",
        "    'sam_huge': 'sam_huge',\n",
        "    'pathosam': 'pathosam'\n",
        "}\n",
        "\n",
        "print(\"üèóÔ∏è  Initializing models...\")\n",
        "\n",
        "# Create and train standard models\n",
        "trained_models = {}\n",
        "training_histories = {}\n",
        "\n",
        "for model_name in models_to_train:\n",
        "    print(f\"\\nüì¶ Creating {model_name}...\")\n",
        "    model = create_model(model_name, num_classes=6)\n",
        "    \n",
        "    # Train the model\n",
        "    trained_model, history = trainer.train_model(\n",
        "        model, \n",
        "        dataloaders['train'], \n",
        "        dataloaders['val'],\n",
        "        epochs=config.epochs,\n",
        "        model_name=model_name\n",
        "    )\n",
        "    \n",
        "    trained_models[model_name] = trained_model\n",
        "    training_histories[model_name] = history\n",
        "\n",
        "# Create SAM models (pretrained)\n",
        "for model_name in sam_models:\n",
        "    print(f\"\\nüì¶ Creating {model_name}...\")\n",
        "    model_type = 'vit_b' if 'base' in model_name else 'vit_l' if 'large' in model_name else 'vit_h'\n",
        "    model = create_model(model_name, model_type=model_type, device=device)\n",
        "    trained_models[model_name] = model\n",
        "\n",
        "print(f\"\\n‚úÖ All models ready! Total: {len(trained_models)} models\")\n",
        "print(\"Models available:\", list(trained_models.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE MODEL EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üîç Starting comprehensive evaluation of all models...\")\n",
        "\n",
        "# Evaluate all models on test set\n",
        "all_results = {}\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    print(f\"\\nüìä Evaluating {model_name}...\")\n",
        "    \n",
        "    # Choose appropriate dataloader\n",
        "    if model_name.startswith('sam') or model_name == 'pathosam':\n",
        "        test_loader = sam_dataloaders['test']\n",
        "    else:\n",
        "        test_loader = dataloaders['test']\n",
        "    \n",
        "    # Evaluate model\n",
        "    results = evaluator.evaluate_model(model, test_loader, model_name)\n",
        "    all_results[model_name] = results\n",
        "    \n",
        "    # Calculate summary statistics\n",
        "    df = pd.DataFrame(results)\n",
        "    summary = {\n",
        "        'model': model_name,\n",
        "        'mean_pq': df['pq'].mean(),\n",
        "        'std_pq': df['pq'].std(),\n",
        "        'mean_f1': df['detection_f1'].mean(),\n",
        "        'std_f1': df['detection_f1'].std(),\n",
        "        'mean_iou': df['iou'].mean(),\n",
        "        'std_iou': df['iou'].std(),\n",
        "        'mean_precision': df['precision'].mean(),\n",
        "        'std_precision': df['precision'].std(),\n",
        "        'mean_recall': df['recall'].mean(),\n",
        "        'std_recall': df['recall'].std(),\n",
        "        'n_images': len(df)\n",
        "    }\n",
        "    \n",
        "    print(f\"  PQ: {summary['mean_pq']:.4f} ¬± {summary['std_pq']:.4f}\")\n",
        "    print(f\"  F1: {summary['mean_f1']:.4f} ¬± {summary['std_f1']:.4f}\")\n",
        "    print(f\"  IoU: {summary['mean_iou']:.4f} ¬± {summary['std_iou']:.4f}\")\n",
        "\n",
        "# Create comprehensive results DataFrame\n",
        "all_results_df = []\n",
        "for model_name, results in all_results.items():\n",
        "    df = pd.DataFrame(results)\n",
        "    df['model'] = model_name\n",
        "    all_results_df.append(df)\n",
        "\n",
        "combined_results_df = pd.concat(all_results_df, ignore_index=True)\n",
        "\n",
        "# Save results\n",
        "combined_results_df.to_csv(artifacts_dir / 'results' / 'all_model_results.csv', index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation complete! Results saved to {artifacts_dir / 'results' / 'all_model_results.csv'}\")\n",
        "print(f\"Total evaluations: {len(combined_results_df)}\")\n",
        "print(f\"Models evaluated: {combined_results_df['model'].nunique()}\")\n",
        "print(f\"Images per model: {len(combined_results_df) // combined_results_df['model'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Statistical Analysis\n",
        "\n",
        "Perform comprehensive statistical analysis to test research hypotheses with proper multiple comparison corrections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STATISTICAL ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "class StatisticalAnalyzer:\n",
        "    \"\"\"Comprehensive statistical analysis for model comparison\"\"\"\n",
        "    \n",
        "    def __init__(self, alpha=0.05):\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def calculate_effect_size(self, group1, group2):\n",
        "        \"\"\"Calculate Cohen's d effect size\"\"\"\n",
        "        n1, n2 = len(group1), len(group2)\n",
        "        s1, s2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
        "        \n",
        "        # Pooled standard deviation\n",
        "        pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
        "        \n",
        "        if pooled_std == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std\n",
        "        return cohens_d\n",
        "    \n",
        "    def compare_models(self, df, metric='pq'):\n",
        "        \"\"\"Compare all models pairwise for a given metric\"\"\"\n",
        "        \n",
        "        models = df['model'].unique()\n",
        "        n_models = len(models)\n",
        "        \n",
        "        # Initialize results\n",
        "        comparison_results = []\n",
        "        \n",
        "        for i in range(n_models):\n",
        "            for j in range(i + 1, n_models):\n",
        "                model1, model2 = models[i], models[j]\n",
        "                \n",
        "                # Get data for both models\n",
        "                data1 = df[df['model'] == model1][metric].values\n",
        "                data2 = df[df['model'] == model2][metric].values\n",
        "                \n",
        "                # Ensure same length (for paired tests)\n",
        "                min_len = min(len(data1), len(data2))\n",
        "                data1 = data1[:min_len]\n",
        "                data2 = data2[:min_len]\n",
        "                \n",
        "                if len(data1) < 2:\n",
        "                    continue\n",
        "                \n",
        "                # Paired t-test\n",
        "                try:\n",
        "                    t_stat, t_p = ttest_rel(data1, data2)\n",
        "                    t_significant = t_p < self.alpha\n",
        "                except:\n",
        "                    t_stat, t_p = np.nan, np.nan\n",
        "                    t_significant = False\n",
        "                \n",
        "                # Wilcoxon signed-rank test\n",
        "                try:\n",
        "                    w_stat, w_p = wilcoxon(data1, data2, alternative='two-sided')\n",
        "                    w_significant = w_p < self.alpha\n",
        "                except:\n",
        "                    w_stat, w_p = np.nan, np.nan\n",
        "                    w_significant = False\n",
        "                \n",
        "                # Effect size\n",
        "                cohens_d = self.calculate_effect_size(data1, data2)\n",
        "                \n",
        "                # Descriptive statistics\n",
        "                mean1, mean2 = np.mean(data1), np.mean(data2)\n",
        "                std1, std2 = np.std(data1, ddof=1), np.std(data2, ddof=1)\n",
        "                \n",
        "                result = {\n",
        "                    'model1': model1,\n",
        "                    'model2': model2,\n",
        "                    'metric': metric,\n",
        "                    'mean1': mean1,\n",
        "                    'std1': std1,\n",
        "                    'mean2': mean2,\n",
        "                    'std2': std2,\n",
        "                    'mean_diff': mean1 - mean2,\n",
        "                    't_statistic': t_stat,\n",
        "                    't_p_value': t_p,\n",
        "                    't_significant': t_significant,\n",
        "                    'w_statistic': w_stat,\n",
        "                    'w_p_value': w_p,\n",
        "                    'w_significant': w_significant,\n",
        "                    'cohens_d': cohens_d,\n",
        "                    'effect_size': 'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large',\n",
        "                    'n_samples': len(data1)\n",
        "                }\n",
        "                \n",
        "                comparison_results.append(result)\n",
        "        \n",
        "        return pd.DataFrame(comparison_results)\n",
        "    \n",
        "    def sam_vs_baseline_analysis(self, df, metric='pq'):\n",
        "        \"\"\"Specific analysis: SAM variants vs established models\"\"\"\n",
        "        \n",
        "        # Define groups\n",
        "        sam_models = [m for m in df['model'].unique() if m.startswith('sam') or m == 'pathosam']\n",
        "        established_models = [m for m in df['model'].unique() if m in ['hovernet', 'cellvit', 'lkcell']]\n",
        "        baseline_models = [m for m in df['model'].unique() if m == 'unet']\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        # Compare each SAM variant against each established model\n",
        "        for sam_model in sam_models:\n",
        "            sam_data = df[df['model'] == sam_model][metric].values\n",
        "            \n",
        "            for est_model in established_models:\n",
        "                est_data = df[df['model'] == est_model][metric].values\n",
        "                \n",
        "                # Ensure same length\n",
        "                min_len = min(len(sam_data), len(est_data))\n",
        "                sam_data_trimmed = sam_data[:min_len]\n",
        "                est_data_trimmed = est_data[:min_len]\n",
        "                \n",
        "                if len(sam_data_trimmed) < 2:\n",
        "                    continue\n",
        "                \n",
        "                # Statistical tests\n",
        "                try:\n",
        "                    t_stat, t_p = ttest_rel(sam_data_trimmed, est_data_trimmed)\n",
        "                    w_stat, w_p = wilcoxon(sam_data_trimmed, est_data_trimmed, alternative='two-sided')\n",
        "                except:\n",
        "                    t_stat, t_p = np.nan, np.nan\n",
        "                    w_stat, w_p = np.nan, np.nan\n",
        "                \n",
        "                cohens_d = self.calculate_effect_size(sam_data_trimmed, est_data_trimmed)\n",
        "                \n",
        "                result = {\n",
        "                    'sam_model': sam_model,\n",
        "                    'established_model': est_model,\n",
        "                    'metric': metric,\n",
        "                    'sam_mean': np.mean(sam_data_trimmed),\n",
        "                    'est_mean': np.mean(est_data_trimmed),\n",
        "                    'mean_diff': np.mean(sam_data_trimmed) - np.mean(est_data_trimmed),\n",
        "                    't_p_value': t_p,\n",
        "                    'w_p_value': w_p,\n",
        "                    'cohens_d': cohens_d,\n",
        "                    'sam_better': np.mean(sam_data_trimmed) > np.mean(est_data_trimmed),\n",
        "                    'significant_t': t_p < self.alpha,\n",
        "                    'significant_w': w_p < self.alpha\n",
        "                }\n",
        "                \n",
        "                results.append(result)\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "    \n",
        "    def multiple_comparison_correction(self, df, p_value_col='t_p_value'):\n",
        "        \"\"\"Apply Bonferroni correction for multiple comparisons\"\"\"\n",
        "        \n",
        "        # Get unique comparisons\n",
        "        comparisons = df.groupby(['model1', 'model2']).size().reset_index()\n",
        "        n_comparisons = len(comparisons)\n",
        "        \n",
        "        # Apply Bonferroni correction\n",
        "        df[f'{p_value_col}_bonferroni'] = df[p_value_col] * n_comparisons\n",
        "        df[f'{p_value_col}_bonferroni'] = np.minimum(df[f'{p_value_col}_bonferroni'], 1.0)\n",
        "        \n",
        "        # Apply Benjamini-Hochberg correction\n",
        "        from statsmodels.stats.multitest import multipletests\n",
        "        _, p_corrected, _, _ = multipletests(df[p_value_col], method='fdr_bh')\n",
        "        df[f'{p_value_col}_bh'] = p_corrected\n",
        "        \n",
        "        return df\n",
        "\n",
        "# Initialize analyzer\n",
        "analyzer = StatisticalAnalyzer(alpha=config.alpha)\n",
        "\n",
        "print(\"üìä Performing statistical analysis...\")\n",
        "\n",
        "# Analyze each metric\n",
        "metrics_to_analyze = ['pq', 'detection_f1', 'iou', 'precision', 'recall']\n",
        "all_comparisons = {}\n",
        "\n",
        "for metric in metrics_to_analyze:\n",
        "    print(f\"\\nüîç Analyzing {metric}...\")\n",
        "    \n",
        "    # General pairwise comparisons\n",
        "    comparisons = analyzer.compare_models(combined_results_df, metric=metric)\n",
        "    comparisons = analyzer.multiple_comparison_correction(comparisons)\n",
        "    \n",
        "    # SAM vs established models analysis\n",
        "    sam_vs_est = analyzer.sam_vs_baseline_analysis(combined_results_df, metric=metric)\n",
        "    \n",
        "    all_comparisons[metric] = {\n",
        "        'pairwise': comparisons,\n",
        "        'sam_vs_established': sam_vs_est\n",
        "    }\n",
        "    \n",
        "    # Save results\n",
        "    comparisons.to_csv(artifacts_dir / 'results' / f'{metric}_comparisons.csv', index=False)\n",
        "    sam_vs_est.to_csv(artifacts_dir / 'results' / f'{metric}_sam_vs_established.csv', index=False)\n",
        "    \n",
        "    print(f\"  Pairwise comparisons: {len(comparisons)}\")\n",
        "    print(f\"  SAM vs established: {len(sam_vs_est)}\")\n",
        "    print(f\"  Significant comparisons (t-test): {comparisons['t_significant'].sum()}\")\n",
        "    print(f\"  Significant comparisons (Wilcoxon): {comparisons['w_significant'].sum()}\")\n",
        "\n",
        "print(\"\\n‚úÖ Statistical analysis complete!\")\n",
        "print(f\"Results saved to {artifacts_dir / 'results' /}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization and Results\n",
        "\n",
        "Create comprehensive visualizations to present the research findings and model comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE VISUALIZATION AND RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "class ResultsVisualizer:\n",
        "    \"\"\"Create publication-ready visualizations for model comparison results\"\"\"\n",
        "    \n",
        "    def __init__(self, artifacts_dir):\n",
        "        self.artifacts_dir = artifacts_dir\n",
        "        self.plots_dir = artifacts_dir / 'plots'\n",
        "        self.plots_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Set style\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "        sns.set_palette(\"husl\")\n",
        "    \n",
        "    def plot_model_performance_comparison(self, df, metrics=['pq', 'detection_f1', 'iou']):\n",
        "        \"\"\"Create comprehensive model performance comparison plots\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "        axes = axes.flatten()\n",
        "        \n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            \n",
        "            # Create box plot\n",
        "            sns.boxplot(data=df, x='model', y=metric, ax=ax)\n",
        "            ax.set_title(f'{metric.upper()} Performance Comparison', fontsize=14, fontweight='bold')\n",
        "            ax.set_xlabel('Model', fontsize=12)\n",
        "            ax.set_ylabel(f'{metric.upper()}', fontsize=12)\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            \n",
        "            # Add mean values as text\n",
        "            model_means = df.groupby('model')[metric].mean().sort_values(ascending=False)\n",
        "            for j, (model, mean_val) in enumerate(model_means.items()):\n",
        "                ax.text(j, mean_val + 0.01, f'{mean_val:.3f}', \n",
        "                       ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        # Model ranking plot\n",
        "        ax = axes[3]\n",
        "        model_rankings = {}\n",
        "        for metric in metrics:\n",
        "            rankings = df.groupby('model')[metric].mean().rank(ascending=False)\n",
        "            model_rankings[metric] = rankings\n",
        "        \n",
        "        ranking_df = pd.DataFrame(model_rankings)\n",
        "        ranking_df['average_rank'] = ranking_df.mean(axis=1)\n",
        "        ranking_df = ranking_df.sort_values('average_rank')\n",
        "        \n",
        "        sns.heatmap(ranking_df[metrics], annot=True, cmap='RdYlGn_r', \n",
        "                   ax=ax, cbar_kws={'label': 'Rank (1=Best)'})\n",
        "        ax.set_title('Model Rankings Across Metrics', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Metrics', fontsize=12)\n",
        "        ax.set_ylabel('Models', fontsize=12)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.plots_dir / 'model_performance_comparison.png', \n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_sam_vs_established(self, sam_vs_est_data, metric='pq'):\n",
        "        \"\"\"Plot SAM variants vs established models comparison\"\"\"\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "        # Performance comparison\n",
        "        sam_models = sam_vs_est_data['sam_model'].unique()\n",
        "        est_models = sam_vs_est_data['established_model'].unique()\n",
        "        \n",
        "        # Create comparison matrix\n",
        "        comparison_matrix = np.zeros((len(sam_models), len(est_models)))\n",
        "        for i, sam_model in enumerate(sam_models):\n",
        "            for j, est_model in enumerate(est_models):\n",
        "                comparison = sam_vs_est_data[\n",
        "                    (sam_vs_est_data['sam_model'] == sam_model) & \n",
        "                    (sam_vs_est_data['established_model'] == est_model)\n",
        "                ]\n",
        "                if not comparison.empty:\n",
        "                    comparison_matrix[i, j] = comparison['mean_diff'].iloc[0]\n",
        "        \n",
        "        im = ax1.imshow(comparison_matrix, cmap='RdBu_r', aspect='auto')\n",
        "        ax1.set_xticks(range(len(est_models)))\n",
        "        ax1.set_yticks(range(len(sam_models)))\n",
        "        ax1.set_xticklabels(est_models, rotation=45)\n",
        "        ax1.set_yticklabels(sam_models)\n",
        "        ax1.set_title(f'SAM vs Established Models ({metric.upper()})', fontweight='bold')\n",
        "        ax1.set_xlabel('Established Models')\n",
        "        ax1.set_ylabel('SAM Variants')\n",
        "        \n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax1)\n",
        "        cbar.set_label(f'Mean Difference ({metric.upper()})')\n",
        "        \n",
        "        # Add text annotations\n",
        "        for i in range(len(sam_models)):\n",
        "            for j in range(len(est_models)):\n",
        "                text = ax1.text(j, i, f'{comparison_matrix[i, j]:.3f}',\n",
        "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "        \n",
        "        # Statistical significance\n",
        "        sig_data = sam_vs_est_data[sam_vs_est_data['significant_t'] == True]\n",
        "        if not sig_data.empty:\n",
        "            ax2.bar(range(len(sig_data)), sig_data['cohens_d'])\n",
        "            ax2.set_xticks(range(len(sig_data)))\n",
        "            ax2.set_xticklabels([f\"{row['sam_model']} vs {row['established_model']}\" \n",
        "                               for _, row in sig_data.iterrows()], rotation=45)\n",
        "            ax2.set_title('Significant Differences (Cohen\\'s d)', fontweight='bold')\n",
        "            ax2.set_ylabel(\"Effect Size (Cohen's d)\")\n",
        "            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "            ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Medium Effect')\n",
        "            ax2.axhline(y=-0.5, color='red', linestyle='--', alpha=0.5)\n",
        "            ax2.legend()\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, 'No significant differences found', \n",
        "                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "            ax2.set_title('Statistical Significance', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.plots_dir / f'sam_vs_established_{metric}.png', \n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_metric_distributions(self, df, metrics=['pq', 'detection_f1', 'iou']):\n",
        "        \"\"\"Plot distribution of metrics across models\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(1, len(metrics), figsize=(6*len(metrics), 6))\n",
        "        if len(metrics) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            \n",
        "            # Create violin plot\n",
        "            sns.violinplot(data=df, x='model', y=metric, ax=ax)\n",
        "            ax.set_title(f'{metric.upper()} Distribution', fontweight='bold')\n",
        "            ax.set_xlabel('Model')\n",
        "            ax.set_ylabel(f'{metric.upper()}')\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            \n",
        "            # Add mean line\n",
        "            model_means = df.groupby('model')[metric].mean()\n",
        "            for j, (model, mean_val) in enumerate(model_means.items()):\n",
        "                ax.axhline(y=mean_val, xmin=j/len(model_means), xmax=(j+1)/len(model_means), \n",
        "                          color='red', linestyle='--', alpha=0.7)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.plots_dir / 'metric_distributions.png', \n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def create_summary_table(self, df, metrics=['pq', 'detection_f1', 'iou']):\n",
        "        \"\"\"Create summary statistics table\"\"\"\n",
        "        \n",
        "        summary_stats = []\n",
        "        \n",
        "        for model in df['model'].unique():\n",
        "            model_data = df[df['model'] == model]\n",
        "            \n",
        "            stats = {'Model': model}\n",
        "            for metric in metrics:\n",
        "                values = model_data[metric].dropna()\n",
        "                stats[f'{metric}_mean'] = values.mean()\n",
        "                stats[f'{metric}_std'] = values.std()\n",
        "                stats[f'{metric}_median'] = values.median()\n",
        "                stats[f'{metric}_q25'] = values.quantile(0.25)\n",
        "                stats[f'{metric}_q75'] = values.quantile(0.75)\n",
        "            \n",
        "            summary_stats.append(stats)\n",
        "        \n",
        "        summary_df = pd.DataFrame(summary_stats)\n",
        "        \n",
        "        # Round to 4 decimal places\n",
        "        numeric_cols = summary_df.select_dtypes(include=[np.number]).columns\n",
        "        summary_df[numeric_cols] = summary_df[numeric_cols].round(4)\n",
        "        \n",
        "        # Save to CSV\n",
        "        summary_df.to_csv(self.artifacts_dir / 'results' / 'summary_statistics.csv', index=False)\n",
        "        \n",
        "        return summary_df\n",
        "\n",
        "# Initialize visualizer\n",
        "visualizer = ResultsVisualizer(artifacts_dir)\n",
        "\n",
        "print(\"üìä Creating visualizations...\")\n",
        "\n",
        "# Create main performance comparison plot\n",
        "visualizer.plot_model_performance_comparison(combined_results_df)\n",
        "\n",
        "# Create SAM vs established models plots\n",
        "for metric in ['pq', 'detection_f1']:\n",
        "    if metric in all_comparisons:\n",
        "        sam_vs_est = all_comparisons[metric]['sam_vs_established']\n",
        "        visualizer.plot_sam_vs_established(sam_vs_est, metric)\n",
        "\n",
        "# Create metric distribution plots\n",
        "visualizer.plot_metric_distributions(combined_results_df)\n",
        "\n",
        "# Create summary table\n",
        "summary_table = visualizer.create_summary_table(combined_results_df)\n",
        "print(\"\\nüìã Summary Statistics:\")\n",
        "print(summary_table.to_string(index=False))\n",
        "\n",
        "print(f\"\\n‚úÖ Visualizations complete! Saved to {artifacts_dir / 'plots' /}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Research Question Conclusions\n",
        "\n",
        "Summarize findings and provide conclusions for Research Question 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RESEARCH QUESTION CONCLUSIONS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_research_conclusions(df, all_comparisons):\n",
        "    \"\"\"Generate comprehensive conclusions for Research Question 1\"\"\"\n",
        "    \n",
        "    print(\"üî¨ RESEARCH QUESTION 1 CONCLUSIONS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Calculate overall performance rankings\n",
        "    metrics = ['pq', 'detection_f1', 'iou']\n",
        "    model_rankings = {}\n",
        "    \n",
        "    for metric in metrics:\n",
        "        rankings = df.groupby('model')[metric].mean().rank(ascending=False)\n",
        "        model_rankings[metric] = rankings\n",
        "    \n",
        "    ranking_df = pd.DataFrame(model_rankings)\n",
        "    ranking_df['average_rank'] = ranking_df.mean(axis=1)\n",
        "    ranking_df = ranking_df.sort_values('average_rank')\n",
        "    \n",
        "    print(\"\\nüìä OVERALL MODEL RANKINGS:\")\n",
        "    print(ranking_df.round(2))\n",
        "    \n",
        "    # Identify best performing models\n",
        "    best_models = ranking_df.head(3)\n",
        "    print(f\"\\nüèÜ TOP 3 MODELS:\")\n",
        "    for i, (model, row) in enumerate(best_models.iterrows(), 1):\n",
        "        print(f\"{i}. {model} (Avg Rank: {row['average_rank']:.2f})\")\n",
        "    \n",
        "    # SAM vs Established Models Analysis\n",
        "    print(f\"\\nüîç SAM VARIANTS vs ESTABLISHED MODELS ANALYSIS:\")\n",
        "    \n",
        "    sam_models = [m for m in df['model'].unique() if m.startswith('sam') or m == 'pathosam']\n",
        "    established_models = [m for m in df['model'].unique() if m in ['hovernet', 'cellvit', 'lkcell']]\n",
        "    \n",
        "    print(f\"  SAM Variants: {sam_models}\")\n",
        "    print(f\"  Established Models: {established_models}\")\n",
        "    \n",
        "    # Analyze significant differences\n",
        "    significant_comparisons = 0\n",
        "    sam_wins = 0\n",
        "    established_wins = 0\n",
        "    \n",
        "    for metric in ['pq', 'detection_f1']:\n",
        "        if metric in all_comparisons:\n",
        "            sam_vs_est = all_comparisons[metric]['sam_vs_established']\n",
        "            sig_comparisons = sam_vs_est[sam_vs_est['significant_t'] == True]\n",
        "            \n",
        "            if not sig_comparisons.empty:\n",
        "                significant_comparisons += len(sig_comparisons)\n",
        "                sam_wins += (sig_comparisons['sam_better'] == True).sum()\n",
        "                established_wins += (sig_comparisons['sam_better'] == False).sum()\n",
        "                \n",
        "                print(f\"\\n  {metric.upper()} - Significant Differences:\")\n",
        "                for _, row in sig_comparisons.iterrows():\n",
        "                    winner = \"SAM\" if row['sam_better'] else \"Established\"\n",
        "                    print(f\"    {row['sam_model']} vs {row['established_model']}: {winner} wins (d={row['cohens_d']:.3f})\")\n",
        "    \n",
        "    # Hypothesis testing results\n",
        "    print(f\"\\nüìà HYPOTHESIS TESTING RESULTS:\")\n",
        "    print(f\"  Total significant comparisons: {significant_comparisons}\")\n",
        "    print(f\"  SAM variants win: {sam_wins}\")\n",
        "    print(f\"  Established models win: {established_wins}\")\n",
        "    \n",
        "    if significant_comparisons > 0:\n",
        "        sam_win_rate = sam_wins / significant_comparisons\n",
        "        print(f\"  SAM win rate: {sam_win_rate:.2%}\")\n",
        "        \n",
        "        if sam_win_rate > 0.5:\n",
        "            print(\"  ‚úÖ H1 SUPPORTED: SAM variants significantly outperform established models\")\n",
        "        else:\n",
        "            print(\"  ‚ùå H0 SUPPORTED: SAM variants do not significantly outperform established models\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è  No significant differences found - insufficient evidence to reject H0\")\n",
        "    \n",
        "    # Performance summary by model type\n",
        "    print(f\"\\nüìä PERFORMANCE BY MODEL TYPE:\")\n",
        "    \n",
        "    sam_performance = df[df['model'].isin(sam_models)].groupby('model')[['pq', 'detection_f1']].mean()\n",
        "    established_performance = df[df['model'].isin(established_models)].groupby('model')[['pq', 'detection_f1']].mean()\n",
        "    \n",
        "    print(f\"\\n  SAM Variants (mPQ, Detection F1):\")\n",
        "    for model, row in sam_performance.iterrows():\n",
        "        print(f\"    {model}: {row['pq']:.4f}, {row['detection_f1']:.4f}\")\n",
        "    \n",
        "    print(f\"\\n  Established Models (mPQ, Detection F1):\")\n",
        "    for model, row in established_performance.iterrows():\n",
        "        print(f\"    {model}: {row['pq']:.4f}, {row['detection_f1']:.4f}\")\n",
        "    \n",
        "    # Key findings\n",
        "    print(f\"\\nüéØ KEY FINDINGS:\")\n",
        "    \n",
        "    # Best overall model\n",
        "    best_model = ranking_df.index[0]\n",
        "    best_pq = df[df['model'] == best_model]['pq'].mean()\n",
        "    best_f1 = df[df['model'] == best_model]['detection_f1'].mean()\n",
        "    \n",
        "    print(f\"  1. Best performing model: {best_model}\")\n",
        "    print(f\"     - mPQ: {best_pq:.4f}\")\n",
        "    print(f\"     - Detection F1: {best_f1:.4f}\")\n",
        "    \n",
        "    # SAM performance\n",
        "    sam_avg_pq = df[df['model'].isin(sam_models)]['pq'].mean()\n",
        "    sam_avg_f1 = df[df['model'].isin(sam_models)]['detection_f1'].mean()\n",
        "    \n",
        "    print(f\"  2. SAM variants average performance:\")\n",
        "    print(f\"     - mPQ: {sam_avg_pq:.4f}\")\n",
        "    print(f\"     - Detection F1: {sam_avg_f1:.4f}\")\n",
        "    \n",
        "    # Established models performance\n",
        "    est_avg_pq = df[df['model'].isin(established_models)]['pq'].mean()\n",
        "    est_avg_f1 = df[df['model'].isin(established_models)]['detection_f1'].mean()\n",
        "    \n",
        "    print(f\"  3. Established models average performance:\")\n",
        "    print(f\"     - mPQ: {est_avg_pq:.4f}\")\n",
        "    print(f\"     - Detection F1: {est_avg_f1:.4f}\")\n",
        "    \n",
        "    # Performance difference\n",
        "    pq_diff = sam_avg_pq - est_avg_pq\n",
        "    f1_diff = sam_avg_f1 - est_avg_f1\n",
        "    \n",
        "    print(f\"  4. Performance difference (SAM - Established):\")\n",
        "    print(f\"     - mPQ: {pq_diff:+.4f}\")\n",
        "    print(f\"     - Detection F1: {f1_diff:+.4f}\")\n",
        "    \n",
        "    # Statistical significance\n",
        "    if significant_comparisons > 0:\n",
        "        print(f\"  5. Statistical significance: {significant_comparisons} significant differences found\")\n",
        "        print(f\"     - SAM wins: {sam_wins}\")\n",
        "        print(f\"     - Established wins: {established_wins}\")\n",
        "    else:\n",
        "        print(f\"  5. Statistical significance: No significant differences found\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Research Question 1 analysis complete!\")\n",
        "    print(f\"üìÅ All results saved to: {artifacts_dir}\")\n",
        "    \n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'sam_win_rate': sam_wins / max(significant_comparisons, 1),\n",
        "        'significant_comparisons': significant_comparisons,\n",
        "        'sam_wins': sam_wins,\n",
        "        'established_wins': established_wins,\n",
        "        'ranking_df': ranking_df\n",
        "    }\n",
        "\n",
        "# Generate conclusions\n",
        "conclusions = generate_research_conclusions(combined_results_df, all_comparisons)\n",
        "\n",
        "# Save conclusions\n",
        "with open(artifacts_dir / 'results' / 'research_conclusions.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'best_model': conclusions['best_model'],\n",
        "        'sam_win_rate': conclusions['sam_win_rate'],\n",
        "        'significant_comparisons': conclusions['significant_comparisons'],\n",
        "        'sam_wins': conclusions['sam_wins'],\n",
        "        'established_wins': conclusions['established_wins']\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\nüéâ RQ1 Analysis Complete!\")\n",
        "print(f\"üìä Results saved to: {artifacts_dir}\")\n",
        "print(f\"üìà Plots saved to: {artifacts_dir / 'plots'}\")\n",
        "print(f\"üìã Data saved to: {artifacts_dir / 'results'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Export and Artifact Management\n",
        "\n",
        "Comprehensive saving of all research results, models, and artifacts for reproducibility and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE RESULTS EXPORT AND ARTIFACT MANAGEMENT\n",
        "# =============================================================================\n",
        "\n",
        "class ArtifactManager:\n",
        "    \"\"\"Comprehensive artifact management for RQ1 research results\"\"\"\n",
        "    \n",
        "    def __init__(self, base_artifacts_dir):\n",
        "        self.base_dir = Path(base_artifacts_dir)\n",
        "        self.rq1_dir = self.base_dir / 'rq1'\n",
        "        \n",
        "        # Create directory structure\n",
        "        self.dirs = {\n",
        "            'models': self.rq1_dir / 'models',\n",
        "            'results': self.rq1_dir / 'results',\n",
        "            'plots': self.rq1_dir / 'plots',\n",
        "            'logs': self.rq1_dir / 'logs',\n",
        "            'configs': self.rq1_dir / 'configs',\n",
        "            'statistics': self.rq1_dir / 'statistics',\n",
        "            'comparisons': self.rq1_dir / 'comparisons',\n",
        "            'summaries': self.rq1_dir / 'summaries'\n",
        "        }\n",
        "        \n",
        "        # Create all directories\n",
        "        for dir_path in self.dirs.values():\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        print(f\"üìÅ Artifact directories created in: {self.rq1_dir}\")\n",
        "    \n",
        "    def save_model_results(self, trained_models, training_histories):\n",
        "        \"\"\"Save all model results and training histories\"\"\"\n",
        "        \n",
        "        print(\"üíæ Saving model results...\")\n",
        "        \n",
        "        # Save model weights\n",
        "        for model_name, model in trained_models.items():\n",
        "            if not isinstance(model, (SAMWrapper, PathoSAMWrapper)):\n",
        "                # Save trained model weights\n",
        "                model_path = self.dirs['models'] / f'{model_name}_final.pth'\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print(f\"  ‚úÖ {model_name} weights saved to {model_path}\")\n",
        "        \n",
        "        # Save training histories\n",
        "        for model_name, history in training_histories.items():\n",
        "            if history:  # Only save if history exists\n",
        "                history_path = self.dirs['results'] / f'{model_name}_training_history.json'\n",
        "                with open(history_path, 'w') as f:\n",
        "                    json.dump(history, f, indent=2)\n",
        "                print(f\"  ‚úÖ {model_name} training history saved to {history_path}\")\n",
        "    \n",
        "    def save_evaluation_results(self, all_results, combined_results_df):\n",
        "        \"\"\"Save comprehensive evaluation results\"\"\"\n",
        "        \n",
        "        print(\"üíæ Saving evaluation results...\")\n",
        "        \n",
        "        # Save individual model results\n",
        "        for model_name, results in all_results.items():\n",
        "            results_path = self.dirs['results'] / f'{model_name}_evaluation_results.csv'\n",
        "            df = pd.DataFrame(results)\n",
        "            df.to_csv(results_path, index=False)\n",
        "            print(f\"  ‚úÖ {model_name} results saved to {results_path}\")\n",
        "        \n",
        "        # Save combined results\n",
        "        combined_path = self.dirs['results'] / 'all_models_combined_results.csv'\n",
        "        combined_results_df.to_csv(combined_path, index=False)\n",
        "        print(f\"  ‚úÖ Combined results saved to {combined_path}\")\n",
        "        \n",
        "        # Save per-model summary statistics\n",
        "        summary_stats = []\n",
        "        for model_name in combined_results_df['model'].unique():\n",
        "            model_data = combined_results_df[combined_results_df['model'] == model_name]\n",
        "            \n",
        "            stats = {\n",
        "                'model': model_name,\n",
        "                'n_images': len(model_data),\n",
        "                'pq_mean': model_data['pq'].mean(),\n",
        "                'pq_std': model_data['pq'].std(),\n",
        "                'pq_median': model_data['pq'].median(),\n",
        "                'pq_q25': model_data['pq'].quantile(0.25),\n",
        "                'pq_q75': model_data['pq'].quantile(0.75),\n",
        "                'f1_mean': model_data['detection_f1'].mean(),\n",
        "                'f1_std': model_data['detection_f1'].std(),\n",
        "                'f1_median': model_data['detection_f1'].median(),\n",
        "                'f1_q25': model_data['detection_f1'].quantile(0.25),\n",
        "                'f1_q75': model_data['detection_f1'].quantile(0.75),\n",
        "                'iou_mean': model_data['iou'].mean(),\n",
        "                'iou_std': model_data['iou'].std(),\n",
        "                'precision_mean': model_data['precision'].mean(),\n",
        "                'recall_mean': model_data['recall'].mean()\n",
        "            }\n",
        "            summary_stats.append(stats)\n",
        "        \n",
        "        summary_df = pd.DataFrame(summary_stats)\n",
        "        summary_path = self.dirs['summaries'] / 'model_performance_summary.csv'\n",
        "        summary_df.to_csv(summary_path, index=False)\n",
        "        print(f\"  ‚úÖ Model performance summary saved to {summary_path}\")\n",
        "    \n",
        "    def save_statistical_analysis(self, all_comparisons):\n",
        "        \"\"\"Save all statistical analysis results\"\"\"\n",
        "        \n",
        "        print(\"üíæ Saving statistical analysis...\")\n",
        "        \n",
        "        for metric, comparisons in all_comparisons.items():\n",
        "            # Save pairwise comparisons\n",
        "            pairwise_path = self.dirs['comparisons'] / f'{metric}_pairwise_comparisons.csv'\n",
        "            comparisons['pairwise'].to_csv(pairwise_path, index=False)\n",
        "            \n",
        "            # Save SAM vs established comparisons\n",
        "            sam_vs_est_path = self.dirs['comparisons'] / f'{metric}_sam_vs_established.csv'\n",
        "            comparisons['sam_vs_established'].to_csv(sam_vs_est_path, index=False)\n",
        "            \n",
        "            print(f\"  ‚úÖ {metric} statistical comparisons saved\")\n",
        "        \n",
        "        # Create statistical summary\n",
        "        stat_summary = []\n",
        "        for metric, comparisons in all_comparisons.items():\n",
        "            pairwise = comparisons['pairwise']\n",
        "            sam_vs_est = comparisons['sam_vs_established']\n",
        "            \n",
        "            summary = {\n",
        "                'metric': metric,\n",
        "                'total_comparisons': len(pairwise),\n",
        "                'significant_t_test': pairwise['t_significant'].sum(),\n",
        "                'significant_wilcoxon': pairwise['w_significant'].sum(),\n",
        "                'sam_vs_est_comparisons': len(sam_vs_est),\n",
        "                'sam_wins': (sam_vs_est['sam_better'] == True).sum(),\n",
        "                'established_wins': (sam_vs_est['sam_better'] == False).sum(),\n",
        "                'significant_sam_vs_est': sam_vs_est['significant_t'].sum()\n",
        "            }\n",
        "            stat_summary.append(summary)\n",
        "        \n",
        "        stat_summary_df = pd.DataFrame(stat_summary)\n",
        "        stat_summary_path = self.dirs['statistics'] / 'statistical_analysis_summary.csv'\n",
        "        stat_summary_df.to_csv(stat_summary_path, index=False)\n",
        "        print(f\"  ‚úÖ Statistical analysis summary saved to {stat_summary_path}\")\n",
        "    \n",
        "    def save_research_conclusions(self, conclusions, ranking_df):\n",
        "        \"\"\"Save research conclusions and final rankings\"\"\"\n",
        "        \n",
        "        print(\"üíæ Saving research conclusions...\")\n",
        "        \n",
        "        # Save conclusions JSON\n",
        "        conclusions_path = self.dirs['summaries'] / 'research_conclusions.json'\n",
        "        with open(conclusions_path, 'w') as f:\n",
        "            json.dump(conclusions, f, indent=2)\n",
        "        \n",
        "        # Save model rankings\n",
        "        ranking_path = self.dirs['summaries'] / 'model_rankings.csv'\n",
        "        ranking_df.to_csv(ranking_path, index=True)\n",
        "        \n",
        "        # Create final research report\n",
        "        report_path = self.dirs['summaries'] / 'research_question_1_report.md'\n",
        "        self.create_research_report(conclusions, ranking_df, report_path)\n",
        "        \n",
        "        print(f\"  ‚úÖ Research conclusions saved to {self.dirs['summaries']}\")\n",
        "    \n",
        "    def create_research_report(self, conclusions, ranking_df, report_path):\n",
        "        \"\"\"Create a comprehensive research report in Markdown format\"\"\"\n",
        "        \n",
        "        report_content = f\"\"\"# Research Question 1: SAM Variants vs Established Models\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This report presents the results of a comprehensive comparison between Segment Anything Model (SAM) variants and established nuclei segmentation models on the PanNuke dataset.\n",
        "\n",
        "## Research Question\n",
        "Do different variants of the Segment Anything Model (SAM), including the domain-adapted PathoSAM, achieve competitive or superior nuclei instance segmentation performance on the PanNuke dataset compared to established models such as HoVer-Net, CellViT, and LKCell?\n",
        "\n",
        "## Hypotheses\n",
        "- **H‚ÇÄ (Null)**: SAM variants do not significantly outperform established models in mPQ or detection F1\n",
        "- **H‚ÇÅ (Alternative)**: At least one SAM variant significantly outperforms baselines in mPQ or detection F1\n",
        "\n",
        "## Methodology\n",
        "- **Dataset**: PanNuke dataset with proper train/val/test splits\n",
        "- **Models Evaluated**: {len(ranking_df)} models total\n",
        "  - SAM Variants: SAM-Base, SAM-Large, SAM-Huge, PathoSAM\n",
        "  - Established Models: HoVer-Net, CellViT, LKCell\n",
        "  - Baseline: U-Net\n",
        "- **Metrics**: Mean Panoptic Quality (mPQ), Detection F1 Score, IoU, Precision, Recall\n",
        "- **Statistical Analysis**: Paired t-tests, Wilcoxon signed-rank tests, multiple comparison corrections\n",
        "\n",
        "## Key Results\n",
        "\n",
        "### Model Rankings\n",
        "{ranking_df.to_string()}\n",
        "\n",
        "### Best Performing Model\n",
        "- **Model**: {conclusions['best_model']}\n",
        "- **SAM Win Rate**: {conclusions['sam_win_rate']:.2%}\n",
        "- **Significant Comparisons**: {conclusions['significant_comparisons']}\n",
        "- **SAM Wins**: {conclusions['sam_wins']}\n",
        "- **Established Model Wins**: {conclusions['established_wins']}\n",
        "\n",
        "### Statistical Significance\n",
        "- Total significant comparisons found: {conclusions['significant_comparisons']}\n",
        "- SAM variants won: {conclusions['sam_wins']} comparisons\n",
        "- Established models won: {conclusions['established_wins']} comparisons\n",
        "\n",
        "## Conclusion\n",
        "\"\"\"\n",
        "        \n",
        "        if conclusions['significant_comparisons'] > 0:\n",
        "            if conclusions['sam_win_rate'] > 0.5:\n",
        "                report_content += \"**H‚ÇÅ SUPPORTED**: SAM variants significantly outperform established models in nuclei segmentation tasks.\\n\"\n",
        "            else:\n",
        "                report_content += \"**H‚ÇÄ SUPPORTED**: SAM variants do not significantly outperform established models in nuclei segmentation tasks.\\n\"\n",
        "        else:\n",
        "            report_content += \"**INCONCLUSIVE**: No significant differences found between SAM variants and established models.\\n\"\n",
        "        \n",
        "        report_content += f\"\"\"\n",
        "## Files Generated\n",
        "- Model weights: `{self.dirs['models']}`\n",
        "- Evaluation results: `{self.dirs['results']}`\n",
        "- Statistical analysis: `{self.dirs['statistics']}`\n",
        "- Visualizations: `{self.dirs['plots']}`\n",
        "- Comparisons: `{self.dirs['comparisons']}`\n",
        "- Summaries: `{self.dirs['summaries']}`\n",
        "\n",
        "## Reproducibility\n",
        "All code, data, and results are saved in the artifacts directory for full reproducibility.\n",
        "\"\"\"\n",
        "        \n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(report_content)\n",
        "        \n",
        "        print(f\"  ‚úÖ Research report saved to {report_path}\")\n",
        "    \n",
        "    def save_configuration(self, config):\n",
        "        \"\"\"Save experiment configuration\"\"\"\n",
        "        \n",
        "        print(\"üíæ Saving experiment configuration...\")\n",
        "        \n",
        "        config_dict = {\n",
        "            'data_root': config.data_root,\n",
        "            'artifacts_dir': config.artifacts_dir,\n",
        "            'batch_size': config.batch_size,\n",
        "            'num_workers': config.num_workers,\n",
        "            'image_size': config.image_size,\n",
        "            'learning_rate': config.learning_rate,\n",
        "            'epochs': config.epochs,\n",
        "            'patience': config.patience,\n",
        "            'confidence_threshold': config.confidence_threshold,\n",
        "            'iou_threshold': config.iou_threshold,\n",
        "            'alpha': config.alpha,\n",
        "            'n_bootstrap': config.n_bootstrap,\n",
        "            'sam_models': config.sam_models\n",
        "        }\n",
        "        \n",
        "        config_path = self.dirs['configs'] / 'experiment_config.json'\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config_dict, f, indent=2)\n",
        "        \n",
        "        print(f\"  ‚úÖ Configuration saved to {config_path}\")\n",
        "    \n",
        "    def create_artifacts_summary(self):\n",
        "        \"\"\"Create a summary of all saved artifacts\"\"\"\n",
        "        \n",
        "        summary = {\n",
        "            'experiment': 'Research Question 1: SAM Variants vs Established Models',\n",
        "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'artifacts_directory': str(self.rq1_dir),\n",
        "            'directories': {name: str(path) for name, path in self.dirs.items()},\n",
        "            'files_generated': {}\n",
        "        }\n",
        "        \n",
        "        # Count files in each directory\n",
        "        for dir_name, dir_path in self.dirs.items():\n",
        "            if dir_path.exists():\n",
        "                files = list(dir_path.glob('*'))\n",
        "                summary['files_generated'][dir_name] = len(files)\n",
        "        \n",
        "        # Save summary\n",
        "        summary_path = self.rq1_dir / 'artifacts_summary.json'\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "        \n",
        "        print(f\"\\nüìã ARTIFACTS SUMMARY:\")\n",
        "        print(f\"  üìÅ Base directory: {self.rq1_dir}\")\n",
        "        print(f\"  üìä Files generated:\")\n",
        "        for dir_name, count in summary['files_generated'].items():\n",
        "            print(f\"    {dir_name}: {count} files\")\n",
        "        \n",
        "        return summary\n",
        "\n",
        "# Initialize artifact manager\n",
        "artifact_manager = ArtifactManager(artifacts_dir.parent)\n",
        "\n",
        "print(\"üöÄ Starting comprehensive artifact management...\")\n",
        "\n",
        "# Save all results\n",
        "artifact_manager.save_model_results(trained_models, training_histories)\n",
        "artifact_manager.save_evaluation_results(all_results, combined_results_df)\n",
        "artifact_manager.save_statistical_analysis(all_comparisons)\n",
        "artifact_manager.save_research_conclusions(conclusions, conclusions['ranking_df'])\n",
        "artifact_manager.save_configuration(config)\n",
        "\n",
        "# Create final summary\n",
        "final_summary = artifact_manager.create_artifacts_summary()\n",
        "\n",
        "print(f\"\\n‚úÖ ALL ARTIFACTS SAVED SUCCESSFULLY!\")\n",
        "print(f\"üìÅ Main artifacts directory: {artifact_manager.rq1_dir}\")\n",
        "print(f\"üìã Summary saved to: {artifact_manager.rq1_dir / 'artifacts_summary.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
