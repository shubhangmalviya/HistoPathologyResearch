{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RQ3: Stain Normalization Impact Analysis - Inference & Statistical Analysis\n",
        "\n",
        "## Research Question\n",
        "**How does stain normalization affect the performance of deep learning models for histopathology image segmentation?**\n",
        "\n",
        "## Objective\n",
        "This notebook performs comprehensive inference analysis on the test set using trained models (original vs normalized) and conducts statistical analysis to answer the research question.\n",
        "\n",
        "## Key Analysis Components:\n",
        "1. **Model Loading**: Load best checkpoint models for both original and normalized data\n",
        "2. **Test Set Inference**: Perform inference on test set for both models\n",
        "3. **Per-Image Analysis**: Calculate metrics (Dice, IoU, Precision, Recall, F1) for each image\n",
        "4. **Per-Tissue Analysis**: Analyze performance across different tissue types\n",
        "5. **Statistical Testing**: Paired t-tests, Wilcoxon tests, and effect size analysis\n",
        "6. **EDA & Visualization**: Comprehensive exploratory data analysis and visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Scientific computing\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_rel, wilcoxon, shapiro, levene\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append('/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch/src')\n",
        "\n",
        "# Import custom modules\n",
        "from models.unet_rq3 import UNetRQ3\n",
        "from utils.metrics import calculate_segmentation_metrics\n",
        "from preprocessing.vahadane_gpu import GPUVahadaneNormalizer\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration and Setup\n",
        "class Config:\n",
        "    # Paths\n",
        "    PROJECT_ROOT = Path('/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch')\n",
        "    ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts' / 'rq3_enhanced'\n",
        "    CHECKPOINT_DIR = ARTIFACTS_DIR / 'checkpoints'\n",
        "    DATASET_DIR = ARTIFACTS_DIR / 'datasets'\n",
        "    RESULTS_DIR = ARTIFACTS_DIR / 'analysis'\n",
        "    \n",
        "    # Model parameters\n",
        "    N_CHANNELS = 3\n",
        "    N_CLASSES = 6\n",
        "    IMG_SIZE = 256\n",
        "    \n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # Batch size for inference\n",
        "    BATCH_SIZE = 8\n",
        "    \n",
        "    # Class names for PanNuke dataset\n",
        "    CLASS_NAMES = ['Neoplastic', 'Inflammatory', 'Connective', 'Dead', 'Epithelial', 'Background']\n",
        "    \n",
        "    # Tissue types\n",
        "    TISSUE_TYPES = [\n",
        "        'Adrenal_gland', 'Bile-duct', 'Bladder', 'Breast', 'Cervix', 'Colon',\n",
        "        'Esophagus', 'HeadNeck', 'Kidney', 'Liver', 'Lung', 'Ovarian',\n",
        "        'Pancreatic', 'Prostate', 'Skin', 'Stomach', 'Testis', 'Thyroid', 'Uterus'\n",
        "    ]\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "\n",
        "print(\"üîß Configuration loaded:\")\n",
        "print(f\"   Device: {config.DEVICE}\")\n",
        "print(f\"   Checkpoint dir: {config.CHECKPOINT_DIR}\")\n",
        "print(f\"   Dataset dir: {config.DATASET_DIR}\")\n",
        "print(f\"   Results dir: {config.RESULTS_DIR}\")\n",
        "print(f\"   Classes: {config.CLASS_NAMES}\")\n",
        "print(f\"   Tissue types: {len(config.TISSUE_TYPES)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility Functions for Analysis\n",
        "\n",
        "def load_model_from_checkpoint(checkpoint_path: Path, model_type: str = 'original') -> nn.Module:\n",
        "    \"\"\"Load a trained model from checkpoint\"\"\"\n",
        "    print(f\"üì• Loading {model_type} model from {checkpoint_path}\")\n",
        "    \n",
        "    # Create model with correct RQ3 UNet implementation\n",
        "    model = UNetRQ3(n_channels=config.N_CHANNELS, n_classes=config.N_CLASSES).to(config.DEVICE)\n",
        "    \n",
        "    # Load checkpoint - the checkpoint contains the state dict directly\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"‚úÖ {model_type} model loaded successfully\")\n",
        "    return model\n",
        "\n",
        "def calculate_per_image_metrics(model: nn.Module, dataloader: DataLoader, data_type: str) -> List[Dict]:\n",
        "    \"\"\"Calculate per-image metrics for a model\"\"\"\n",
        "    print(f\"üîç Calculating per-image metrics for {data_type} model...\")\n",
        "    \n",
        "    results = []\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "            images = images.to(config.DEVICE)\n",
        "            masks = masks.to(config.DEVICE)\n",
        "            \n",
        "            # Get predictions\n",
        "            outputs = model(images)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            \n",
        "            # Calculate metrics for each image in the batch\n",
        "            for i in range(images.size(0)):\n",
        "                # Get single image and mask\n",
        "                pred = predictions[i].cpu().numpy()\n",
        "                true = masks[i].cpu().numpy()\n",
        "                \n",
        "                # Get filename for tissue type extraction\n",
        "                global_idx = batch_idx * config.BATCH_SIZE + i\n",
        "                if hasattr(dataloader.dataset, 'filenames') and global_idx < len(dataloader.dataset.filenames):\n",
        "                    filename = dataloader.dataset.filenames[global_idx]\n",
        "                    tissue_type = extract_tissue_type(filename)\n",
        "                else:\n",
        "                    tissue_type = 'Unknown'\n",
        "                \n",
        "                # Calculate metrics\n",
        "                dice_scores = []\n",
        "                iou_scores = []\n",
        "                precision_scores = []\n",
        "                recall_scores = []\n",
        "                f1_scores = []\n",
        "                \n",
        "                for class_idx in range(config.N_CLASSES):\n",
        "                    if class_idx == 5:  # Skip background class\n",
        "                        continue\n",
        "                        \n",
        "                    # Create binary masks for this class\n",
        "                    pred_binary = (pred == class_idx).astype(np.uint8)\n",
        "                    true_binary = (true == class_idx).astype(np.uint8)\n",
        "                    \n",
        "                    # Calculate metrics\n",
        "                    if np.sum(true_binary) > 0 or np.sum(pred_binary) > 0:\n",
        "                        # Dice score\n",
        "                        intersection = np.sum(pred_binary * true_binary)\n",
        "                        dice = (2.0 * intersection) / (np.sum(pred_binary) + np.sum(true_binary) + 1e-8)\n",
        "                        dice_scores.append(dice)\n",
        "                        \n",
        "                        # IoU score\n",
        "                        union = np.sum(pred_binary) + np.sum(true_binary) - intersection\n",
        "                        iou = intersection / (union + 1e-8)\n",
        "                        iou_scores.append(iou)\n",
        "                        \n",
        "                        # Precision, Recall, F1\n",
        "                        if np.sum(pred_binary) > 0:\n",
        "                            precision = np.sum(pred_binary * true_binary) / np.sum(pred_binary)\n",
        "                        else:\n",
        "                            precision = 0.0\n",
        "                        precision_scores.append(precision)\n",
        "                        \n",
        "                        if np.sum(true_binary) > 0:\n",
        "                            recall = np.sum(pred_binary * true_binary) / np.sum(true_binary)\n",
        "                        else:\n",
        "                            recall = 0.0\n",
        "                        recall_scores.append(recall)\n",
        "                        \n",
        "                        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "                        f1_scores.append(f1)\n",
        "                    else:\n",
        "                        dice_scores.append(0.0)\n",
        "                        iou_scores.append(0.0)\n",
        "                        precision_scores.append(0.0)\n",
        "                        recall_scores.append(0.0)\n",
        "                        f1_scores.append(0.0)\n",
        "                \n",
        "                # Calculate mean metrics\n",
        "                mean_dice = np.mean(dice_scores)\n",
        "                mean_iou = np.mean(iou_scores)\n",
        "                mean_precision = np.mean(precision_scores)\n",
        "                mean_recall = np.mean(recall_scores)\n",
        "                mean_f1 = np.mean(f1_scores)\n",
        "                \n",
        "                # Store results\n",
        "                result = {\n",
        "                    'data_type': data_type,\n",
        "                    'batch_idx': batch_idx,\n",
        "                    'image_idx': i,\n",
        "                    'global_idx': global_idx,\n",
        "                    'filename': filename if 'filename' in locals() else f'image_{global_idx}',\n",
        "                    'tissue_type': tissue_type,\n",
        "                    'dice_score': mean_dice,\n",
        "                    'iou_score': mean_iou,\n",
        "                    'precision': mean_precision,\n",
        "                    'recall': mean_recall,\n",
        "                    'f1_score': mean_f1,\n",
        "                    'dice_per_class': dice_scores,\n",
        "                    'iou_per_class': iou_scores,\n",
        "                    'precision_per_class': precision_scores,\n",
        "                    'recall_per_class': recall_scores,\n",
        "                    'f1_per_class': f1_scores\n",
        "                }\n",
        "                results.append(result)\n",
        "    \n",
        "    print(f\"‚úÖ Per-image metrics calculated: {len(results)} images\")\n",
        "    return results\n",
        "\n",
        "def extract_tissue_type(filename: str) -> str:\n",
        "    \"\"\"Extract tissue type from filename\"\"\"\n",
        "    for tissue in config.TISSUE_TYPES:\n",
        "        if tissue.lower() in filename.lower():\n",
        "            return tissue\n",
        "    return 'Unknown'\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Custom dataset for test images and masks\"\"\"\n",
        "    \n",
        "    def __init__(self, images_dir: Path, masks_dir: Path, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Get all image files\n",
        "        self.image_files = sorted(list(images_dir.glob(\"*.png\")))\n",
        "        self.filenames = [f.name for f in self.image_files]\n",
        "        \n",
        "        print(f\"Found {len(self.image_files)} test images\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # Load corresponding mask\n",
        "        mask_name = img_path.name\n",
        "        mask_path = self.masks_dir / mask_name\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "        \n",
        "        return image, mask\n",
        "\n",
        "def create_test_dataloader(data_type: str) -> DataLoader:\n",
        "    \"\"\"Create test dataloader for given data type\"\"\"\n",
        "    test_dir = config.DATASET_DIR / data_type / 'test'\n",
        "    images_dir = test_dir / 'images'\n",
        "    masks_dir = test_dir / 'masks'\n",
        "    \n",
        "    if not images_dir.exists():\n",
        "        raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
        "    if not masks_dir.exists():\n",
        "        raise FileNotFoundError(f\"Masks directory not found: {masks_dir}\")\n",
        "    \n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = TestDataset(images_dir, masks_dir, transform=transform)\n",
        "    \n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ {data_type} test dataloader created: {len(dataset)} images\")\n",
        "    return dataloader\n",
        "\n",
        "print(\"‚úÖ Utility functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Trained Models\n",
        "print(\"üöÄ Loading trained models from checkpoints...\")\n",
        "\n",
        "# Load original model\n",
        "original_checkpoint = config.CHECKPOINT_DIR / 'unet_original_enhanced_best.pth'\n",
        "if original_checkpoint.exists():\n",
        "    original_model = load_model_from_checkpoint(original_checkpoint, 'original')\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Original model checkpoint not found: {original_checkpoint}\")\n",
        "\n",
        "# Load normalized model\n",
        "normalized_checkpoint = config.CHECKPOINT_DIR / 'unet_normalized_enhanced_best.pth'\n",
        "if normalized_checkpoint.exists():\n",
        "    normalized_model = load_model_from_checkpoint(normalized_checkpoint, 'normalized')\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Normalized model checkpoint not found: {normalized_checkpoint}\")\n",
        "\n",
        "print(\"‚úÖ Both models loaded successfully!\")\n",
        "print(f\"   Original model: {original_checkpoint}\")\n",
        "print(f\"   Normalized model: {normalized_checkpoint}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Test DataLoaders\n",
        "print(\"üìä Creating test dataloaders...\")\n",
        "\n",
        "# Create original test dataloader\n",
        "original_dataloader = create_test_dataloader('original')\n",
        "\n",
        "# Create normalized test dataloader\n",
        "normalized_dataloader = create_test_dataloader('normalized')\n",
        "\n",
        "print(\"‚úÖ Test dataloaders created successfully!\")\n",
        "print(f\"   Original test images: {len(original_dataloader.dataset)}\")\n",
        "print(f\"   Normalized test images: {len(normalized_dataloader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Inference and Calculate Metrics\n",
        "print(\"üîç Performing inference on test set...\")\n",
        "\n",
        "# Calculate metrics for original model\n",
        "print(\"\\nüìä Evaluating original model...\")\n",
        "original_results = calculate_per_image_metrics(original_model, original_dataloader, 'original')\n",
        "\n",
        "# Calculate metrics for normalized model\n",
        "print(\"\\nüìä Evaluating normalized model...\")\n",
        "normalized_results = calculate_per_image_metrics(normalized_model, normalized_dataloader, 'normalized')\n",
        "\n",
        "# Combine results\n",
        "all_results = original_results + normalized_results\n",
        "\n",
        "print(f\"\\n‚úÖ Inference completed!\")\n",
        "print(f\"   Original model: {len(original_results)} images\")\n",
        "print(f\"   Normalized model: {len(normalized_results)} images\")\n",
        "print(f\"   Total evaluations: {len(all_results)}\")\n",
        "\n",
        "# Convert to DataFrame for analysis\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Save results\n",
        "results_dir = config.RESULTS_DIR / 'inference_analysis'\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "results_df.to_csv(results_dir / 'per_image_metrics.csv', index=False)\n",
        "print(f\"üíæ Results saved to: {results_dir / 'per_image_metrics.csv'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "print(\"üìà Performing Exploratory Data Analysis...\")\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('RQ3: Stain Normalization Impact Analysis - EDA', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Overall Performance Comparison\n",
        "ax1 = axes[0, 0]\n",
        "metrics = ['dice_score', 'iou_score', 'precision', 'recall', 'f1_score']\n",
        "original_means = [results_df[results_df['data_type'] == 'original'][metric].mean() for metric in metrics]\n",
        "normalized_means = [results_df[results_df['data_type'] == 'normalized'][metric].mean() for metric in metrics]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, original_means, width, label='Original', alpha=0.8)\n",
        "bars2 = ax1.bar(x + width/2, normalized_means, width, label='Normalized', alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Metrics')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Overall Performance Comparison')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(metrics, rotation=45)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 2. Dice Score Distribution\n",
        "ax2 = axes[0, 1]\n",
        "original_dice = results_df[results_df['data_type'] == 'original']['dice_score']\n",
        "normalized_dice = results_df[results_df['data_type'] == 'normalized']['dice_score']\n",
        "\n",
        "ax2.hist(original_dice, bins=30, alpha=0.7, label='Original', density=True)\n",
        "ax2.hist(normalized_dice, bins=30, alpha=0.7, label='Normalized', density=True)\n",
        "ax2.set_xlabel('Dice Score')\n",
        "ax2.set_ylabel('Density')\n",
        "ax2.set_title('Dice Score Distribution')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. IoU Score Distribution\n",
        "ax3 = axes[0, 2]\n",
        "original_iou = results_df[results_df['data_type'] == 'original']['iou_score']\n",
        "normalized_iou = results_df[results_df['data_type'] == 'normalized']['iou_score']\n",
        "\n",
        "ax3.hist(original_iou, bins=30, alpha=0.7, label='Original', density=True)\n",
        "ax3.hist(normalized_iou, bins=30, alpha=0.7, label='Normalized', density=True)\n",
        "ax3.set_xlabel('IoU Score')\n",
        "ax3.set_ylabel('Density')\n",
        "ax3.set_title('IoU Score Distribution')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Box Plot Comparison\n",
        "ax4 = axes[1, 0]\n",
        "dice_data = [original_dice, normalized_dice]\n",
        "box_plot = ax4.boxplot(dice_data, labels=['Original', 'Normalized'], patch_artist=True)\n",
        "colors = ['lightblue', 'lightcoral']\n",
        "for patch, color in zip(box_plot['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "ax4.set_ylabel('Dice Score')\n",
        "ax4.set_title('Dice Score Box Plot Comparison')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Scatter Plot: Dice vs IoU\n",
        "ax5 = axes[1, 1]\n",
        "ax5.scatter(original_dice, original_iou, alpha=0.6, label='Original', s=50)\n",
        "ax5.scatter(normalized_dice, normalized_iou, alpha=0.6, label='Normalized', s=50)\n",
        "ax5.set_xlabel('Dice Score')\n",
        "ax5.set_ylabel('IoU Score')\n",
        "ax5.set_title('Dice vs IoU Scatter Plot')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Performance Improvement\n",
        "ax6 = axes[1, 2]\n",
        "improvements = []\n",
        "for metric in metrics:\n",
        "    orig_mean = results_df[results_df['data_type'] == 'original'][metric].mean()\n",
        "    norm_mean = results_df[results_df['data_type'] == 'normalized'][metric].mean()\n",
        "    improvement = ((norm_mean - orig_mean) / orig_mean) * 100\n",
        "    improvements.append(improvement)\n",
        "\n",
        "bars = ax6.bar(metrics, improvements, color=['green' if x > 0 else 'red' for x in improvements], alpha=0.7)\n",
        "ax6.set_xlabel('Metrics')\n",
        "ax6.set_ylabel('Improvement (%)')\n",
        "ax6.set_title('Performance Improvement (Normalized vs Original)')\n",
        "ax6.set_xticklabels(metrics, rotation=45)\n",
        "ax6.grid(True, alpha=0.3)\n",
        "ax6.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "\n",
        "# Add value labels\n",
        "for bar, improvement in zip(bars, improvements):\n",
        "    height = bar.get_height()\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2., height + (0.5 if height > 0 else -0.5),\n",
        "             f'{improvement:.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(results_dir / 'eda_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ EDA completed and saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-Tissue Analysis\n",
        "print(\"üî¨ Performing per-tissue analysis...\")\n",
        "\n",
        "# Results already contain tissue information from the metrics calculation\n",
        "# Recreate DataFrame with all results\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Per-tissue analysis\n",
        "if 'tissue_type' in results_df.columns:\n",
        "    print(\"üìä Per-tissue performance analysis:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Get unique tissue types\n",
        "    tissue_types = results_df['tissue_type'].unique()\n",
        "    tissue_types = [t for t in tissue_types if t != 'Unknown']\n",
        "    \n",
        "    print(f\"Found {len(tissue_types)} tissue types: {tissue_types}\")\n",
        "    \n",
        "    if len(tissue_types) > 0:\n",
        "        # Group by tissue type and data type\n",
        "        tissue_analysis = results_df.groupby(['tissue_type', 'data_type']).agg({\n",
        "            'dice_score': ['mean', 'std', 'count'],\n",
        "            'iou_score': ['mean', 'std'],\n",
        "            'precision': ['mean', 'std'],\n",
        "            'recall': ['mean', 'std'],\n",
        "            'f1_score': ['mean', 'std']\n",
        "        }).round(4)\n",
        "        \n",
        "        print(tissue_analysis)\n",
        "        \n",
        "        # Save per-tissue analysis\n",
        "        tissue_analysis.to_csv(results_dir / 'per_tissue_analysis.csv')\n",
        "        print(f\"üíæ Per-tissue analysis saved to: {results_dir / 'per_tissue_analysis.csv'}\")\n",
        "        \n",
        "        # Create per-tissue visualization\n",
        "        plt.figure(figsize=(20, 12))\n",
        "        \n",
        "        # Create subplots for each metric\n",
        "        metrics = ['dice_score', 'iou_score', 'precision', 'recall', 'f1_score']\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        axes = axes.flatten()\n",
        "        \n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            \n",
        "            # Prepare data for plotting\n",
        "            tissue_data = []\n",
        "            labels = []\n",
        "            \n",
        "            for tissue in tissue_types:\n",
        "                orig_data = results_df[(results_df['tissue_type'] == tissue) & \n",
        "                                     (results_df['data_type'] == 'original')][metric]\n",
        "                norm_data = results_df[(results_df['tissue_type'] == tissue) & \n",
        "                                     (results_df['data_type'] == 'normalized')][metric]\n",
        "                \n",
        "                if len(orig_data) > 0 and len(norm_data) > 0:\n",
        "                    tissue_data.append([orig_data, norm_data])\n",
        "                    labels.append(tissue)\n",
        "            \n",
        "            # Create box plot\n",
        "            if tissue_data:\n",
        "                # Create grouped box plot\n",
        "                bp = ax.boxplot(tissue_data, labels=labels, patch_artist=True)\n",
        "                \n",
        "                # Color the boxes\n",
        "                colors = ['lightblue', 'lightcoral'] * len(tissue_data)\n",
        "                for patch, color in zip(bp['boxes'], colors):\n",
        "                    patch.set_facecolor(color)\n",
        "                \n",
        "                ax.set_title(f'{metric.replace(\"_\", \" \").title()} by Tissue Type')\n",
        "                ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
        "                ax.set_xlabel('Tissue Type')\n",
        "                ax.tick_params(axis='x', rotation=45)\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                \n",
        "                # Add legend\n",
        "                if i == 0:  # Only add legend to first subplot\n",
        "                    from matplotlib.patches import Patch\n",
        "                    legend_elements = [Patch(facecolor='lightblue', label='Original'),\n",
        "                                     Patch(facecolor='lightcoral', label='Normalized')]\n",
        "                    ax.legend(handles=legend_elements, loc='upper right')\n",
        "        \n",
        "        # Remove empty subplot\n",
        "        if len(metrics) < len(axes):\n",
        "            axes[-1].remove()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(results_dir / 'per_tissue_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"‚úÖ Per-tissue analysis completed!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No tissue type information found in dataset\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Tissue type information not available in dataset\")\n",
        "    print(\"   Skipping per-tissue analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical Analysis\n",
        "print(\"üìä Performing statistical analysis...\")\n",
        "\n",
        "# Prepare data for statistical analysis\n",
        "original_data = results_df[results_df['data_type'] == 'original']\n",
        "normalized_data = results_df[results_df['data_type'] == 'normalized']\n",
        "\n",
        "# Ensure we have the same number of samples for paired tests\n",
        "min_samples = min(len(original_data), len(normalized_data))\n",
        "original_data = original_data.head(min_samples)\n",
        "normalized_data = normalized_data.head(min_samples)\n",
        "\n",
        "print(f\"üìà Statistical analysis on {min_samples} samples\")\n",
        "\n",
        "# Statistical tests for each metric\n",
        "metrics = ['dice_score', 'iou_score', 'precision', 'recall', 'f1_score']\n",
        "statistical_results = {}\n",
        "\n",
        "for metric in metrics:\n",
        "    print(f\"\\nüîç Analyzing {metric}...\")\n",
        "    \n",
        "    orig_values = original_data[metric].values\n",
        "    norm_values = normalized_data[metric].values\n",
        "    \n",
        "    # Basic statistics\n",
        "    orig_mean = np.mean(orig_values)\n",
        "    orig_std = np.std(orig_values)\n",
        "    norm_mean = np.mean(norm_values)\n",
        "    norm_std = np.std(norm_values)\n",
        "    \n",
        "    # Paired t-test\n",
        "    t_stat, t_pvalue = ttest_rel(norm_values, orig_values)\n",
        "    \n",
        "    # Wilcoxon signed-rank test (non-parametric alternative)\n",
        "    try:\n",
        "        w_stat, w_pvalue = wilcoxon(norm_values, orig_values)\n",
        "    except ValueError:\n",
        "        w_stat, w_pvalue = np.nan, np.nan\n",
        "    \n",
        "    # Effect size (Cohen's d)\n",
        "    pooled_std = np.sqrt(((len(orig_values) - 1) * orig_std**2 + \n",
        "                         (len(norm_values) - 1) * norm_std**2) / \n",
        "                        (len(orig_values) + len(norm_values) - 2))\n",
        "    cohens_d = (norm_mean - orig_mean) / pooled_std\n",
        "    \n",
        "    # Confidence interval for the difference\n",
        "    diff = norm_values - orig_values\n",
        "    diff_mean = np.mean(diff)\n",
        "    diff_std = np.std(diff)\n",
        "    se_diff = diff_std / np.sqrt(len(diff))\n",
        "    ci_95 = (diff_mean - 1.96 * se_diff, diff_mean + 1.96 * se_diff)\n",
        "    \n",
        "    # Normality test (Shapiro-Wilk)\n",
        "    shapiro_orig = shapiro(orig_values)\n",
        "    shapiro_norm = shapiro(norm_values)\n",
        "    \n",
        "    # Store results\n",
        "    statistical_results[metric] = {\n",
        "        'original_mean': orig_mean,\n",
        "        'original_std': orig_std,\n",
        "        'normalized_mean': norm_mean,\n",
        "        'normalized_std': norm_std,\n",
        "        'difference_mean': diff_mean,\n",
        "        'difference_std': diff_std,\n",
        "        't_statistic': t_stat,\n",
        "        't_pvalue': t_pvalue,\n",
        "        'wilcoxon_statistic': w_stat,\n",
        "        'wilcoxon_pvalue': w_pvalue,\n",
        "        'cohens_d': cohens_d,\n",
        "        'ci_95_lower': ci_95[0],\n",
        "        'ci_95_upper': ci_95[1],\n",
        "        'shapiro_original_pvalue': shapiro_orig.pvalue,\n",
        "        'shapiro_normalized_pvalue': shapiro_norm.pvalue,\n",
        "        'shapiro_original_normal': shapiro_orig.pvalue > 0.05,\n",
        "        'shapiro_normalized_normal': shapiro_norm.pvalue > 0.05\n",
        "    }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"   Original: {orig_mean:.4f} ¬± {orig_std:.4f}\")\n",
        "    print(f\"   Normalized: {norm_mean:.4f} ¬± {norm_std:.4f}\")\n",
        "    print(f\"   Difference: {diff_mean:.4f} ¬± {diff_std:.4f}\")\n",
        "    print(f\"   Paired t-test: t={t_stat:.4f}, p={t_pvalue:.4f}\")\n",
        "    print(f\"   Wilcoxon test: W={w_stat:.4f}, p={w_pvalue:.4f}\")\n",
        "    print(f\"   Cohen's d: {cohens_d:.4f}\")\n",
        "    print(f\"   95% CI: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\n",
        "    print(f\"   Normality (Original): p={shapiro_orig.pvalue:.4f} ({'Normal' if shapiro_orig.pvalue > 0.05 else 'Non-normal'})\")\n",
        "    print(f\"   Normality (Normalized): p={shapiro_norm.pvalue:.4f} ({'Normal' if shapiro_norm.pvalue > 0.05 else 'Non-normal'})\")\n",
        "\n",
        "# Create summary table\n",
        "summary_df = pd.DataFrame(statistical_results).T\n",
        "summary_df = summary_df.round(4)\n",
        "\n",
        "print(\"\\nüìä Statistical Summary Table:\")\n",
        "print(\"=\" * 80)\n",
        "print(summary_df)\n",
        "\n",
        "# Save statistical results\n",
        "summary_df.to_csv(results_dir / 'statistical_analysis.csv')\n",
        "print(f\"\\nüíæ Statistical analysis saved to: {results_dir / 'statistical_analysis.csv'}\")\n",
        "\n",
        "# Save detailed results as JSON\n",
        "with open(results_dir / 'statistical_analysis.json', 'w') as f:\n",
        "    json.dump(statistical_results, f, indent=2, default=str)\n",
        "print(f\"üíæ Detailed statistical results saved to: {results_dir / 'statistical_analysis.json'}\")\n",
        "\n",
        "print(\"\\n‚úÖ Statistical analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Research Question Analysis and Conclusions\n",
        "print(\"üéØ Research Question Analysis and Conclusions\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìã RESEARCH QUESTION:\")\n",
        "print(\"How does stain normalization affect the performance of deep learning models for histopathology image segmentation?\")\n",
        "\n",
        "print(\"\\nüìä KEY FINDINGS:\")\n",
        "\n",
        "# Analyze statistical significance\n",
        "significant_metrics = []\n",
        "for metric, results in statistical_results.items():\n",
        "    if results['t_pvalue'] < 0.05:\n",
        "        significant_metrics.append(metric)\n",
        "        improvement = results['difference_mean']\n",
        "        effect_size = results['cohens_d']\n",
        "        \n",
        "        print(f\"\\n‚úÖ {metric.upper()}:\")\n",
        "        print(f\"   ‚Ä¢ Statistically significant difference (p = {results['t_pvalue']:.4f})\")\n",
        "        print(f\"   ‚Ä¢ Mean improvement: {improvement:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Effect size (Cohen's d): {effect_size:.4f}\")\n",
        "        \n",
        "        if effect_size < 0.2:\n",
        "            effect_interpretation = \"negligible\"\n",
        "        elif effect_size < 0.5:\n",
        "            effect_interpretation = \"small\"\n",
        "        elif effect_size < 0.8:\n",
        "            effect_interpretation = \"medium\"\n",
        "        else:\n",
        "            effect_interpretation = \"large\"\n",
        "        \n",
        "        print(f\"   ‚Ä¢ Effect size interpretation: {effect_interpretation}\")\n",
        "        \n",
        "        if improvement > 0:\n",
        "            print(f\"   ‚Ä¢ Conclusion: Stain normalization IMPROVES {metric}\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ Conclusion: Stain normalization DECREASES {metric}\")\n",
        "\n",
        "# Overall performance analysis\n",
        "print(f\"\\nüìà OVERALL PERFORMANCE ANALYSIS:\")\n",
        "print(f\"   ‚Ä¢ Statistically significant metrics: {len(significant_metrics)}/{len(metrics)}\")\n",
        "print(f\"   ‚Ä¢ Significant metrics: {', '.join(significant_metrics) if significant_metrics else 'None'}\")\n",
        "\n",
        "# Calculate overall improvement\n",
        "overall_improvements = []\n",
        "for metric in metrics:\n",
        "    improvement = statistical_results[metric]['difference_mean']\n",
        "    overall_improvements.append(improvement)\n",
        "\n",
        "mean_improvement = np.mean(overall_improvements)\n",
        "print(f\"   ‚Ä¢ Mean improvement across all metrics: {mean_improvement:.4f}\")\n",
        "\n",
        "if mean_improvement > 0:\n",
        "    print(\"   ‚Ä¢ Overall conclusion: Stain normalization IMPROVES model performance\")\n",
        "elif mean_improvement < 0:\n",
        "    print(\"   ‚Ä¢ Overall conclusion: Stain normalization DECREASES model performance\")\n",
        "else:\n",
        "    print(\"   ‚Ä¢ Overall conclusion: Stain normalization has NO EFFECT on model performance\")\n",
        "\n",
        "# Practical significance\n",
        "print(f\"\\nüîç PRACTICAL SIGNIFICANCE:\")\n",
        "for metric in significant_metrics:\n",
        "    results = statistical_results[metric]\n",
        "    improvement_pct = (results['difference_mean'] / results['original_mean']) * 100\n",
        "    print(f\"   ‚Ä¢ {metric}: {improvement_pct:.2f}% improvement\")\n",
        "\n",
        "# Research implications\n",
        "print(f\"\\nüéì RESEARCH IMPLICATIONS:\")\n",
        "if len(significant_metrics) > len(metrics) / 2:\n",
        "    print(\"   ‚Ä¢ Stain normalization shows significant benefits for histopathology segmentation\")\n",
        "    print(\"   ‚Ä¢ Consider implementing stain normalization in clinical workflows\")\n",
        "    print(\"   ‚Ä¢ Further research on optimal normalization parameters recommended\")\n",
        "elif len(significant_metrics) > 0:\n",
        "    print(\"   ‚Ä¢ Stain normalization shows mixed results\")\n",
        "    print(\"   ‚Ä¢ Benefits are metric-specific and context-dependent\")\n",
        "    print(\"   ‚Ä¢ Consider case-by-case evaluation for different tissue types\")\n",
        "else:\n",
        "    print(\"   ‚Ä¢ Stain normalization shows no significant benefits\")\n",
        "    print(\"   ‚Ä¢ Original images may be sufficient for this task\")\n",
        "    print(\"   ‚Ä¢ Consider alternative preprocessing approaches\")\n",
        "\n",
        "print(f\"\\nüìù RECOMMENDATIONS:\")\n",
        "print(\"   1. Validate findings on larger, diverse datasets\")\n",
        "print(\"   2. Investigate tissue-specific normalization effects\")\n",
        "print(\"   3. Consider computational cost vs. performance trade-offs\")\n",
        "print(\"   4. Evaluate clinical relevance of observed improvements\")\n",
        "\n",
        "print(f\"\\n‚úÖ Research question analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary and Report Generation\n",
        "print(\"üìã Generating Final Summary Report...\")\n",
        "\n",
        "# Create comprehensive summary\n",
        "summary_report = {\n",
        "    \"research_question\": \"How does stain normalization affect the performance of deep learning models for histopathology image segmentation?\",\n",
        "    \"analysis_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"dataset_info\": {\n",
        "        \"total_images\": len(all_results),\n",
        "        \"original_images\": len(original_results),\n",
        "        \"normalized_images\": len(normalized_results),\n",
        "        \"tissue_types\": len(config.TISSUE_TYPES),\n",
        "        \"classes\": config.CLASS_NAMES\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"original_checkpoint\": str(original_checkpoint),\n",
        "        \"normalized_checkpoint\": str(normalized_checkpoint),\n",
        "        \"device\": str(config.DEVICE),\n",
        "        \"batch_size\": config.BATCH_SIZE\n",
        "    },\n",
        "    \"metrics_analyzed\": metrics,\n",
        "    \"statistical_summary\": {\n",
        "        \"significant_metrics\": significant_metrics,\n",
        "        \"total_metrics\": len(metrics),\n",
        "        \"significance_rate\": len(significant_metrics) / len(metrics),\n",
        "        \"mean_improvement\": float(mean_improvement)\n",
        "    },\n",
        "    \"key_findings\": {\n",
        "        \"overall_conclusion\": \"Stain normalization IMPROVES model performance\" if mean_improvement > 0 else \"Stain normalization DECREASES model performance\" if mean_improvement < 0 else \"Stain normalization has NO EFFECT on model performance\",\n",
        "        \"statistically_significant\": len(significant_metrics) > 0,\n",
        "        \"practical_significance\": any(abs(statistical_results[metric]['cohens_d']) > 0.2 for metric in significant_metrics) if significant_metrics else False\n",
        "    },\n",
        "    \"files_generated\": [\n",
        "        \"per_image_metrics.csv\",\n",
        "        \"per_tissue_analysis.csv\",\n",
        "        \"statistical_analysis.csv\",\n",
        "        \"statistical_analysis.json\",\n",
        "        \"eda_analysis.png\",\n",
        "        \"per_tissue_analysis.png\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save summary report\n",
        "with open(results_dir / 'summary_report.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=2, default=str)\n",
        "\n",
        "# Create markdown report\n",
        "markdown_report = f\"\"\"# RQ3: Stain Normalization Impact Analysis - Final Report\n",
        "\n",
        "## Research Question\n",
        "{summary_report['research_question']}\n",
        "\n",
        "## Analysis Date\n",
        "{summary_report['analysis_date']}\n",
        "\n",
        "## Dataset Information\n",
        "- **Total Images**: {summary_report['dataset_info']['total_images']}\n",
        "- **Original Images**: {summary_report['dataset_info']['original_images']}\n",
        "- **Normalized Images**: {summary_report['dataset_info']['normalized_images']}\n",
        "- **Tissue Types**: {summary_report['dataset_info']['tissue_types']}\n",
        "- **Classes**: {', '.join(summary_report['dataset_info']['classes'])}\n",
        "\n",
        "## Model Information\n",
        "- **Original Model**: {summary_report['model_info']['original_checkpoint']}\n",
        "- **Normalized Model**: {summary_report['model_info']['normalized_checkpoint']}\n",
        "- **Device**: {summary_report['model_info']['device']}\n",
        "- **Batch Size**: {summary_report['model_info']['batch_size']}\n",
        "\n",
        "## Statistical Analysis Results\n",
        "- **Metrics Analyzed**: {', '.join(summary_report['metrics_analyzed'])}\n",
        "- **Significant Metrics**: {len(summary_report['statistical_summary']['significant_metrics'])}/{summary_report['statistical_summary']['total_metrics']}\n",
        "- **Significance Rate**: {summary_report['statistical_summary']['significance_rate']:.2%}\n",
        "- **Mean Improvement**: {summary_report['statistical_summary']['mean_improvement']:.4f}\n",
        "\n",
        "## Key Findings\n",
        "- **Overall Conclusion**: {summary_report['key_findings']['overall_conclusion']}\n",
        "- **Statistically Significant**: {'Yes' if summary_report['key_findings']['statistically_significant'] else 'No'}\n",
        "- **Practically Significant**: {'Yes' if summary_report['key_findings']['practical_significance'] else 'No'}\n",
        "\n",
        "## Files Generated\n",
        "{chr(10).join(f\"- {file}\" for file in summary_report['files_generated'])}\n",
        "\n",
        "## Detailed Results\n",
        "See the individual CSV and JSON files for detailed statistical analysis results.\n",
        "\n",
        "---\n",
        "*Generated by RQ3 Inference Analysis Pipeline*\n",
        "\"\"\"\n",
        "\n",
        "# Save markdown report\n",
        "with open(results_dir / 'summary_report.md', 'w') as f:\n",
        "    f.write(markdown_report)\n",
        "\n",
        "print(\"‚úÖ Final summary report generated!\")\n",
        "print(f\"   üìÑ JSON report: {results_dir / 'summary_report.json'}\")\n",
        "print(f\"   üìÑ Markdown report: {results_dir / 'summary_report.md'}\")\n",
        "\n",
        "print(f\"\\nüéâ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"   üìÅ Results directory: {results_dir}\")\n",
        "print(f\"   üìä Total files generated: {len(summary_report['files_generated'])}\")\n",
        "print(f\"   ‚è±Ô∏è  Analysis completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\nüìã NEXT STEPS:\")\n",
        "print(f\"   1. Review the generated visualizations and statistical results\")\n",
        "print(f\"   2. Examine the per-tissue analysis for tissue-specific insights\")\n",
        "print(f\"   3. Consider the practical implications of the findings\")\n",
        "print(f\"   4. Use the results to inform future research directions\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
