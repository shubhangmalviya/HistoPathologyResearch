{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RQ4: Do lightweight explainability techniques (Grad-CAM) enhance interpretability of U-Net-based nuclei segmentation on PanNuke, and does stain normalization improve this further?\n",
        "\n",
        "**Motivation**\n",
        "Deep learning is increasingly integrated into clinical workflows where transparency is essential. Lightweight explainability such as Grad-CAM can offer visual cues into model decision-making, yet its effectiveness for biomedical image segmentation, especially under varying preprocessing, is underexplored. This study assesses whether stain normalization improves the clarity and spatial accuracy of Grad-CAM-based explanations and whether these explanations align with biologically relevant regions.\n",
        "\n",
        "**Hypotheses**\n",
        "- H0: Stain normalization does not improve Grad-CAM alignment.\n",
        "- H1: Stain normalization significantly improves Grad-CAM alignment.\n",
        "\n",
        "We compare U-Net predictions and Grad-CAM maps on original vs Vahadane-normalized inputs and quantify alignment with nuclei masks using point-and-area-based metrics, followed by paired non-parametric tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "project_root = Path(\"/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch\")\n",
        "\n",
        "# Paths\n",
        "DATASET_ROOT = project_root / \"dataset\"\n",
        "DATASET_TISSUES = project_root / \"dataset_tissues\"\n",
        "ARTIFACTS_RQ3 = project_root / \"artifacts\" / \"rq3_enhanced\"\n",
        "CHECKPOINTS = ARTIFACTS_RQ3 / \"checkpoints\"\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.datasets.pannuke_tissue_dataset import PanNukeTissueDataset\n",
        "from src.datasets.paired_list_dataset import gather_all_items_for_tissue\n",
        "from src.models.unet_rq3 import create_unet_rq3\n",
        "from src.preprocessing.vahadane_gpu import GPUVahadaneNormalizer\n",
        "\n",
        "# Basic plotting helpers\n",
        "sns.set_context(\"notebook\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "def show_image_mask(img, mask, title=None):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "    ax[0].imshow(np.clip((img.permute(1,2,0).cpu().numpy() * 0.229 + 0.485), 0, 1))\n",
        "    ax[0].set_title(\"Image\")\n",
        "    ax[0].axis(\"off\")\n",
        "    ax[1].imshow(mask.cpu().numpy(), cmap=\"magma\")\n",
        "    ax[1].set_title(\"Mask\")\n",
        "    ax[1].axis(\"off\")\n",
        "    if title:\n",
        "        fig.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset EDA\n",
        "We summarize the evaluation split and visualize a few random samples per tissue. We also inspect class-0 (background) vs nuclei coverage to understand expected signal for Grad-CAM alignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Gather available tissues\n",
        "available_tissues = [p.name for p in DATASET_TISSUES.iterdir() if p.is_dir()]\n",
        "\n",
        "records = []\n",
        "for tissue in sorted(available_tissues):\n",
        "    tissue_root = DATASET_TISSUES / tissue\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        img_dir = tissue_root / split / \"images\"\n",
        "        sem_dir = tissue_root / split / \"sem_masks\"\n",
        "        if img_dir.is_dir() and sem_dir.is_dir():\n",
        "            n_imgs = len([n for n in img_dir.iterdir() if n.suffix == \".png\"])            \n",
        "            records.append({\"tissue\": tissue, \"split\": split, \"n\": n_imgs})\n",
        "\n",
        "eda_df = pd.DataFrame(records)\n",
        "eda_pivot = eda_df.pivot(index=\"tissue\", columns=\"split\", values=\"n\").fillna(0).astype(int)\n",
        "eda_pivot.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, max(4, len(eda_pivot) * 0.3)))\n",
        "sns.heatmap(eda_pivot, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Per-tissue image counts by split\")\n",
        "plt.ylabel(\"Tissue\")\n",
        "plt.xlabel(\"Split\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample a few examples per tissue from test split\n",
        "n_per_tissue = 2\n",
        "sampled_examples = []\n",
        "for tissue in sorted(available_tissues):\n",
        "    items = []\n",
        "    img_dir = DATASET_TISSUES / tissue / \"test\" / \"images\"\n",
        "    sem_dir = DATASET_TISSUES / tissue / \"test\" / \"sem_masks\"\n",
        "    if img_dir.is_dir() and sem_dir.is_dir():\n",
        "        names = [n.name for n in img_dir.iterdir() if n.suffix == \".png\"]\n",
        "        for name in random.sample(names, min(n_per_tissue, len(names))):\n",
        "            items.append({\n",
        "                \"image_path\": str(img_dir / name),\n",
        "                \"sem_path\": str(sem_dir / name.replace(\"img_\", \"sem_\", 1))\n",
        "            })\n",
        "    sampled_examples.extend([(tissue, e) for e in items])\n",
        "\n",
        "# Visualize\n",
        "for tissue, entry in sampled_examples[:6]:\n",
        "    img = PanNukeTissueDataset.default_image_transform(Image.open(entry[\"image_path\"]).convert(\"RGB\"))\n",
        "    msk = PanNukeTissueDataset.default_target_transform(Image.open(entry[\"sem_path\"]))\n",
        "    show_image_mask(img, msk, title=f\"{tissue}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from scipy.stats import wilcoxon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nuclei coverage EDA on a small sample\n",
        "cov_records = []\n",
        "for tissue, entry in sampled_examples:\n",
        "    mask = np.array(Image.open(entry[\"sem_path\"]))\n",
        "    nuclei_frac = (mask > 0).mean()\n",
        "    cov_records.append({\"tissue\": tissue, \"nuclei_fraction\": nuclei_frac})\n",
        "\n",
        "cov_df = pd.DataFrame(cov_records)\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(data=cov_df, x=\"tissue\", y=\"nuclei_fraction\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"Nuclei coverage fraction across sampled test images\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "cov_df.groupby(\"tissue\")[\"nuclei_fraction\"].describe().round(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model loading: original vs normalized\n",
        "from src.models.unet_rq3 import UNetRQ3\n",
        "\n",
        "def load_model(checkpoint_path: Path, device: torch.device, n_channels=3, n_classes=6):\n",
        "    model = UNetRQ3(n_channels=n_channels, n_classes=n_classes).to(device)\n",
        "    if checkpoint_path.exists():\n",
        "        state = torch.load(checkpoint_path, map_location=device)\n",
        "        # handle typical structures\n",
        "        state_dict = state.get('model_state_dict', state)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print(f\"Loaded weights from {checkpoint_path.name}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint not found: {checkpoint_path.name} — using random init\")\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "ckpt_orig = CHECKPOINTS / \"best_model_original.pth\"\n",
        "ckpt_norm = CHECKPOINTS / \"best_model_normalized.pth\"\n",
        "\n",
        "model_original = load_model(ckpt_orig, device)\n",
        "model_normalized = load_model(ckpt_norm, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Vahadane normalizer\n",
        "try:\n",
        "    normalizer = GPUVahadaneNormalizer(device=device)\n",
        "    # Fit target using a representative tissue test image if available\n",
        "    # fallback to a random sampled image from earlier\n",
        "    if len(sampled_examples) > 0:\n",
        "        target_path = sampled_examples[0][1][\"image_path\"]\n",
        "        normalizer.fit(target_path)\n",
        "    else:\n",
        "        print(\"No sampled examples to fit normalizer; using default stains\")\n",
        "except Exception as e:\n",
        "    print(f\"Normalizer init/fit failed: {e}\")\n",
        "    normalizer = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SegmentationGradCAM:\n",
        "    def __init__(self, model: nn.Module, target_layers, device):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.device = device\n",
        "        self.activations = []\n",
        "        self.gradients = []\n",
        "        self.handles = []\n",
        "        for layer in self.target_layers:\n",
        "            self.handles.append(layer.register_forward_hook(self._forward_hook))\n",
        "            self.handles.append(layer.register_full_backward_hook(self._backward_hook))\n",
        "    \n",
        "    def _forward_hook(self, module, input, output):\n",
        "        self.activations.append(output.detach())\n",
        "    \n",
        "    def _backward_hook(self, module, grad_input, grad_output):\n",
        "        self.gradients.append(grad_output[0].detach())\n",
        "    \n",
        "    def _find_target_layers(self):\n",
        "        # Fallback: use last conv layer in decoder path\n",
        "        layers = []\n",
        "        for name, m in self.model.named_modules():\n",
        "            if isinstance(m, nn.Conv2d) and ('dec' in name or 'up' in name):\n",
        "                layers.append(m)\n",
        "        return layers[-1:] if layers else []\n",
        "    \n",
        "    def generate(self, x: torch.Tensor) -> np.ndarray:\n",
        "        self.activations.clear()\n",
        "        self.gradients.clear()\n",
        "        if not self.target_layers:\n",
        "            self.target_layers = self._find_target_layers()\n",
        "        \n",
        "        x = x.to(self.device).requires_grad_(True)\n",
        "        logits = self.model(x)\n",
        "        # For binary nuclei vs background: take foreground logit (assume channel 1 if 0 is background; fallback to max-prob)\n",
        "        if logits.shape[1] > 1:\n",
        "            target = logits[:, 1, :, :].sum()\n",
        "        else:\n",
        "            target = logits[:, 0, :, :].sum()\n",
        "        self.model.zero_grad(set_to_none=True)\n",
        "        target.backward(retain_graph=False)\n",
        "        \n",
        "        # Use last recorded activation/gradient\n",
        "        A = self.activations[-1]\n",
        "        G = self.gradients[-1]\n",
        "        weights = G.mean(dim=(2,3), keepdim=True)  # GAP over spatial dims\n",
        "        cam = (weights * A).sum(dim=1, keepdim=True)\n",
        "        cam = torch.relu(cam)\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / (cam.max() + 1e-6)\n",
        "        cam_np = cam.squeeze().detach().cpu().numpy()\n",
        "        return cam_np\n",
        "    \n",
        "    def close(self):\n",
        "        for h in self.handles:\n",
        "            h.remove()\n",
        "        self.handles.clear()\n",
        "        self.activations.clear()\n",
        "        self.gradients.clear()\n",
        "\n",
        "# Build wrappers for both models\n",
        "def build_gradcam_wrappers(model_original, model_normalized, device):\n",
        "    def pick_layers(model):\n",
        "        layers = []\n",
        "        for name, m in model.named_modules():\n",
        "            if isinstance(m, nn.Conv2d) and ('dec' in name or 'up' in name or 'classifier' not in name):\n",
        "                layers.append(m)\n",
        "        return layers[-1:] if layers else []\n",
        "    \n",
        "    layers_orig = pick_layers(model_original)\n",
        "    layers_norm = pick_layers(model_normalized)\n",
        "    return (\n",
        "        SegmentationGradCAM(model_original, layers_orig, device),\n",
        "        SegmentationGradCAM(model_normalized, layers_norm, device)\n",
        "    )\n",
        "\n",
        "gradcam_orig, gradcam_norm = build_gradcam_wrappers(model_original, model_normalized, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explainability metrics\n",
        "from skimage.measure import regionprops\n",
        "\n",
        "def cam_energy_in_mask(cam: np.ndarray, mask: np.ndarray) -> float:\n",
        "    cam = (cam - cam.min()) / (cam.max() + 1e-6)\n",
        "    if mask.max() > 1:\n",
        "        mask = (mask > 0).astype(np.uint8)\n",
        "    return float((cam * mask).sum() / (cam.sum() + 1e-6))\n",
        "\n",
        "\n",
        "def pointing_game(cam: np.ndarray, mask: np.ndarray) -> float:\n",
        "    # Hit if argmax of cam falls inside nuclei mask\n",
        "    y, x = np.unravel_index(np.argmax(cam), cam.shape)\n",
        "    return float(mask[y, x] > 0)\n",
        "\n",
        "\n",
        "def cam_iou_threshold(cam: np.ndarray, mask: np.ndarray, thr: float = 0.5) -> float:\n",
        "    cam_bin = (cam >= thr).astype(np.uint8)\n",
        "    mask_bin = (mask > 0).astype(np.uint8)\n",
        "    inter = np.logical_and(cam_bin, mask_bin).sum()\n",
        "    union = np.logical_or(cam_bin, mask_bin).sum()\n",
        "    return float(inter / (union + 1e-6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation loop on a moderate sample size per tissue\n",
        "results = []\n",
        "examples_per_tissue = 20\n",
        "\n",
        "for tissue in sorted(available_tissues):\n",
        "    img_dir = DATASET_TISSUES / tissue / \"test\" / \"images\"\n",
        "    sem_dir = DATASET_TISSUES / tissue / \"test\" / \"sem_masks\"\n",
        "    if not (img_dir.is_dir() and sem_dir.is_dir()):\n",
        "        continue\n",
        "    names = [n.name for n in img_dir.iterdir() if n.suffix == \".png\"]\n",
        "    names = random.sample(names, min(examples_per_tissue, len(names)))\n",
        "    for name in names:\n",
        "        img_path = img_dir / name\n",
        "        sem_path = sem_dir / name.replace(\"img_\", \"sem_\", 1)\n",
        "        # Load\n",
        "        img = PanNukeTissueDataset.default_image_transform(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = PanNukeTissueDataset.default_target_transform(Image.open(sem_path))\n",
        "        img_batch = img.unsqueeze(0)\n",
        "        # GradCAM original\n",
        "        cam_orig = gradcam_orig.generate(img_batch)\n",
        "        # GradCAM normalized inputs (optional: normalize image first)\n",
        "        if normalizer is not None:\n",
        "            rgb_np = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "            norm_rgb = normalizer.transform(rgb_np)\n",
        "            norm_tensor = PanNukeTissueDataset.default_image_transform(Image.fromarray(norm_rgb))\n",
        "            cam_norm = gradcam_norm.generate(norm_tensor.unsqueeze(0))\n",
        "        else:\n",
        "            cam_norm = gradcam_norm.generate(img_batch)\n",
        "        m = mask.numpy()\n",
        "        results.append({\n",
        "            \"tissue\": tissue,\n",
        "            \"image_id\": name,\n",
        "            \"energy_orig\": cam_energy_in_mask(cam_orig, m),\n",
        "            \"energy_norm\": cam_energy_in_mask(cam_norm, m),\n",
        "            \"point_orig\": pointing_game(cam_orig, m),\n",
        "            \"point_norm\": pointing_game(cam_norm, m),\n",
        "            \"iou_orig\": cam_iou_threshold(cam_orig, m, thr=0.5),\n",
        "            \"iou_norm\": cam_iou_threshold(cam_norm, m, thr=0.5),\n",
        "        })\n",
        "\n",
        "res_df = pd.DataFrame(results)\n",
        "res_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paired statistics: Wilcoxon signed-rank tests per metric (H1: normalized > original)\n",
        "stats_rows = []\n",
        "for metric in [\"energy\", \"point\", \"iou\"]:\n",
        "    x = res_df[f\"{metric}_orig\"].values\n",
        "    y = res_df[f\"{metric}_norm\"].values\n",
        "    # Wilcoxon signed-rank for paired samples\n",
        "    stat, p = wilcoxon(y, x, zero_method='wilcox', alternative='greater', method='approx')\n",
        "    diff = y - x\n",
        "    # Effect size: r = Z / sqrt(N); approximate Z from statistic via normal approximation\n",
        "    # scipy returns statistic; we use standardized z from p (one-sided)\n",
        "    from scipy.stats import norm\n",
        "    z = norm.isf(p) if p > 0 else np.inf\n",
        "    r = z / np.sqrt(len(diff))\n",
        "    stats_rows.append({\"metric\": metric, \"n\": len(diff), \"wilcoxon_stat\": stat, \"p_value\": p, \"effect_r\": r, \"mean_diff\": float(diff.mean())})\n",
        "\n",
        "stats_df = pd.DataFrame(stats_rows)\n",
        "stats_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qualitative overlays\n",
        "\n",
        "def overlay_cam_on_image(img_tensor: torch.Tensor, cam: np.ndarray, alpha=0.5):\n",
        "    img = img_tensor.permute(1,2,0).cpu().numpy()\n",
        "    img = np.clip(img * 0.229 + 0.485, 0, 1)\n",
        "    cam_color = plt.cm.inferno(cam)[..., :3]\n",
        "    overlay = (1 - alpha) * img + alpha * cam_color\n",
        "    return np.clip(overlay, 0, 1)\n",
        "\n",
        "# Visualize a few\n",
        "for tissue, entry in sampled_examples[:6]:\n",
        "    img = PanNukeTissueDataset.default_image_transform(Image.open(entry[\"image_path\"]).convert(\"RGB\"))\n",
        "    mask = PanNukeTissueDataset.default_target_transform(Image.open(entry[\"sem_path\"]))\n",
        "    cam_o = gradcam_orig.generate(img.unsqueeze(0))\n",
        "    if normalizer is not None:\n",
        "        rgb_np = np.array(Image.open(entry[\"image_path\"]).convert(\"RGB\"))\n",
        "        norm_rgb = normalizer.transform(rgb_np)\n",
        "        norm_tensor = PanNukeTissueDataset.default_image_transform(Image.fromarray(norm_rgb))\n",
        "        cam_n = gradcam_norm.generate(norm_tensor.unsqueeze(0))\n",
        "    else:\n",
        "        cam_n = gradcam_norm.generate(img.unsqueeze(0))\n",
        "    ov_o = overlay_cam_on_image(img, cam_o)\n",
        "    ov_n = overlay_cam_on_image(img, cam_n)\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
        "    ax[0].imshow(img.permute(1,2,0).cpu().numpy() * 0.229 + 0.485)\n",
        "    ax[0].set_title(f\"{tissue} image\")\n",
        "    ax[0].axis(\"off\")\n",
        "    ax[1].imshow(ov_o)\n",
        "    ax[1].set_title(\"CAM original\")\n",
        "    ax[1].axis(\"off\")\n",
        "    ax[2].imshow(ov_n)\n",
        "    ax[2].set_title(\"CAM normalized\")\n",
        "    ax[2].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Summary and Interpretation\n",
        "We test H1 (normalized > original) for three alignment metrics:\n",
        "- Energy-in-mask: proportion of Grad-CAM energy within nuclei.\n",
        "- Pointing game: whether peak activation falls within nuclei.\n",
        "- IoU@0.5: overlap between binarized CAM and nuclei.\n",
        "\n",
        "We report paired Wilcoxon p-values and effect sizes r.\n",
        "A significant p-value with positive mean difference supports H1 that stain normalization improves Grad-CAM alignment.\n",
        "\n",
        "We also provide qualitative overlays to judge spatial plausibility and potential failure modes (e.g., background leakage or focus on stromal regions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compact results tables and plots\n",
        "summary = (\n",
        "    res_df.assign(\n",
        "        d_energy = res_df.energy_norm - res_df.energy_orig,\n",
        "        d_point = res_df.point_norm - res_df.point_orig,\n",
        "        d_iou = res_df.iou_norm - res_df.iou_orig,\n",
        "    )\n",
        ")\n",
        "\n",
        "summary_by_tissue = summary.groupby(\"tissue\")[\n",
        "    [\"d_energy\", \"d_point\", \"d_iou\"]\n",
        "].agg([\"mean\", \"median\", \"count\"]).round(3)\n",
        "summary_by_tissue\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
        "sns.violinplot(data=summary, y=\"d_energy\", ax=axes[0])\n",
        "axes[0].axhline(0, color='k', ls='--', lw=1)\n",
        "axes[0].set_title(\"Δ Energy (norm - orig)\")\n",
        "sns.violinplot(data=summary, y=\"d_point\", ax=axes[1])\n",
        "axes[1].axhline(0, color='k', ls='--', lw=1)\n",
        "axes[1].set_title(\"Δ Pointing\")\n",
        "sns.violinplot(data=summary, y=\"d_iou\", ax=axes[2])\n",
        "axes[2].axhline(0, color='k', ls='--', lw=1)\n",
        "axes[2].set_title(\"Δ IoU@0.5\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "stats_df.round(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "- We tested whether stain normalization improves Grad-CAM alignment with nuclei masks.\n",
        "- Paired Wilcoxon tests across sampled test images provide statistical evidence (or not) for H1.\n",
        "- Qualitative overlays illustrate spatial behavior and highlight consistency or failure cases.\n",
        "\n",
        "If p-values are significant and deltas are positive, we reject H0 and conclude that Vahadane normalization improves alignment of Grad-CAM explanations with nuclei.\n",
        "Otherwise, we fail to reject H0 for this setup, and we discuss potential reasons: model calibration, target layer choice, or dataset-specific staining variability.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
