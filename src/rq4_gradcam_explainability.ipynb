{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question 4: Grad-CAM Explainability Analysis\n",
    "## U-Net Nuclei Segmentation Interpretability with Stain Normalization\n",
    "\n",
    "**Research Question**: Do lightweight explainability techniquesâ€”like Grad-CAMâ€”enhance interpretability of U-Net-based nuclei segmentation on PanNuke, and does stain normalization improve this further?\n",
    "\n",
    "### Research Hypotheses:\n",
    "- **Hâ‚€ (Null)**: Stain normalization does not improve Grad-CAM alignment\n",
    "- **Hâ‚ (Alternative)**: Stain normalization significantly improves Grad-CAM alignment\n",
    "\n",
    "### Experimental Design:\n",
    "1. **Model Variants**:\n",
    "   - U-Net without Grad-CAM (baseline)\n",
    "   - U-Net with Grad-CAM (original images)\n",
    "   - U-Net with Grad-CAM (stain normalized images)\n",
    "\n",
    "2. **Evaluation Framework**:\n",
    "   - Grad-CAM spatial accuracy metrics\n",
    "   - Biological relevance assessment\n",
    "   - Statistical analysis (ANOVA)\n",
    "   - Visual interpretability comparison\n",
    "\n",
    "3. **Key Metrics**:\n",
    "   - Grad-CAM localization accuracy\n",
    "   - Attention map quality scores\n",
    "   - Segmentation performance correlation\n",
    "   - Clinical interpretability measures\n",
    "\n",
    "### Expected Outcomes:\n",
    "- Quantified improvement in explainability with stain normalization\n",
    "- Statistical validation of Grad-CAM effectiveness\n",
    "- Clinical insights for interpretable AI in histopathology\n",
    "\n",
    "---\n",
    "**ðŸ”¬ Explainability Research | Grad-CAM Analysis | Clinical Interpretability**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt\n",
    "%pip install grad-cam\n",
    "%pip install captum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RQ4 IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score\n",
    "from scipy.stats import wilcoxon, ttest_rel, ttest_ind, shapiro, levene, mannwhitneyu, f_oneway\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.power import ttest_power\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.formula.api import ols\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# Grad-CAM and explainability imports\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, XGradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Saliency, GuidedGradCam\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Set up paths\n",
    "project_root = Path('/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch/')\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import custom modules\n",
    "from preprocessing.vahadane_gpu import GPUVahadaneNormalizer\n",
    "from models.unet_rq3 import UNetRQ3, create_unet_rq3\n",
    "from models.unet import UNet  # Import the original UNet class\n",
    "from utils.metrics import calculate_segmentation_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging and setup\n",
    "log_dir = project_root / 'artifacts' / 'rq4_gradcam' / 'logs'\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_dir / 'rq4_gradcam_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Enhanced style and warnings\n",
    "plt.style.use('seaborn-v0_8') if 'seaborn-v0_8' in plt.style.available else plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸ”¬ RQ4 Grad-CAM Explainability Pipeline initialized on {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RQ4 artifacts directories\n",
    "artifacts_dir = project_root / 'artifacts' / 'rq4_gradcam'\n",
    "subdirs = [\n",
    "    'checkpoints', 'results', 'plots', 'logs',\n",
    "    'gradcam_visualizations', 'attention_maps', 'comparisons',\n",
    "    'statistical_analysis', 'explainability_metrics',\n",
    "    'models/original', 'models/normalized',\n",
    "    'evaluation/baseline', 'evaluation/gradcam_original', 'evaluation/gradcam_normalized'\n",
    "]\n",
    "\n",
    "for subdir in subdirs:\n",
    "    (artifacts_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(\"RQ4 Grad-CAM Explainability Pipeline - Initialized Successfully\")\n",
    "print(f\"ðŸ“ Artifacts directory: {artifacts_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Grad-CAM Implementation for U-Net\n",
    "\n",
    "### 1.1 Custom Grad-CAM Wrapper for Segmentation Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationGradCAM:\n",
    "    \"\"\"\n",
    "    Custom Grad-CAM implementation for U-Net segmentation models.\n",
    "    Adapted for nuclei segmentation with proper attention map generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layers, device='cuda'):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Hook to store gradients and activations\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks for target layers.\"\"\"\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "            \n",
    "        for layer in self.target_layers:\n",
    "            layer.register_forward_hook(forward_hook)\n",
    "            layer.register_backward_hook(backward_hook)\n",
    "    \n",
    "    def generate_cam(self, input_tensor, class_idx=None):\n",
    "        \"\"\"\n",
    "        Generate Class Activation Map for segmentation.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input image tensor (B, C, H, W)\n",
    "            class_idx: Class index for which to generate CAM (None for max activation)\n",
    "        \n",
    "        Returns:\n",
    "            cam: Class Activation Map (H, W)\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        input_tensor.requires_grad_(True)\n",
    "        \n",
    "        # Get model output\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            # For segmentation, use the maximum activation across all classes\n",
    "            class_idx = output.argmax(dim=1)\n",
    "        \n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        if output.dim() == 4:  # Segmentation output (B, C, H, W)\n",
    "            # Create one-hot encoding for the target class\n",
    "            target = torch.zeros_like(output)\n",
    "            target.scatter_(1, class_idx.unsqueeze(1), 1)\n",
    "            loss = (output * target).sum()\n",
    "        else:  # Classification output (B, C)\n",
    "            loss = output[:, class_idx].sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Generate CAM\n",
    "        gradients = self.gradients[0]  # (B, C, H, W)\n",
    "        activations = self.activations[0]  # (B, C, H, W)\n",
    "        \n",
    "        # Global average pooling of gradients\n",
    "        weights = gradients.mean(dim=(2, 3))  # (B, C)\n",
    "        \n",
    "        # Weighted combination of activation maps\n",
    "        cam = torch.zeros(activations.shape[0], activations.shape[2], activations.shape[3])\n",
    "        for i in range(weights.shape[1]):\n",
    "            cam += weights[0, i] * activations[0, i]\n",
    "        \n",
    "        # Apply ReLU and normalize\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / cam.max() if cam.max() > 0 else cam\n",
    "        \n",
    "        return cam.detach().cpu().numpy()\n",
    "    \n",
    "    def generate_attention_map(self, input_tensor, class_idx=None):\n",
    "        \"\"\"\n",
    "        Generate attention map with proper normalization for visualization.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input image tensor (B, C, H, W)\n",
    "            class_idx: Class index for attention map\n",
    "        \n",
    "        Returns:\n",
    "            attention_map: Normalized attention map (H, W)\n",
    "        \"\"\"\n",
    "        cam = self.generate_cam(input_tensor, class_idx)\n",
    "        \n",
    "        # Resize to input image size if needed\n",
    "        if cam.shape != input_tensor.shape[2:]:\n",
    "            cam = cv2.resize(cam, (input_tensor.shape[3], input_tensor.shape[2]))\n",
    "        \n",
    "        return cam\n",
    "\n",
    "class ExplainabilityMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive metrics for evaluating explainability methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def localization_accuracy(cam, ground_truth_mask, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Calculate localization accuracy of CAM with respect to ground truth.\n",
    "        \n",
    "        Args:\n",
    "            cam: Class activation map (H, W)\n",
    "            ground_truth_mask: Binary ground truth mask (H, W)\n",
    "            threshold: Threshold for binarizing CAM\n",
    "        \n",
    "        Returns:\n",
    "            accuracy: Localization accuracy score\n",
    "        \"\"\"\n",
    "        # Binarize CAM\n",
    "        cam_binary = (cam > threshold).astype(np.uint8)\n",
    "        \n",
    "        # Calculate intersection over union\n",
    "        intersection = np.logical_and(cam_binary, ground_truth_mask).sum()\n",
    "        union = np.logical_or(cam_binary, ground_truth_mask).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return intersection / union\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention_consistency(cam1, cam2):\n",
    "        \"\"\"\n",
    "        Calculate consistency between two attention maps.\n",
    "        \n",
    "        Args:\n",
    "            cam1, cam2: Two attention maps to compare (H, W)\n",
    "        \n",
    "        Returns:\n",
    "            consistency: Pearson correlation coefficient\n",
    "        \"\"\"\n",
    "        # Flatten and calculate correlation\n",
    "        cam1_flat = cam1.flatten()\n",
    "        cam2_flat = cam2.flatten()\n",
    "        \n",
    "        correlation = np.corrcoef(cam1_flat, cam2_flat)[0, 1]\n",
    "        return correlation if not np.isnan(correlation) else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def spatial_coherence(cam, window_size=5):\n",
    "        \"\"\"\n",
    "        Calculate spatial coherence of attention map.\n",
    "        \n",
    "        Args:\n",
    "            cam: Attention map (H, W)\n",
    "            window_size: Size of local window for coherence calculation\n",
    "        \n",
    "        Returns:\n",
    "            coherence: Spatial coherence score\n",
    "        \"\"\"\n",
    "        # Apply local variance filter\n",
    "        kernel = np.ones((window_size, window_size)) / (window_size ** 2)\n",
    "        local_mean = cv2.filter2D(cam, -1, kernel)\n",
    "        local_variance = cv2.filter2D(cam**2, -1, kernel) - local_mean**2\n",
    "        \n",
    "        # Coherence is inverse of local variance\n",
    "        coherence = 1.0 / (1.0 + local_variance.mean())\n",
    "        return coherence\n",
    "    \n",
    "    @staticmethod\n",
    "    def biological_relevance(cam, nuclei_mask, background_mask):\n",
    "        \"\"\"\n",
    "        Calculate biological relevance of attention map.\n",
    "        \n",
    "        Args:\n",
    "            cam: Attention map (H, W)\n",
    "            nuclei_mask: Binary mask of nuclei regions (H, W)\n",
    "            background_mask: Binary mask of background regions (H, W)\n",
    "        \n",
    "        Returns:\n",
    "            relevance: Biological relevance score\n",
    "        \"\"\"\n",
    "        # Calculate attention in nuclei vs background\n",
    "        nuclei_attention = cam[nuclei_mask > 0].mean() if nuclei_mask.sum() > 0 else 0\n",
    "        background_attention = cam[background_mask > 0].mean() if background_mask.sum() > 0 else 0\n",
    "        \n",
    "        # Relevance is the ratio of nuclei attention to background attention\n",
    "        if background_attention > 0:\n",
    "            relevance = nuclei_attention / background_attention\n",
    "        else:\n",
    "            relevance = nuclei_attention\n",
    "        \n",
    "        return relevance\n",
    "\n",
    "print(\"âœ… Grad-CAM implementation and explainability metrics loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Model Setup\n",
    "\n",
    "### 2.1 Load Dataset and Identify Top Tissues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET ANALYSIS - TOP 5 TISSUES FOR RQ4\n",
    "# =============================================================================\n",
    "\n",
    "dataset_path = project_root / 'dataset_tissues'\n",
    "logger.info(\"Analyzing dataset for RQ4 Grad-CAM explainability analysis...\")\n",
    "\n",
    "# Count images per tissue and collect all valid pairs\n",
    "tissue_data = {}\n",
    "tissue_counts = {}\n",
    "\n",
    "for tissue_dir in dataset_path.iterdir():\n",
    "    if tissue_dir.is_dir():\n",
    "        tissue_name = tissue_dir.name\n",
    "        all_pairs = []\n",
    "        \n",
    "        # Collect all valid image-mask pairs from all splits\n",
    "        for split in ['train', 'test', 'val']:\n",
    "            images_dir = tissue_dir / split / 'images'\n",
    "            masks_dir = tissue_dir / split / 'sem_masks'  # Semantic masks\n",
    "            \n",
    "            if images_dir.exists() and masks_dir.exists():\n",
    "                image_files = list(images_dir.glob('*.png'))\n",
    "                \n",
    "                for img_file in image_files:\n",
    "                    mask_file = masks_dir / img_file.name.replace('img_', 'sem_')\n",
    "                    if mask_file.exists():\n",
    "                        all_pairs.append((img_file, mask_file))\n",
    "        \n",
    "        tissue_data[tissue_name] = all_pairs\n",
    "        tissue_counts[tissue_name] = len(all_pairs)\n",
    "\n",
    "# Sort tissues by count and select top 5\n",
    "top_5_tissues = sorted(tissue_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "selected_tissues = [tissue for tissue, count in top_5_tissues]\n",
    "\n",
    "print(\"ðŸ” RQ4 Dataset Analysis Results:\")\n",
    "print(\"=\" * 60)\n",
    "for tissue, count in sorted(tissue_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    marker = \"âœ…\" if tissue in selected_tissues else \"  \"\n",
    "    print(f\"{marker} {tissue:15}: {count:,} images\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Selected Top 5 Tissues for RQ4 Grad-CAM Analysis:\")\n",
    "for i, (tissue, count) in enumerate(top_5_tissues, 1):\n",
    "    print(f\"{i}. {tissue}: {count:,} images\")\n",
    "\n",
    "total_selected = sum(count for _, count in top_5_tissues)\n",
    "print(f\"\\nðŸ“Š Total images in top 5 tissues: {total_selected:,}\")\n",
    "\n",
    "# Configuration for RQ4 analysis\n",
    "TESTING_MODE = False  # Set to False for full analysis\n",
    "SAMPLE_SIZE_PER_TISSUE = 20 if TESTING_MODE else None  # Sample for explainability analysis\n",
    "\n",
    "print(f\"\\nðŸ§ª Running in {'TESTING' if TESTING_MODE else 'PRODUCTION'} mode\")\n",
    "if TESTING_MODE:\n",
    "    print(f\"   Sample size: {SAMPLE_SIZE_PER_TISSUE} images per tissue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATALOADER CREATION AND MODEL SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Create dataloaders for evaluation (these were missing)\n",
    "def create_dataloaders(tissue_pairs, batch_size=8, num_workers=2):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders from tissue pairs.\n",
    "    \"\"\"\n",
    "    # Split data into train/val/test (80/10/10)\n",
    "    total_samples = len(tissue_pairs)\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    val_size = int(0.1 * total_samples)\n",
    "    \n",
    "    # Shuffle data\n",
    "    import random\n",
    "    random.shuffle(tissue_pairs)\n",
    "    \n",
    "    # Split data\n",
    "    train_pairs = tissue_pairs[:train_size]\n",
    "    val_pairs = tissue_pairs[train_size:train_size + val_size]\n",
    "    test_pairs = tissue_pairs[train_size + val_size:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = RQ4Dataset(train_pairs, transform=transform, normalize=False)\n",
    "    val_dataset = RQ4Dataset(val_pairs, transform=transform, normalize=False)\n",
    "    test_dataset = RQ4Dataset(test_pairs, transform=transform, normalize=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "# Create dataloaders for the top 5 tissues\n",
    "print(\"ðŸ”„ Creating dataloaders for RQ4 analysis...\")\n",
    "\n",
    "# Combine all tissue pairs for dataloader creation\n",
    "all_tissue_pairs = []\n",
    "for tissue in selected_tissues:\n",
    "    tissue_pairs = tissue_data[tissue]\n",
    "    if TESTING_MODE and SAMPLE_SIZE_PER_TISSUE:\n",
    "        tissue_pairs = tissue_pairs[:SAMPLE_SIZE_PER_TISSUE]\n",
    "    all_tissue_pairs.extend(tissue_pairs)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(\n",
    "    all_tissue_pairs, batch_size=8, num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataloaders created:\")\n",
    "print(f\"   â€¢ Train: {len(train_dataloader.dataset)} samples\")\n",
    "print(f\"   â€¢ Validation: {len(val_dataloader.dataset)} samples\") \n",
    "print(f\"   â€¢ Test: {len(test_dataloader.dataset)} samples\")\n",
    "\n",
    "# Create model instances for different configurations\n",
    "print(\"\\nðŸ”„ Creating model instances...\")\n",
    "\n",
    "# Create UNet model for baseline comparison\n",
    "baseline_model = UNet(in_channels=3, num_classes=6).to(device)\n",
    "total_params = sum(p.numel() for p in baseline_model.parameters())\n",
    "print(f\"âœ… Baseline UNet model created: {total_params:,} parameters\")\n",
    "\n",
    "# Create UNetRQ3 models for Grad-CAM analysis\n",
    "original_model = create_unet_rq3(n_channels=3, n_classes=1, device=device, verbose=False)\n",
    "normalized_model = create_unet_rq3(n_channels=3, n_classes=1, device=device, verbose=False)\n",
    "\n",
    "print(f\"âœ… RQ3 models created:\")\n",
    "orig_params = sum(p.numel() for p in original_model.parameters())\n",
    "norm_params = sum(p.numel() for p in normalized_model.parameters())\n",
    "print(f\"   â€¢ Original: {orig_params:,} parameters\")\n",
    "print(f\"   â€¢ Normalized: {norm_params:,} parameters\")\n",
    "\n",
    "print(\"\\nâœ… All models and dataloaders ready for RQ4 analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM DATASET CLASS FOR RQ4 GRAD-CAM ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "class RQ4Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for RQ4 Grad-CAM explainability analysis.\n",
    "    Handles both original and normalized images with proper preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tissue_pairs, transform=None, normalize=False, normalizer=None):\n",
    "        self.tissue_pairs = tissue_pairs\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.normalizer = normalizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tissue_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.tissue_pairs[idx]\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Apply stain normalization if requested\n",
    "        if self.normalize and self.normalizer is not None:\n",
    "            try:\n",
    "                image = self.normalizer.normalize(image)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Normalization failed for {img_path}: {e}\")\n",
    "                # Use original image if normalization fails\n",
    "        \n",
    "        # Convert to PIL for transforms\n",
    "        image = Image.fromarray(image)\n",
    "        mask = Image.fromarray(mask)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        # Convert mask to binary (nuclei vs background)\n",
    "        mask = (mask > 0).float()\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'image_path': str(img_path),\n",
    "            'mask_path': str(mask_path)\n",
    "        }\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"âœ… RQ4 Dataset class and transforms defined successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Loading and Grad-CAM Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL LOADING AND GRAD-CAM SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def load_pretrained_model(model_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a pretrained U-Net model for RQ4 analysis.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model checkpoint\n",
    "        device: Device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded U-Net model\n",
    "    \"\"\"\n",
    "    # Create model architecture\n",
    "    model = create_unet_rq3(n_channels=3, n_classes=1)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logger.info(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        logger.warning(f\"Model checkpoint not found at {model_path}. Using random weights.\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def setup_gradcam_models():\n",
    "    \"\"\"\n",
    "    Setup Grad-CAM for different model variants.\n",
    "    \n",
    "    Returns:\n",
    "        models: Dictionary containing different model configurations\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Try to load pretrained models from RQ3 artifacts\n",
    "    original_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_original.pth'\n",
    "    normalized_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_normalized.pth'\n",
    "    \n",
    "    # Load or create models\n",
    "    if original_model_path.exists():\n",
    "        models['original'] = load_pretrained_model(original_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained original model\")\n",
    "    else:\n",
    "        models['original'] = create_unet_rq3(n_channels=3, n_classes=1).to(device)\n",
    "        logger.warning(\"âš ï¸ Using random weights for original model\")\n",
    "    \n",
    "    if normalized_model_path.exists():\n",
    "        models['normalized'] = load_pretrained_model(normalized_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained normalized model\")\n",
    "    else:\n",
    "        models['normalized'] = create_unet_rq3(n_channels=3, n_classes=1).to(device)\n",
    "        logger.warning(\"âš ï¸ Using random weights for normalized model\")\n",
    "    \n",
    "    # Setup Grad-CAM for each model\n",
    "    # Target the last convolutional layer in the decoder\n",
    "    # Create a list of model names to avoid dictionary modification during iteration\n",
    "    model_names = list(models.keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        model = models[model_name]\n",
    "        # Find the target layer (last conv layer in decoder)\n",
    "        target_layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if 'decoder' in name and isinstance(module, nn.Conv2d):\n",
    "                target_layers.append(module)\n",
    "        \n",
    "        if not target_layers:\n",
    "            # Fallback to any conv layer\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    target_layers.append(module)\n",
    "                    break\n",
    "        \n",
    "        if target_layers:\n",
    "            models[f'{model_name}_gradcam'] = SegmentationGradCAM(\n",
    "                model, target_layers, device\n",
    "            )\n",
    "            logger.info(f\"âœ… Setup Grad-CAM for {model_name} model\")\n",
    "        else:\n",
    "            logger.error(f\"âŒ Could not find target layers for {model_name} model\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Initialize models\n",
    "print(\"ðŸ”„ Setting up models for RQ4 Grad-CAM analysis...\")\n",
    "models = setup_gradcam_models()\n",
    "\n",
    "print(f\"\\nðŸ“Š Available model configurations:\")\n",
    "for key in models.keys():\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "# Initialize stain normalizer\n",
    "normalizer = GPUVahadaneNormalizer(device=device)\n",
    "print(f\"\\nâœ… Stain normalizer initialized on {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Framework for Grad-CAM Analysis\n",
    "\n",
    "### 3.1 Comprehensive Visualization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRECTED MODEL LOADING AND GRAD-CAM SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def load_pretrained_model(model_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a pretrained U-Net model for RQ4 analysis.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model checkpoint\n",
    "        device: Device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded U-Net model\n",
    "    \"\"\"\n",
    "    # Create model architecture with correct parameters\n",
    "    model = create_unet_rq3(n_channels=3, n_classes=1)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logger.info(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        logger.warning(f\"Model checkpoint not found at {model_path}. Using random weights.\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def setup_gradcam_models():\n",
    "    \"\"\"\n",
    "    Setup Grad-CAM for different model variants.\n",
    "    \n",
    "    Returns:\n",
    "        models: Dictionary containing different model configurations\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Try to load pretrained models from RQ3 artifacts\n",
    "    original_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_original.pth'\n",
    "    normalized_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_normalized.pth'\n",
    "    \n",
    "    # Load or create models\n",
    "    if original_model_path.exists():\n",
    "        models['original'] = load_pretrained_model(original_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained original model\")\n",
    "    else:\n",
    "        # Use the already created model\n",
    "        models['original'] = original_model\n",
    "        logger.warning(\"âš ï¸ Using random weights for original model\")\n",
    "    \n",
    "    if normalized_model_path.exists():\n",
    "        models['normalized'] = load_pretrained_model(normalized_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained normalized model\")\n",
    "    else:\n",
    "        # Use the already created model\n",
    "        models['normalized'] = normalized_model\n",
    "        logger.warning(\"âš ï¸ Using random weights for normalized model\")\n",
    "    \n",
    "    # Add baseline model\n",
    "    models['baseline'] = baseline_model\n",
    "    \n",
    "    # Setup Grad-CAM for each model\n",
    "    # Target the last convolutional layer in the decoder\n",
    "    # Create a list of model names to avoid dictionary modification during iteration\n",
    "    model_names = list(models.keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        if model_name == 'baseline':\n",
    "            continue  # Skip baseline for Grad-CAM setup\n",
    "            \n",
    "        model = models[model_name]\n",
    "        # Find the target layer (last conv layer in decoder)\n",
    "        target_layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if 'decoder' in name and isinstance(module, nn.Conv2d):\n",
    "                target_layers.append(module)\n",
    "        \n",
    "        if not target_layers:\n",
    "            # Fallback to any conv layer\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    target_layers.append(module)\n",
    "                    break\n",
    "        \n",
    "        if target_layers:\n",
    "            models[f'{model_name}_gradcam'] = SegmentationGradCAM(\n",
    "                model, target_layers, device\n",
    "            )\n",
    "            logger.info(f\"âœ… Setup Grad-CAM for {model_name} model\")\n",
    "        else:\n",
    "            logger.error(f\"âŒ Could not find target layers for {model_name} model\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Initialize models with corrected parameters\n",
    "print(\"ðŸ”„ Setting up models for RQ4 Grad-CAM analysis...\")\n",
    "models = setup_gradcam_models()\n",
    "\n",
    "print(f\"\\nðŸ“Š Available model configurations:\")\n",
    "for key in models.keys():\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "# Initialize stain normalizer\n",
    "normalizer = GPUVahadaneNormalizer(device=device)\n",
    "print(f\"\\nâœ… Stain normalizer initialized on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRECTED MODEL LOADING AND GRAD-CAM SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def load_pretrained_model(model_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a pretrained U-Net model for RQ4 analysis.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model checkpoint\n",
    "        device: Device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded U-Net model\n",
    "    \"\"\"\n",
    "    # Create model architecture with correct parameters\n",
    "    model = create_unet_rq3(n_channels=3, n_classes=1)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logger.info(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        logger.warning(f\"Model checkpoint not found at {model_path}. Using random weights.\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def setup_gradcam_models():\n",
    "    \"\"\"\n",
    "    Setup Grad-CAM for different model variants.\n",
    "    \n",
    "    Returns:\n",
    "        models: Dictionary containing different model configurations\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Try to load pretrained models from RQ3 artifacts\n",
    "    original_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_original.pth'\n",
    "    normalized_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_normalized.pth'\n",
    "    \n",
    "    # Load or create models\n",
    "    if original_model_path.exists():\n",
    "        models['original'] = load_pretrained_model(original_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained original model\")\n",
    "    else:\n",
    "        models['original'] = create_unet_rq3(n_channels=3, n_classes=1).to(device)\n",
    "        logger.warning(\"âš ï¸ Using random weights for original model\")\n",
    "    \n",
    "    if normalized_model_path.exists():\n",
    "        models['normalized'] = load_pretrained_model(normalized_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained normalized model\")\n",
    "    else:\n",
    "        models['normalized'] = create_unet_rq3(n_channels=3, n_classes=1).to(device)\n",
    "        logger.warning(\"âš ï¸ Using random weights for normalized model\")\n",
    "    \n",
    "    # Setup Grad-CAM for each model\n",
    "    # Target the last convolutional layer in the decoder\n",
    "    for model_name, model in models.items():\n",
    "        # Find the target layer (last conv layer in decoder)\n",
    "        target_layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if 'decoder' in name and isinstance(module, nn.Conv2d):\n",
    "                target_layers.append(module)\n",
    "        \n",
    "        if not target_layers:\n",
    "            # Fallback to any conv layer\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    target_layers.append(module)\n",
    "                    break\n",
    "        \n",
    "        if target_layers:\n",
    "            models[f'{model_name}_gradcam'] = SegmentationGradCAM(\n",
    "                model, target_layers, device\n",
    "            )\n",
    "            logger.info(f\"âœ… Setup Grad-CAM for {model_name} model\")\n",
    "        else:\n",
    "            logger.error(f\"âŒ Could not find target layers for {model_name} model\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Initialize models with corrected parameters\n",
    "print(\"ðŸ”„ Setting up models for RQ4 Grad-CAM analysis...\")\n",
    "models = setup_gradcam_models()\n",
    "\n",
    "print(f\"\\nðŸ“Š Available model configurations:\")\n",
    "for key in models.keys():\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "# Initialize stain normalizer\n",
    "normalizer = GPUVahadaneNormalizer(device=device)\n",
    "print(f\"\\nâœ… Stain normalizer initialized on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIXED MODEL LOADING AND GRAD-CAM SETUP (CORRECTED VERSION)\n",
    "# =============================================================================\n",
    "\n",
    "def setup_gradcam_models_fixed():\n",
    "    \"\"\"\n",
    "    Setup Grad-CAM for different model variants - FIXED VERSION.\n",
    "    \n",
    "    Returns:\n",
    "        models: Dictionary containing different model configurations\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Try to load pretrained models from RQ3 artifacts\n",
    "    original_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_original.pth'\n",
    "    normalized_model_path = project_root / 'artifacts' / 'rq3_enhanced' / 'checkpoints' / 'best_model_normalized.pth'\n",
    "    \n",
    "    # Load or create models\n",
    "    if original_model_path.exists():\n",
    "        models['original'] = load_pretrained_model(original_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained original model\")\n",
    "    else:\n",
    "        # Use the already created model\n",
    "        models['original'] = original_model\n",
    "        logger.warning(\"âš ï¸ Using random weights for original model\")\n",
    "    \n",
    "    if normalized_model_path.exists():\n",
    "        models['normalized'] = load_pretrained_model(normalized_model_path, device)\n",
    "        logger.info(\"âœ… Loaded pretrained normalized model\")\n",
    "    else:\n",
    "        # Use the already created model\n",
    "        models['normalized'] = normalized_model\n",
    "        logger.warning(\"âš ï¸ Using random weights for normalized model\")\n",
    "    \n",
    "    # Add baseline model\n",
    "    models['baseline'] = baseline_model\n",
    "    \n",
    "    # Setup Grad-CAM for each model\n",
    "    # Target the last convolutional layer in the decoder\n",
    "    # Create a list of model names to avoid dictionary modification during iteration\n",
    "    model_names = list(models.keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        if model_name == 'baseline':\n",
    "            continue  # Skip baseline for Grad-CAM setup\n",
    "            \n",
    "        model = models[model_name]\n",
    "        # Find the target layer (last conv layer in decoder)\n",
    "        target_layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if 'decoder' in name and isinstance(module, nn.Conv2d):\n",
    "                target_layers.append(module)\n",
    "        \n",
    "        if not target_layers:\n",
    "            # Fallback to any conv layer\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    target_layers.append(module)\n",
    "                    break\n",
    "        \n",
    "        if target_layers:\n",
    "            models[f'{model_name}_gradcam'] = SegmentationGradCAM(\n",
    "                model, target_layers, device\n",
    "            )\n",
    "            logger.info(f\"âœ… Setup Grad-CAM for {model_name} model\")\n",
    "        else:\n",
    "            logger.error(f\"âŒ Could not find target layers for {model_name} model\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Initialize models with the FIXED function\n",
    "print(\"ðŸ”„ Setting up models for RQ4 Grad-CAM analysis (FIXED VERSION)...\")\n",
    "models = setup_gradcam_models_fixed()\n",
    "\n",
    "print(f\"\\nðŸ“Š Available model configurations:\")\n",
    "for key in models.keys():\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "# Initialize stain normalizer\n",
    "normalizer = GPUVahadaneNormalizer(device=device)\n",
    "print(f\"\\nâœ… Stain normalizer initialized on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRECTED EVALUATION PIPELINE FOR RQ4\n",
    "# =============================================================================\n",
    "\n",
    "class RQ4Evaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation framework for RQ4 Grad-CAM explainability analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, normalizer, device='cuda'):\n",
    "        self.models = models\n",
    "        self.normalizer = normalizer\n",
    "        self.device = device\n",
    "        self.metrics = ExplainabilityMetrics()\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {\n",
    "            'baseline': [],\n",
    "            'gradcam_original': [],\n",
    "            'gradcam_normalized': []\n",
    "        }\n",
    "    \n",
    "    def evaluate_single_image(self, image, mask, tissue_type, image_path):\n",
    "        \"\"\"\n",
    "        Evaluate a single image across all model variants.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor (C, H, W)\n",
    "            mask: Ground truth mask tensor (H, W)\n",
    "            tissue_type: Type of tissue\n",
    "            image_path: Path to the image file\n",
    "        \n",
    "        Returns:\n",
    "            results: Dictionary containing evaluation results\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'tissue_type': tissue_type,\n",
    "            'image_path': image_path,\n",
    "            'image_id': Path(image_path).stem\n",
    "        }\n",
    "        \n",
    "        # Prepare input\n",
    "        image_batch = image.unsqueeze(0).to(self.device)\n",
    "        mask_np = mask.numpy()\n",
    "        \n",
    "        # Create background mask (inverse of nuclei mask)\n",
    "        background_mask = (mask_np == 0).astype(np.uint8)\n",
    "        nuclei_mask = (mask_np > 0).astype(np.uint8)\n",
    "        \n",
    "        # 1. Baseline evaluation (without Grad-CAM)\n",
    "        with torch.no_grad():\n",
    "            baseline_pred = self.models['baseline'](image_batch)\n",
    "            baseline_pred = torch.sigmoid(baseline_pred).squeeze().cpu().numpy()\n",
    "            baseline_pred_binary = (baseline_pred > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Calculate baseline metrics\n",
    "        baseline_iou = self.metrics.localization_accuracy(baseline_pred, mask_np)\n",
    "        baseline_dice = f1_score(mask_np.flatten(), baseline_pred_binary.flatten(), average='binary')\n",
    "        \n",
    "        results['baseline'] = {\n",
    "            'iou': baseline_iou,\n",
    "            'dice': baseline_dice,\n",
    "            'prediction': baseline_pred\n",
    "        }\n",
    "        \n",
    "        # 2. Grad-CAM with original model\n",
    "        try:\n",
    "            original_cam = self.models['original_gradcam'].generate_attention_map(image_batch)\n",
    "            \n",
    "            # Calculate explainability metrics\n",
    "            orig_localization = self.metrics.localization_accuracy(original_cam, mask_np)\n",
    "            orig_biological = self.metrics.biological_relevance(original_cam, nuclei_mask, background_mask)\n",
    "            orig_coherence = self.metrics.spatial_coherence(original_cam)\n",
    "            \n",
    "            results['gradcam_original'] = {\n",
    "                'localization_accuracy': orig_localization,\n",
    "                'biological_relevance': orig_biological,\n",
    "                'spatial_coherence': orig_coherence,\n",
    "                'attention_map': original_cam,\n",
    "                'iou': baseline_iou,  # Same segmentation performance\n",
    "                'dice': baseline_dice\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Grad-CAM evaluation failed for original model: {e}\")\n",
    "            results['gradcam_original'] = None\n",
    "        \n",
    "        # 3. Grad-CAM with normalized model\n",
    "        try:\n",
    "            # Apply stain normalization\n",
    "            image_denorm = self.denormalize_image(image)\n",
    "            image_np = image_denorm.permute(1, 2, 0).numpy()\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "            \n",
    "            # Normalize image\n",
    "            normalized_image = self.normalizer.normalize(image_np)\n",
    "            normalized_image = Image.fromarray(normalized_image)\n",
    "            normalized_image = transform(normalized_image)\n",
    "            normalized_image_batch = normalized_image.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Generate Grad-CAM\n",
    "            normalized_cam = self.models['normalized_gradcam'].generate_attention_map(normalized_image_batch)\n",
    "            \n",
    "            # Calculate explainability metrics\n",
    "            norm_localization = self.metrics.localization_accuracy(normalized_cam, mask_np)\n",
    "            norm_biological = self.metrics.biological_relevance(normalized_cam, nuclei_mask, background_mask)\n",
    "            norm_coherence = self.metrics.spatial_coherence(normalized_cam)\n",
    "            \n",
    "            # Calculate attention consistency with original\n",
    "            if results['gradcam_original'] is not None:\n",
    "                consistency = self.metrics.attention_consistency(original_cam, normalized_cam)\n",
    "            else:\n",
    "                consistency = 0.0\n",
    "            \n",
    "            results['gradcam_normalized'] = {\n",
    "                'localization_accuracy': norm_localization,\n",
    "                'biological_relevance': norm_biological,\n",
    "                'spatial_coherence': norm_coherence,\n",
    "                'attention_consistency': consistency,\n",
    "                'attention_map': normalized_cam,\n",
    "                'iou': baseline_iou,  # Same segmentation performance\n",
    "                'dice': baseline_dice\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Grad-CAM evaluation failed for normalized model: {e}\")\n",
    "            results['gradcam_normalized'] = None\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def denormalize_image(self, tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        \"\"\"Denormalize tensor image for visualization.\"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, mean, std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "    def evaluate_dataset(self, dataset, max_samples=None):\n",
    "        \"\"\"\n",
    "        Evaluate entire dataset across all model variants.\n",
    "        \n",
    "        Args:\n",
    "            dataset: RQ4Dataset instance\n",
    "            max_samples: Maximum number of samples to evaluate\n",
    "        \n",
    "        Returns:\n",
    "            results: Comprehensive evaluation results\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting comprehensive evaluation of {len(dataset)} samples...\")\n",
    "        \n",
    "        num_samples = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "        \n",
    "        for i in tqdm(range(num_samples), desc=\"Evaluating samples\"):\n",
    "            try:\n",
    "                sample = dataset[i]\n",
    "                image = sample['image']\n",
    "                mask = sample['mask']\n",
    "                image_path = sample['image_path']\n",
    "                \n",
    "                # Extract tissue type from path\n",
    "                tissue_type = Path(image_path).parent.parent.parent.name\n",
    "                \n",
    "                # Evaluate single image\n",
    "                results = self.evaluate_single_image(image, mask, tissue_type, image_path)\n",
    "                \n",
    "                # Store results\n",
    "                self.results['baseline'].append(results['baseline'])\n",
    "                if results['gradcam_original']:\n",
    "                    self.results['gradcam_original'].append(results['gradcam_original'])\n",
    "                if results['gradcam_normalized']:\n",
    "                    self.results['gradcam_normalized'].append(results['gradcam_normalized'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Evaluation failed for sample {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Evaluation completed. Processed {len(self.results['baseline'])} samples.\")\n",
    "        return self.results\n",
    "    \n",
    "    def generate_summary_statistics(self):\n",
    "        \"\"\"Generate summary statistics for all model variants.\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for model_type, results in self.results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "                \n",
    "            # Convert to DataFrame for easier analysis\n",
    "            df = pd.DataFrame(results)\n",
    "            \n",
    "            summary[model_type] = {\n",
    "                'count': len(df),\n",
    "                'localization_accuracy': {\n",
    "                    'mean': df['localization_accuracy'].mean() if 'localization_accuracy' in df.columns else None,\n",
    "                    'std': df['localization_accuracy'].std() if 'localization_accuracy' in df.columns else None,\n",
    "                    'median': df['localization_accuracy'].median() if 'localization_accuracy' in df.columns else None\n",
    "                },\n",
    "                'biological_relevance': {\n",
    "                    'mean': df['biological_relevance'].mean() if 'biological_relevance' in df.columns else None,\n",
    "                    'std': df['biological_relevance'].std() if 'biological_relevance' in df.columns else None,\n",
    "                    'median': df['biological_relevance'].median() if 'biological_relevance' in df.columns else None\n",
    "                },\n",
    "                'spatial_coherence': {\n",
    "                    'mean': df['spatial_coherence'].mean() if 'spatial_coherence' in df.columns else None,\n",
    "                    'std': df['spatial_coherence'].std() if 'spatial_coherence' in df.columns else None,\n",
    "                    'median': df['spatial_coherence'].median() if 'spatial_coherence' in df.columns else None\n",
    "                },\n",
    "                'attention_consistency': {\n",
    "                    'mean': df['attention_consistency'].mean() if 'attention_consistency' in df.columns else None,\n",
    "                    'std': df['attention_consistency'].std() if 'attention_consistency' in df.columns else None,\n",
    "                    'median': df['attention_consistency'].median() if 'attention_consistency' in df.columns else None\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RQ4Evaluator(models, normalizer, device)\n",
    "print(\"âœ… RQ4 evaluation framework initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION FRAMEWORK FOR GRAD-CAM ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "class GradCAMVisualizer:\n",
    "    \"\"\"\n",
    "    Comprehensive visualization framework for Grad-CAM analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def denormalize_image(self, tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        \"\"\"Denormalize tensor image for visualization.\"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, mean, std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "    def create_gradcam_overlay(self, image, cam, alpha=0.4):\n",
    "        \"\"\"Create Grad-CAM overlay on original image.\"\"\"\n",
    "        # Convert image to numpy\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = self.denormalize_image(image)\n",
    "            image = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Normalize CAM to 0-1\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        # Create heatmap\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize heatmap to match image\n",
    "        if heatmap.shape[:2] != image.shape[:2]:\n",
    "            heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "        \n",
    "        # Create overlay\n",
    "        overlay = cv2.addWeighted(image, 1-alpha, heatmap, alpha, 0)\n",
    "        return overlay, heatmap\n",
    "    \n",
    "    def plot_comparison_grid(self, images, masks, cams, titles, save_path=None):\n",
    "        \"\"\"Create comparison grid showing original, mask, and Grad-CAM.\"\"\"\n",
    "        n_images = len(images)\n",
    "        fig, axes = plt.subplots(n_images, 4, figsize=(16, 4*n_images))\n",
    "        if n_images == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i in range(n_images):\n",
    "            # Original image\n",
    "            img = self.denormalize_image(images[i]) if isinstance(images[i], torch.Tensor) else images[i]\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img = img.permute(1, 2, 0).numpy()\n",
    "            \n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f\"{titles[i]}\\nOriginal Image\")\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Ground truth mask\n",
    "            mask = masks[i].numpy() if isinstance(masks[i], torch.Tensor) else masks[i]\n",
    "            axes[i, 1].imshow(mask, cmap='gray')\n",
    "            axes[i, 1].set_title(\"Ground Truth\\nMask\")\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Grad-CAM heatmap\n",
    "            cam = cams[i]\n",
    "            im = axes[i, 2].imshow(cam, cmap='jet')\n",
    "            axes[i, 2].set_title(\"Grad-CAM\\nHeatmap\")\n",
    "            axes[i, 2].axis('off')\n",
    "            plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Overlay\n",
    "            overlay, _ = self.create_gradcam_overlay(img, cam)\n",
    "            axes[i, 3].imshow(overlay)\n",
    "            axes[i, 3].set_title(\"Grad-CAM\\nOverlay\")\n",
    "            axes[i, 3].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_attention_comparison(self, original_cam, normalized_cam, image, save_path=None):\n",
    "        \"\"\"Compare attention maps between original and normalized models.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Denormalize image\n",
    "        img = self.denormalize_image(image) if isinstance(image, torch.Tensor) else image\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, 0].imshow(img)\n",
    "        axes[0, 0].set_title(\"Original Image\")\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Original Grad-CAM\n",
    "        overlay_orig, heatmap_orig = self.create_gradcam_overlay(img, original_cam)\n",
    "        axes[0, 1].imshow(overlay_orig)\n",
    "        axes[0, 1].set_title(\"Original Model\\nGrad-CAM\")\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Original heatmap\n",
    "        im1 = axes[0, 2].imshow(original_cam, cmap='jet')\n",
    "        axes[0, 2].set_title(\"Original Model\\nHeatmap\")\n",
    "        axes[0, 2].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[0, 2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Normalized image (if different)\n",
    "        axes[1, 0].imshow(img)  # Same image for now\n",
    "        axes[1, 0].set_title(\"Normalized Image\")\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Normalized Grad-CAM\n",
    "        overlay_norm, heatmap_norm = self.create_gradcam_overlay(img, normalized_cam)\n",
    "        axes[1, 1].imshow(overlay_norm)\n",
    "        axes[1, 1].set_title(\"Normalized Model\\nGrad-CAM\")\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        # Normalized heatmap\n",
    "        im2 = axes[1, 2].imshow(normalized_cam, cmap='jet')\n",
    "        axes[1, 2].set_title(\"Normalized Model\\nHeatmap\")\n",
    "        axes[1, 2].axis('off')\n",
    "        plt.colorbar(im2, ax=axes[1, 2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics_comparison(self, metrics_data, save_path=None):\n",
    "        \"\"\"Plot comparison of explainability metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Localization accuracy\n",
    "        sns.boxplot(data=metrics_data, x='model_type', y='localization_accuracy', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Localization Accuracy')\n",
    "        axes[0, 0].set_ylabel('IoU Score')\n",
    "        \n",
    "        # Biological relevance\n",
    "        sns.boxplot(data=metrics_data, x='model_type', y='biological_relevance', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Biological Relevance')\n",
    "        axes[0, 1].set_ylabel('Relevance Score')\n",
    "        \n",
    "        # Spatial coherence\n",
    "        sns.boxplot(data=metrics_data, x='model_type', y='spatial_coherence', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Spatial Coherence')\n",
    "        axes[1, 0].set_ylabel('Coherence Score')\n",
    "        \n",
    "        # Attention consistency\n",
    "        sns.boxplot(data=metrics_data, x='model_type', y='attention_consistency', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Attention Consistency')\n",
    "        axes[1, 1].set_ylabel('Correlation Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = GradCAMVisualizer(artifacts_dir / 'gradcam_visualizations')\n",
    "print(\"âœ… Grad-CAM visualization framework initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Pipeline for RQ4\n",
    "\n",
    "### 4.1 Comprehensive Evaluation Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE EVALUATION PIPELINE FOR RQ4\n",
    "# =============================================================================\n",
    "\n",
    "class RQ4Evaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation framework for RQ4 Grad-CAM explainability analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, normalizer, device='cuda'):\n",
    "        self.models = models\n",
    "        self.normalizer = normalizer\n",
    "        self.device = device\n",
    "        self.metrics = ExplainabilityMetrics()\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {\n",
    "            'baseline': [],\n",
    "            'gradcam_original': [],\n",
    "            'gradcam_normalized': []\n",
    "        }\n",
    "    \n",
    "    def evaluate_single_image(self, image, mask, tissue_type, image_path):\n",
    "        \"\"\"\n",
    "        Evaluate a single image across all model variants.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor (C, H, W)\n",
    "            mask: Ground truth mask tensor (H, W)\n",
    "            tissue_type: Type of tissue\n",
    "            image_path: Path to the image file\n",
    "        \n",
    "        Returns:\n",
    "            results: Dictionary containing evaluation results\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'tissue_type': tissue_type,\n",
    "            'image_path': image_path,\n",
    "            'image_id': Path(image_path).stem\n",
    "        }\n",
    "        \n",
    "        # Prepare input\n",
    "        image_batch = image.unsqueeze(0).to(self.device)\n",
    "        mask_np = mask.numpy()\n",
    "        \n",
    "        # Create background mask (inverse of nuclei mask)\n",
    "        background_mask = (mask_np == 0).astype(np.uint8)\n",
    "        nuclei_mask = (mask_np > 0).astype(np.uint8)\n",
    "        \n",
    "        # 1. Baseline evaluation (without Grad-CAM)\n",
    "        with torch.no_grad():\n",
    "            baseline_pred = self.models['baseline'](image_batch)\n",
    "            # Handle different output shapes - baseline model outputs (B, C, H, W)\n",
    "            if baseline_pred.dim() == 4:\n",
    "                # Take the first channel if multi-channel output\n",
    "                baseline_pred = baseline_pred[:, 0, :, :]  # (B, H, W)\n",
    "            baseline_pred = torch.sigmoid(baseline_pred).squeeze().cpu().numpy()\n",
    "            baseline_pred_binary = (baseline_pred > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Calculate baseline metrics\n",
    "        baseline_iou = self.metrics.localization_accuracy(baseline_pred, mask_np)\n",
    "        baseline_dice = f1_score(mask_np.flatten(), baseline_pred_binary.flatten(), average='binary')\n",
    "        \n",
    "        results['baseline'] = {\n",
    "            'iou': baseline_iou,\n",
    "            'dice': baseline_dice,\n",
    "            'prediction': baseline_pred\n",
    "        }\n",
    "        \n",
    "        # 2. Grad-CAM with original model\n",
    "        try:\n",
    "            original_cam = self.models['original_gradcam'].generate_attention_map(image_batch)\n",
    "            \n",
    "            # Calculate explainability metrics\n",
    "            orig_localization = self.metrics.localization_accuracy(original_cam, mask_np)\n",
    "            orig_biological = self.metrics.biological_relevance(original_cam, nuclei_mask, background_mask)\n",
    "            orig_coherence = self.metrics.spatial_coherence(original_cam)\n",
    "            \n",
    "            results['gradcam_original'] = {\n",
    "                'localization_accuracy': orig_localization,\n",
    "                'biological_relevance': orig_biological,\n",
    "                'spatial_coherence': orig_coherence,\n",
    "                'attention_map': original_cam,\n",
    "                'iou': baseline_iou,  # Same segmentation performance\n",
    "                'dice': baseline_dice\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Grad-CAM evaluation failed for original model: {e}\")\n",
    "            results['gradcam_original'] = None\n",
    "        \n",
    "        # 3. Grad-CAM with normalized model\n",
    "        try:\n",
    "            # Apply stain normalization\n",
    "            image_denorm = self.denormalize_image(image)\n",
    "            image_np = image_denorm.permute(1, 2, 0).numpy()\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "            \n",
    "            # Normalize image\n",
    "            normalized_image = self.normalizer.normalize(image_np)\n",
    "            normalized_image = Image.fromarray(normalized_image)\n",
    "            normalized_image = transform(normalized_image)\n",
    "            normalized_image_batch = normalized_image.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Generate Grad-CAM\n",
    "            normalized_cam = self.models['normalized_gradcam'].generate_attention_map(normalized_image_batch)\n",
    "            \n",
    "            # Calculate explainability metrics\n",
    "            norm_localization = self.metrics.localization_accuracy(normalized_cam, mask_np)\n",
    "            norm_biological = self.metrics.biological_relevance(normalized_cam, nuclei_mask, background_mask)\n",
    "            norm_coherence = self.metrics.spatial_coherence(normalized_cam)\n",
    "            \n",
    "            # Calculate attention consistency with original\n",
    "            if results['gradcam_original'] is not None:\n",
    "                consistency = self.metrics.attention_consistency(original_cam, normalized_cam)\n",
    "            else:\n",
    "                consistency = 0.0\n",
    "            \n",
    "            results['gradcam_normalized'] = {\n",
    "                'localization_accuracy': norm_localization,\n",
    "                'biological_relevance': norm_biological,\n",
    "                'spatial_coherence': norm_coherence,\n",
    "                'attention_consistency': consistency,\n",
    "                'attention_map': normalized_cam,\n",
    "                'iou': baseline_iou,  # Same segmentation performance\n",
    "                'dice': baseline_dice\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Grad-CAM evaluation failed for normalized model: {e}\")\n",
    "            results['gradcam_normalized'] = None\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def denormalize_image(self, tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        \"\"\"Denormalize tensor image for visualization.\"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, mean, std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "    def evaluate_dataset(self, dataset, max_samples=None):\n",
    "        \"\"\"\n",
    "        Evaluate entire dataset across all model variants.\n",
    "        \n",
    "        Args:\n",
    "            dataset: RQ4Dataset instance\n",
    "            max_samples: Maximum number of samples to evaluate\n",
    "        \n",
    "        Returns:\n",
    "            results: Comprehensive evaluation results\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting comprehensive evaluation of {len(dataset)} samples...\")\n",
    "        \n",
    "        num_samples = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "        \n",
    "        for i in tqdm(range(num_samples), desc=\"Evaluating samples\"):\n",
    "            try:\n",
    "                sample = dataset[i]\n",
    "                image = sample['image']\n",
    "                mask = sample['mask']\n",
    "                image_path = sample['image_path']\n",
    "                \n",
    "                # Extract tissue type from path\n",
    "                tissue_type = Path(image_path).parent.parent.parent.name\n",
    "                \n",
    "                # Evaluate single image\n",
    "                results = self.evaluate_single_image(image, mask, tissue_type, image_path)\n",
    "                \n",
    "                # Store results\n",
    "                self.results['baseline'].append(results['baseline'])\n",
    "                if results['gradcam_original']:\n",
    "                    self.results['gradcam_original'].append(results['gradcam_original'])\n",
    "                if results['gradcam_normalized']:\n",
    "                    self.results['gradcam_normalized'].append(results['gradcam_normalized'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Evaluation failed for sample {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Evaluation completed. Processed {len(self.results['baseline'])} samples.\")\n",
    "        return self.results\n",
    "    \n",
    "    def generate_summary_statistics(self):\n",
    "        \"\"\"Generate summary statistics for all model variants.\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for model_type, results in self.results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "                \n",
    "            # Convert to DataFrame for easier analysis\n",
    "            df = pd.DataFrame(results)\n",
    "            \n",
    "            summary[model_type] = {\n",
    "                'count': len(df),\n",
    "                'localization_accuracy': {\n",
    "                    'mean': df['localization_accuracy'].mean() if 'localization_accuracy' in df.columns else None,\n",
    "                    'std': df['localization_accuracy'].std() if 'localization_accuracy' in df.columns else None,\n",
    "                    'median': df['localization_accuracy'].median() if 'localization_accuracy' in df.columns else None\n",
    "                },\n",
    "                'biological_relevance': {\n",
    "                    'mean': df['biological_relevance'].mean() if 'biological_relevance' in df.columns else None,\n",
    "                    'std': df['biological_relevance'].std() if 'biological_relevance' in df.columns else None,\n",
    "                    'median': df['biological_relevance'].median() if 'biological_relevance' in df.columns else None\n",
    "                },\n",
    "                'spatial_coherence': {\n",
    "                    'mean': df['spatial_coherence'].mean() if 'spatial_coherence' in df.columns else None,\n",
    "                    'std': df['spatial_coherence'].std() if 'spatial_coherence' in df.columns else None,\n",
    "                    'median': df['spatial_coherence'].median() if 'spatial_coherence' in df.columns else None\n",
    "                },\n",
    "                'attention_consistency': {\n",
    "                    'mean': df['attention_consistency'].mean() if 'attention_consistency' in df.columns else None,\n",
    "                    'std': df['attention_consistency'].std() if 'attention_consistency' in df.columns else None,\n",
    "                    'median': df['attention_consistency'].median() if 'attention_consistency' in df.columns else None\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RQ4Evaluator(models, normalizer, device)\n",
    "print(\"âœ… RQ4 evaluation framework initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis Framework\n",
    "\n",
    "### 5.1 ANOVA and Statistical Tests for Explainability Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATISTICAL ANALYSIS FRAMEWORK FOR RQ4\n",
    "# =============================================================================\n",
    "\n",
    "class RQ4StatisticalAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive statistical analysis framework for RQ4 explainability evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, results):\n",
    "        self.results = results\n",
    "        self.alpha = 0.05  # Significance level\n",
    "    \n",
    "    def prepare_data_for_analysis(self):\n",
    "        \"\"\"Prepare data in format suitable for statistical analysis.\"\"\"\n",
    "        analysis_data = []\n",
    "        \n",
    "        # Process each model type\n",
    "        for model_type, results in self.results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "                \n",
    "            for result in results:\n",
    "                if model_type == 'baseline':\n",
    "                    analysis_data.append({\n",
    "                        'model_type': 'Baseline',\n",
    "                        'localization_accuracy': result['iou'],\n",
    "                        'biological_relevance': None,\n",
    "                        'spatial_coherence': None,\n",
    "                        'attention_consistency': None,\n",
    "                        'tissue_type': result.get('tissue_type', 'Unknown')\n",
    "                    })\n",
    "                else:\n",
    "                    analysis_data.append({\n",
    "                        'model_type': 'GradCAM_Original' if 'original' in model_type else 'GradCAM_Normalized',\n",
    "                        'localization_accuracy': result['localization_accuracy'],\n",
    "                        'biological_relevance': result['biological_relevance'],\n",
    "                        'spatial_coherence': result['spatial_coherence'],\n",
    "                        'attention_consistency': result.get('attention_consistency', None),\n",
    "                        'tissue_type': result.get('tissue_type', 'Unknown')\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(analysis_data)\n",
    "    \n",
    "    def perform_anova_analysis(self, df):\n",
    "        \"\"\"Perform ANOVA analysis for explainability metrics.\"\"\"\n",
    "        anova_results = {}\n",
    "        \n",
    "        # Metrics to analyze\n",
    "        metrics = ['localization_accuracy', 'biological_relevance', 'spatial_coherence']\n",
    "        \n",
    "        for metric in metrics:\n",
    "            # Filter out None values\n",
    "            metric_data = df[df[metric].notna()]\n",
    "            \n",
    "            if len(metric_data) < 3:\n",
    "                logger.warning(f\"Insufficient data for {metric} ANOVA analysis\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare data for ANOVA\n",
    "            groups = []\n",
    "            group_names = []\n",
    "            \n",
    "            for model_type in metric_data['model_type'].unique():\n",
    "                group_data = metric_data[metric_data['model_type'] == model_type][metric].values\n",
    "                if len(group_data) > 0:\n",
    "                    groups.append(group_data)\n",
    "                    group_names.append(model_type)\n",
    "            \n",
    "            if len(groups) < 2:\n",
    "                logger.warning(f\"Not enough groups for {metric} ANOVA\")\n",
    "                continue\n",
    "            \n",
    "            # Perform ANOVA\n",
    "            try:\n",
    "                f_stat, p_value = f_oneway(*groups)\n",
    "                \n",
    "                anova_results[metric] = {\n",
    "                    'f_statistic': f_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value < self.alpha,\n",
    "                    'groups': group_names,\n",
    "                    'group_sizes': [len(group) for group in groups],\n",
    "                    'group_means': [np.mean(group) for group in groups],\n",
    "                    'group_stds': [np.std(group) for group in groups]\n",
    "                }\n",
    "                \n",
    "                logger.info(f\"ANOVA for {metric}: F={f_stat:.4f}, p={p_value:.4f}, significant={p_value < self.alpha}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"ANOVA failed for {metric}: {e}\")\n",
    "                anova_results[metric] = None\n",
    "        \n",
    "        return anova_results\n",
    "    \n",
    "    def perform_post_hoc_tests(self, df, anova_results):\n",
    "        \"\"\"Perform post-hoc tests for significant ANOVA results.\"\"\"\n",
    "        post_hoc_results = {}\n",
    "        \n",
    "        for metric, anova_result in anova_results.items():\n",
    "            if anova_result is None or not anova_result['significant']:\n",
    "                continue\n",
    "            \n",
    "            # Prepare data for post-hoc tests\n",
    "            metric_data = df[df[metric].notna()]\n",
    "            \n",
    "            # Perform pairwise t-tests\n",
    "            pairwise_results = {}\n",
    "            model_types = metric_data['model_type'].unique()\n",
    "            \n",
    "            for i, model1 in enumerate(model_types):\n",
    "                for j, model2 in enumerate(model_types):\n",
    "                    if i >= j:\n",
    "                        continue\n",
    "                    \n",
    "                    group1 = metric_data[metric_data['model_type'] == model1][metric].values\n",
    "                    group2 = metric_data[metric_data['model_type'] == model2][metric].values\n",
    "                    \n",
    "                    if len(group1) < 2 or len(group2) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Perform t-test\n",
    "                        t_stat, p_value = ttest_rel(group1, group2) if len(group1) == len(group2) else ttest_ind(group1, group2)\n",
    "                        \n",
    "                        pairwise_results[f\"{model1}_vs_{model2}\"] = {\n",
    "                            't_statistic': t_stat,\n",
    "                            'p_value': p_value,\n",
    "                            'significant': p_value < self.alpha,\n",
    "                            'mean_diff': np.mean(group1) - np.mean(group2),\n",
    "                            'effect_size': self.calculate_cohens_d(group1, group2)\n",
    "                        }\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Post-hoc test failed for {model1} vs {model2}: {e}\")\n",
    "            \n",
    "            post_hoc_results[metric] = pairwise_results\n",
    "        \n",
    "        return post_hoc_results\n",
    "    \n",
    "    def calculate_cohens_d(self, group1, group2):\n",
    "        \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        s1, s2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "        \n",
    "        # Pooled standard deviation\n",
    "        pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
    "        \n",
    "        if pooled_std == 0:\n",
    "            return 0\n",
    "        \n",
    "        return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "    \n",
    "    def perform_normality_tests(self, df):\n",
    "        \"\"\"Perform normality tests for each group.\"\"\"\n",
    "        normality_results = {}\n",
    "        \n",
    "        for model_type in df['model_type'].unique():\n",
    "            model_data = df[df['model_type'] == model_type]\n",
    "            normality_results[model_type] = {}\n",
    "            \n",
    "            for metric in ['localization_accuracy', 'biological_relevance', 'spatial_coherence']:\n",
    "                metric_data = model_data[model_data[metric].notna()][metric].values\n",
    "                \n",
    "                if len(metric_data) < 3:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Shapiro-Wilk test\n",
    "                    shapiro_stat, shapiro_p = shapiro(metric_data)\n",
    "                    \n",
    "                    normality_results[model_type][metric] = {\n",
    "                        'shapiro_statistic': shapiro_stat,\n",
    "                        'shapiro_p_value': shapiro_p,\n",
    "                        'is_normal': shapiro_p > self.alpha,\n",
    "                        'sample_size': len(metric_data)\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Normality test failed for {model_type} - {metric}: {e}\")\n",
    "        \n",
    "        return normality_results\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate comprehensive statistical analysis report.\"\"\"\n",
    "        logger.info(\"Generating comprehensive statistical analysis report...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df = self.prepare_data_for_analysis()\n",
    "        \n",
    "        # Perform analyses\n",
    "        anova_results = self.perform_anova_analysis(df)\n",
    "        post_hoc_results = self.perform_post_hoc_tests(df, anova_results)\n",
    "        normality_results = self.perform_normality_tests(df)\n",
    "        \n",
    "        # Generate summary\n",
    "        report = {\n",
    "            'data_summary': {\n",
    "                'total_samples': len(df),\n",
    "                'model_types': df['model_type'].value_counts().to_dict(),\n",
    "                'tissue_types': df['tissue_type'].value_counts().to_dict()\n",
    "            },\n",
    "            'anova_results': anova_results,\n",
    "            'post_hoc_results': post_hoc_results,\n",
    "            'normality_results': normality_results,\n",
    "            'descriptive_statistics': self.generate_descriptive_statistics(df)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def generate_descriptive_statistics(self, df):\n",
    "        \"\"\"Generate descriptive statistics for all metrics.\"\"\"\n",
    "        desc_stats = {}\n",
    "        \n",
    "        for metric in ['localization_accuracy', 'biological_relevance', 'spatial_coherence']:\n",
    "            metric_data = df[df[metric].notna()]\n",
    "            \n",
    "            if len(metric_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            desc_stats[metric] = {\n",
    "                'overall': {\n",
    "                    'mean': metric_data[metric].mean(),\n",
    "                    'std': metric_data[metric].std(),\n",
    "                    'median': metric_data[metric].median(),\n",
    "                    'min': metric_data[metric].min(),\n",
    "                    'max': metric_data[metric].max(),\n",
    "                    'q25': metric_data[metric].quantile(0.25),\n",
    "                    'q75': metric_data[metric].quantile(0.75)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Per model type statistics\n",
    "            for model_type in metric_data['model_type'].unique():\n",
    "                model_metric_data = metric_data[metric_data['model_type'] == model_type][metric]\n",
    "                \n",
    "                desc_stats[metric][model_type] = {\n",
    "                    'mean': model_metric_data.mean(),\n",
    "                    'std': model_metric_data.std(),\n",
    "                    'median': model_metric_data.median(),\n",
    "                    'count': len(model_metric_data)\n",
    "                }\n",
    "        \n",
    "        return desc_stats\n",
    "    \n",
    "    def save_results(self, report, save_path):\n",
    "        \"\"\"Save statistical analysis results.\"\"\"\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"Statistical analysis results saved to {save_path}\")\n",
    "\n",
    "# Initialize statistical analyzer\n",
    "statistical_analyzer = RQ4StatisticalAnalyzer(evaluator.results)\n",
    "print(\"âœ… RQ4 statistical analysis framework initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution Pipeline\n",
    "\n",
    "### 6.1 Complete RQ4 Analysis Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION PIPELINE FOR RQ4\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_rq4_analysis():\n",
    "    \"\"\"\n",
    "    Execute the complete RQ4 Grad-CAM explainability analysis pipeline.\n",
    "    \"\"\"\n",
    "    logger.info(\"ðŸš€ Starting RQ4 Complete Analysis Pipeline\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Prepare datasets\n",
    "        logger.info(\"ðŸ“Š Step 1: Preparing datasets...\")\n",
    "        \n",
    "        # Create datasets for each tissue type\n",
    "        datasets = {}\n",
    "        for tissue in selected_tissues:\n",
    "            tissue_pairs = tissue_data[tissue]\n",
    "            \n",
    "            # Sample data if in testing mode\n",
    "            if TESTING_MODE and SAMPLE_SIZE_PER_TISSUE:\n",
    "                tissue_pairs = tissue_pairs[:SAMPLE_SIZE_PER_TISSUE]\n",
    "            \n",
    "            # Create original dataset\n",
    "            datasets[f'{tissue}_original'] = RQ4Dataset(\n",
    "                tissue_pairs, transform=transform, normalize=False, normalizer=None\n",
    "            )\n",
    "            \n",
    "            # Create normalized dataset\n",
    "            datasets[f'{tissue}_normalized'] = RQ4Dataset(\n",
    "                tissue_pairs, transform=transform, normalize=True, normalizer=normalizer\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"âœ… Created {len(datasets)} datasets\")\n",
    "        \n",
    "        # Step 2: Run evaluation on test data\n",
    "        logger.info(\"ðŸ”¬ Step 2: Running comprehensive evaluation...\")\n",
    "        \n",
    "        # Combine all test data\n",
    "        all_test_pairs = []\n",
    "        for tissue in selected_tissues:\n",
    "            tissue_pairs = tissue_data[tissue]\n",
    "            if TESTING_MODE and SAMPLE_SIZE_PER_TISSUE:\n",
    "                tissue_pairs = tissue_pairs[:SAMPLE_SIZE_PER_TISSUE]\n",
    "            all_test_pairs.extend(tissue_pairs)\n",
    "        \n",
    "        # Create evaluation dataset\n",
    "        eval_dataset = RQ4Dataset(all_test_pairs, transform=transform, normalize=False)\n",
    "        \n",
    "        # Run evaluation\n",
    "        evaluation_results = evaluator.evaluate_dataset(\n",
    "            eval_dataset, \n",
    "            max_samples=100 if TESTING_MODE else None\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"âœ… Evaluation completed. Processed {len(evaluation_results['baseline'])} samples\")\n",
    "        \n",
    "        # Step 3: Generate visualizations\n",
    "        logger.info(\"ðŸ“ˆ Step 3: Generating visualizations...\")\n",
    "        \n",
    "        # Create sample visualizations\n",
    "        sample_indices = [0, 1, 2] if len(evaluation_results['baseline']) >= 3 else [0]\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            if idx >= len(evaluation_results['baseline']):\n",
    "                break\n",
    "                \n",
    "            # Get sample data\n",
    "            sample = eval_dataset[idx]\n",
    "            image = sample['image']\n",
    "            mask = sample['mask']\n",
    "            \n",
    "            # Generate Grad-CAM visualizations\n",
    "            if 'gradcam_original' in models and len(evaluation_results['gradcam_original']) > idx:\n",
    "                try:\n",
    "                    # Original Grad-CAM\n",
    "                    original_cam = evaluation_results['gradcam_original'][idx]['attention_map']\n",
    "                    \n",
    "                    # Create comparison plot\n",
    "                    save_path = artifacts_dir / 'gradcam_visualizations' / f'sample_{i}_comparison.png'\n",
    "                    visualizer.plot_comparison_grid(\n",
    "                        [image], [mask], [original_cam], \n",
    "                        [f'Sample {i+1}'], save_path\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Visualization failed for sample {i}: {e}\")\n",
    "        \n",
    "        # Step 4: Statistical analysis\n",
    "        logger.info(\"ðŸ“Š Step 4: Performing statistical analysis...\")\n",
    "        \n",
    "        # Update statistical analyzer with results\n",
    "        statistical_analyzer.results = evaluation_results\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        statistical_report = statistical_analyzer.generate_comprehensive_report()\n",
    "        \n",
    "        # Save statistical results\n",
    "        stats_save_path = artifacts_dir / 'statistical_analysis' / 'rq4_statistical_report.json'\n",
    "        statistical_analyzer.save_results(statistical_report, stats_save_path)\n",
    "        \n",
    "        # Step 5: Generate summary visualizations\n",
    "        logger.info(\"ðŸ“Š Step 5: Generating summary visualizations...\")\n",
    "        \n",
    "        # Prepare data for visualization\n",
    "        df = statistical_analyzer.prepare_data_for_analysis()\n",
    "        \n",
    "        # Create metrics comparison plot\n",
    "        if len(df) > 0:\n",
    "            metrics_save_path = artifacts_dir / 'plots' / 'rq4_metrics_comparison.png'\n",
    "            visualizer.plot_metrics_comparison(df, metrics_save_path)\n",
    "        \n",
    "        # Step 6: Generate final report\n",
    "        logger.info(\"ðŸ“‹ Step 6: Generating final report...\")\n",
    "        \n",
    "        # Create summary statistics\n",
    "        summary_stats = evaluator.generate_summary_statistics()\n",
    "        \n",
    "        # Save summary\n",
    "        summary_save_path = artifacts_dir / 'results' / 'rq4_summary_statistics.json'\n",
    "        with open(summary_save_path, 'w') as f:\n",
    "            json.dump(summary_stats, f, indent=2, default=str)\n",
    "        \n",
    "        # Print results summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸŽ¯ RQ4 GRAD-CAM EXPLAINABILITY ANALYSIS - RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "        print(f\"   â€¢ Total samples evaluated: {len(evaluation_results['baseline'])}\")\n",
    "        print(f\"   â€¢ Tissues analyzed: {', '.join(selected_tissues)}\")\n",
    "        print(f\"   â€¢ Model variants: {len([k for k in evaluation_results.keys() if evaluation_results[k]])}\")\n",
    "        \n",
    "        print(f\"\\nðŸ”¬ Statistical Analysis Results:\")\n",
    "        for metric, anova_result in statistical_report['anova_results'].items():\n",
    "            if anova_result:\n",
    "                significance = \"âœ… SIGNIFICANT\" if anova_result['significant'] else \"âŒ NOT SIGNIFICANT\"\n",
    "                print(f\"   â€¢ {metric}: F={anova_result['f_statistic']:.4f}, p={anova_result['p_value']:.4f} {significance}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Key Findings:\")\n",
    "        if 'localization_accuracy' in statistical_report['anova_results']:\n",
    "            la_result = statistical_report['anova_results']['localization_accuracy']\n",
    "            if la_result and la_result['significant']:\n",
    "                print(\"   â€¢ Grad-CAM significantly improves localization accuracy\")\n",
    "            else:\n",
    "                print(\"   â€¢ No significant improvement in localization accuracy\")\n",
    "        \n",
    "        if 'biological_relevance' in statistical_report['anova_results']:\n",
    "            br_result = statistical_report['anova_results']['biological_relevance']\n",
    "            if br_result and br_result['significant']:\n",
    "                print(\"   â€¢ Stain normalization significantly improves biological relevance\")\n",
    "            else:\n",
    "                print(\"   â€¢ No significant improvement in biological relevance with normalization\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Results saved to: {artifacts_dir}\")\n",
    "        print(f\"   â€¢ Visualizations: {artifacts_dir / 'gradcam_visualizations'}\")\n",
    "        print(f\"   â€¢ Statistical analysis: {artifacts_dir / 'statistical_analysis'}\")\n",
    "        print(f\"   â€¢ Summary results: {artifacts_dir / 'results'}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\nâ±ï¸  Total execution time: {duration:.2f} seconds\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        logger.info(f\"âœ… RQ4 analysis completed successfully in {duration:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'evaluation_results': evaluation_results,\n",
    "            'statistical_report': statistical_report,\n",
    "            'summary_statistics': summary_stats,\n",
    "            'execution_time': duration\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ RQ4 analysis failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "print(\"âœ… RQ4 main execution pipeline ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Execute RQ4 Analysis\n",
    "\n",
    "**Ready to run the complete RQ4 Grad-CAM explainability analysis!**\n",
    "\n",
    "The pipeline will:\n",
    "1. âœ… Load and prepare datasets from top 5 tissues\n",
    "2. âœ… Evaluate U-Net models with and without Grad-CAM\n",
    "3. âœ… Compare original vs stain-normalized images\n",
    "4. âœ… Generate comprehensive visualizations\n",
    "5. âœ… Perform statistical analysis (ANOVA)\n",
    "6. âœ… Generate publication-ready results\n",
    "\n",
    "**Click the cell below to start the analysis:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the complete RQ4 analysis\n",
    "print(\"ðŸš€ Starting RQ4 Grad-CAM Explainability Analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the complete analysis\n",
    "results = run_complete_rq4_analysis()\n",
    "\n",
    "print(\"\\nðŸŽ‰ RQ4 Analysis Complete!\")\n",
    "print(\"Check the artifacts directory for detailed results and visualizations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Interpretation and Clinical Insights\n",
    "\n",
    "### 7.1 Understanding the Results\n",
    "\n",
    "The RQ4 analysis provides comprehensive insights into the effectiveness of Grad-CAM explainability techniques for U-Net-based nuclei segmentation:\n",
    "\n",
    "**Key Metrics Evaluated:**\n",
    "- **Localization Accuracy**: How well Grad-CAM attention maps align with ground truth nuclei regions\n",
    "- **Biological Relevance**: The ratio of attention in nuclei vs background regions\n",
    "- **Spatial Coherence**: The spatial consistency of attention maps\n",
    "- **Attention Consistency**: Correlation between original and normalized model attention maps\n",
    "\n",
    "**Statistical Analysis:**\n",
    "- **ANOVA**: Tests for significant differences between model variants\n",
    "- **Post-hoc Tests**: Pairwise comparisons between specific model types\n",
    "- **Effect Sizes**: Quantifies the magnitude of differences (Cohen's d)\n",
    "\n",
    "### 7.2 Clinical Implications\n",
    "\n",
    "**For Medical AI Validation:**\n",
    "- Grad-CAM provides visual evidence of model decision-making\n",
    "- Stain normalization may improve attention map quality\n",
    "- Statistical validation ensures reliable clinical insights\n",
    "\n",
    "**For Model Interpretability:**\n",
    "- Attention maps help identify model focus areas\n",
    "- Biological relevance metrics ensure clinically meaningful explanations\n",
    "- Spatial coherence indicates attention map reliability\n",
    "\n",
    "### 7.3 Future Research Directions\n",
    "\n",
    "1. **Multi-scale Analysis**: Evaluate attention at different resolution levels\n",
    "2. **Tissue-specific Studies**: Analyze explainability across different tissue types\n",
    "3. **Clinical Validation**: Expert evaluation of attention map clinical relevance\n",
    "4. **Advanced Techniques**: Compare with other explainability methods (LIME, SHAP)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”¬ Research Question 4 Complete | Grad-CAM Explainability Analysis | Publication Ready**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED RESULT SAVING AND ORGANIZATION FOR RQ4\n",
    "# =============================================================================\n",
    "\n",
    "class RQ4ResultManager:\n",
    "    \"\"\"\n",
    "    Comprehensive result management and saving system for RQ4 analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, artifacts_dir):\n",
    "        self.artifacts_dir = Path(artifacts_dir)\n",
    "        self.rq4_dir = self.artifacts_dir / 'rq4_gradcam'\n",
    "        \n",
    "        # Create comprehensive directory structure\n",
    "        self.create_directory_structure()\n",
    "    \n",
    "    def create_directory_structure(self):\n",
    "        \"\"\"Create organized directory structure for RQ4 results.\"\"\"\n",
    "        directories = [\n",
    "            'results/raw_data',\n",
    "            'results/processed_data', \n",
    "            'results/statistical_analysis',\n",
    "            'results/summary_reports',\n",
    "            'visualizations/gradcam_maps',\n",
    "            'visualizations/comparison_plots',\n",
    "            'visualizations/metrics_plots',\n",
    "            'models/checkpoints',\n",
    "            'models/gradcam_weights',\n",
    "            'logs/execution_logs',\n",
    "            'logs/error_logs',\n",
    "            'data/sample_images',\n",
    "            'data/attention_maps',\n",
    "            'reports/publication_ready',\n",
    "            'reports/clinical_insights'\n",
    "        ]\n",
    "        \n",
    "        for directory in directories:\n",
    "            (self.rq4_dir / directory).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"âœ… RQ4 directory structure created at {self.rq4_dir}\")\n",
    "    \n",
    "    def save_evaluation_results(self, evaluation_results, tissue_types):\n",
    "        \"\"\"Save comprehensive evaluation results.\"\"\"\n",
    "        logger.info(\"ðŸ’¾ Saving evaluation results...\")\n",
    "        \n",
    "        # Save raw evaluation data\n",
    "        raw_data_path = self.rq4_dir / 'results' / 'raw_data' / 'evaluation_results.json'\n",
    "        with open(raw_data_path, 'w') as f:\n",
    "            json.dump(evaluation_results, f, indent=2, default=str)\n",
    "        \n",
    "        # Create processed DataFrame for analysis\n",
    "        processed_data = self.process_evaluation_data(evaluation_results, tissue_types)\n",
    "        \n",
    "        # Save processed data as CSV\n",
    "        csv_path = self.rq4_dir / 'results' / 'processed_data' / 'evaluation_metrics.csv'\n",
    "        processed_data.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Save per-tissue analysis\n",
    "        self.save_tissue_specific_results(processed_data, tissue_types)\n",
    "        \n",
    "        logger.info(f\"âœ… Evaluation results saved to {self.rq4_dir / 'results'}\")\n",
    "        return processed_data\n",
    "    \n",
    "    def process_evaluation_data(self, evaluation_results, tissue_types):\n",
    "        \"\"\"Process evaluation results into structured DataFrame.\"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        for model_type, results in evaluation_results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "                \n",
    "            for i, result in enumerate(results):\n",
    "                row = {\n",
    "                    'model_type': model_type,\n",
    "                    'sample_id': i,\n",
    "                    'tissue_type': result.get('tissue_type', 'Unknown'),\n",
    "                    'image_path': result.get('image_path', ''),\n",
    "                    'iou': result.get('iou', None),\n",
    "                    'dice': result.get('dice', None),\n",
    "                    'localization_accuracy': result.get('localization_accuracy', None),\n",
    "                    'biological_relevance': result.get('biological_relevance', None),\n",
    "                    'spatial_coherence': result.get('spatial_coherence', None),\n",
    "                    'attention_consistency': result.get('attention_consistency', None)\n",
    "                }\n",
    "                processed_data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(processed_data)\n",
    "    \n",
    "    def save_tissue_specific_results(self, processed_data, tissue_types):\n",
    "        \"\"\"Save tissue-specific analysis results.\"\"\"\n",
    "        for tissue in tissue_types:\n",
    "            tissue_data = processed_data[processed_data['tissue_type'] == tissue]\n",
    "            \n",
    "            if len(tissue_data) > 0:\n",
    "                # Save tissue-specific CSV\n",
    "                tissue_path = self.rq4_dir / 'results' / 'processed_data' / f'{tissue}_analysis.csv'\n",
    "                tissue_data.to_csv(tissue_path, index=False)\n",
    "                \n",
    "                # Generate tissue-specific summary\n",
    "                summary = self.generate_tissue_summary(tissue_data, tissue)\n",
    "                summary_path = self.rq4_dir / 'results' / 'summary_reports' / f'{tissue}_summary.json'\n",
    "                with open(summary_path, 'w') as f:\n",
    "                    json.dump(summary, f, indent=2, default=str)\n",
    "    \n",
    "    def generate_tissue_summary(self, tissue_data, tissue_name):\n",
    "        \"\"\"Generate summary statistics for specific tissue.\"\"\"\n",
    "        summary = {\n",
    "            'tissue_name': tissue_name,\n",
    "            'total_samples': len(tissue_data),\n",
    "            'model_types': tissue_data['model_type'].value_counts().to_dict(),\n",
    "            'metrics_summary': {}\n",
    "        }\n",
    "        \n",
    "        # Calculate metrics for each model type\n",
    "        for model_type in tissue_data['model_type'].unique():\n",
    "            model_data = tissue_data[tissue_data['model_type'] == model_type]\n",
    "            \n",
    "            summary['metrics_summary'][model_type] = {\n",
    "                'sample_count': len(model_data),\n",
    "                'localization_accuracy': {\n",
    "                    'mean': model_data['localization_accuracy'].mean() if 'localization_accuracy' in model_data.columns else None,\n",
    "                    'std': model_data['localization_accuracy'].std() if 'localization_accuracy' in model_data.columns else None,\n",
    "                    'median': model_data['localization_accuracy'].median() if 'localization_accuracy' in model_data.columns else None\n",
    "                },\n",
    "                'biological_relevance': {\n",
    "                    'mean': model_data['biological_relevance'].mean() if 'biological_relevance' in model_data.columns else None,\n",
    "                    'std': model_data['biological_relevance'].std() if 'biological_relevance' in model_data.columns else None,\n",
    "                    'median': model_data['biological_relevance'].median() if 'biological_relevance' in model_data.columns else None\n",
    "                },\n",
    "                'spatial_coherence': {\n",
    "                    'mean': model_data['spatial_coherence'].mean() if 'spatial_coherence' in model_data.columns else None,\n",
    "                    'std': model_data['spatial_coherence'].std() if 'spatial_coherence' in model_data.columns else None,\n",
    "                    'median': model_data['spatial_coherence'].median() if 'spatial_coherence' in model_data.columns else None\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_statistical_analysis(self, statistical_report):\n",
    "        \"\"\"Save comprehensive statistical analysis results.\"\"\"\n",
    "        logger.info(\"ðŸ’¾ Saving statistical analysis results...\")\n",
    "        \n",
    "        # Save full statistical report\n",
    "        stats_path = self.rq4_dir / 'results' / 'statistical_analysis' / 'complete_statistical_report.json'\n",
    "        with open(stats_path, 'w') as f:\n",
    "            json.dump(statistical_report, f, indent=2, default=str)\n",
    "        \n",
    "        # Save ANOVA results separately\n",
    "        anova_path = self.rq4_dir / 'results' / 'statistical_analysis' / 'anova_results.json'\n",
    "        with open(anova_path, 'w') as f:\n",
    "            json.dump(statistical_report['anova_results'], f, indent=2, default=str)\n",
    "        \n",
    "        # Save post-hoc results\n",
    "        posthoc_path = self.rq4_dir / 'results' / 'statistical_analysis' / 'posthoc_results.json'\n",
    "        with open(posthoc_path, 'w') as f:\n",
    "            json.dump(statistical_report['post_hoc_results'], f, indent=2, default=str)\n",
    "        \n",
    "        # Create publication-ready statistical summary\n",
    "        pub_summary = self.create_publication_summary(statistical_report)\n",
    "        pub_path = self.rq4_dir / 'reports' / 'publication_ready' / 'statistical_summary.json'\n",
    "        with open(pub_path, 'w') as f:\n",
    "            json.dump(pub_summary, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"âœ… Statistical analysis saved to {self.rq4_dir / 'results' / 'statistical_analysis'}\")\n",
    "    \n",
    "    def create_publication_summary(self, statistical_report):\n",
    "        \"\"\"Create publication-ready statistical summary.\"\"\"\n",
    "        summary = {\n",
    "            'research_question': 'RQ4: Grad-CAM Explainability Analysis',\n",
    "            'analysis_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'sample_size': statistical_report['data_summary']['total_samples'],\n",
    "            'tissues_analyzed': list(statistical_report['data_summary']['tissue_types'].keys()),\n",
    "            'model_variants': list(statistical_report['data_summary']['model_types'].keys()),\n",
    "            'key_findings': {},\n",
    "            'statistical_significance': {}\n",
    "        }\n",
    "        \n",
    "        # Extract key findings\n",
    "        for metric, anova_result in statistical_report['anova_results'].items():\n",
    "            if anova_result:\n",
    "                summary['statistical_significance'][metric] = {\n",
    "                    'f_statistic': anova_result['f_statistic'],\n",
    "                    'p_value': anova_result['p_value'],\n",
    "                    'significant': anova_result['significant'],\n",
    "                    'effect_size': 'Large' if anova_result['f_statistic'] > 10 else 'Medium' if anova_result['f_statistic'] > 5 else 'Small'\n",
    "                }\n",
    "                \n",
    "                if anova_result['significant']:\n",
    "                    summary['key_findings'][metric] = f\"Significant difference found (F={anova_result['f_statistic']:.3f}, p={anova_result['p_value']:.3f})\"\n",
    "                else:\n",
    "                    summary['key_findings'][metric] = f\"No significant difference (F={anova_result['f_statistic']:.3f}, p={anova_result['p_value']:.3f})\"\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_attention_maps(self, evaluation_results, sample_indices=[0, 1, 2]):\n",
    "        \"\"\"Save attention maps as separate files.\"\"\"\n",
    "        logger.info(\"ðŸ’¾ Saving attention maps...\")\n",
    "        \n",
    "        attention_dir = self.rq4_dir / 'data' / 'attention_maps'\n",
    "        attention_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            if idx >= len(evaluation_results['baseline']):\n",
    "                break\n",
    "            \n",
    "            # Save original Grad-CAM\n",
    "            if 'gradcam_original' in evaluation_results and len(evaluation_results['gradcam_original']) > idx:\n",
    "                orig_cam = evaluation_results['gradcam_original'][idx]['attention_map']\n",
    "                orig_path = attention_dir / f'sample_{i}_original_gradcam.npy'\n",
    "                np.save(orig_path, orig_cam)\n",
    "            \n",
    "            # Save normalized Grad-CAM\n",
    "            if 'gradcam_normalized' in evaluation_results and len(evaluation_results['gradcam_normalized']) > idx:\n",
    "                norm_cam = evaluation_results['gradcam_normalized'][idx]['attention_map']\n",
    "                norm_path = attention_dir / f'sample_{i}_normalized_gradcam.npy'\n",
    "                np.save(norm_path, norm_cam)\n",
    "        \n",
    "        logger.info(f\"âœ… Attention maps saved to {attention_dir}\")\n",
    "    \n",
    "    def save_visualizations(self, visualizer, evaluation_results, sample_indices=[0, 1, 2]):\n",
    "        \"\"\"Save all visualizations with proper organization.\"\"\"\n",
    "        logger.info(\"ðŸ’¾ Saving visualizations...\")\n",
    "        \n",
    "        # Save Grad-CAM visualizations\n",
    "        gradcam_dir = self.rq4_dir / 'visualizations' / 'gradcam_maps'\n",
    "        gradcam_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save comparison plots\n",
    "        comparison_dir = self.rq4_dir / 'visualizations' / 'comparison_plots'\n",
    "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save metrics plots\n",
    "        metrics_dir = self.rq4_dir / 'visualizations' / 'metrics_plots'\n",
    "        metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"âœ… Visualizations saved to {self.rq4_dir / 'visualizations'}\")\n",
    "    \n",
    "    def create_final_report(self, evaluation_results, statistical_report, execution_time):\n",
    "        \"\"\"Create comprehensive final report.\"\"\"\n",
    "        logger.info(\"ðŸ“‹ Creating final RQ4 report...\")\n",
    "        \n",
    "        final_report = {\n",
    "            'research_question': 'RQ4: Grad-CAM Explainability Analysis',\n",
    "            'execution_info': {\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'execution_time_seconds': execution_time,\n",
    "                'device_used': str(device),\n",
    "                'total_samples': len(evaluation_results['baseline'])\n",
    "            },\n",
    "            'dataset_info': {\n",
    "                'tissues_analyzed': selected_tissues,\n",
    "                'total_images': sum(len(tissue_data[tissue]) for tissue in selected_tissues),\n",
    "                'testing_mode': TESTING_MODE\n",
    "            },\n",
    "            'model_variants': {\n",
    "                'baseline': 'U-Net without Grad-CAM',\n",
    "                'gradcam_original': 'U-Net with Grad-CAM (original images)',\n",
    "                'gradcam_normalized': 'U-Net with Grad-CAM (stain normalized images)'\n",
    "            },\n",
    "            'key_results': self.extract_key_results(statistical_report),\n",
    "            'clinical_implications': self.generate_clinical_implications(statistical_report),\n",
    "            'file_locations': {\n",
    "                'raw_data': str(self.rq4_dir / 'results' / 'raw_data'),\n",
    "                'processed_data': str(self.rq4_dir / 'results' / 'processed_data'),\n",
    "                'statistical_analysis': str(self.rq4_dir / 'results' / 'statistical_analysis'),\n",
    "                'visualizations': str(self.rq4_dir / 'visualizations'),\n",
    "                'attention_maps': str(self.rq4_dir / 'data' / 'attention_maps'),\n",
    "                'publication_reports': str(self.rq4_dir / 'reports' / 'publication_ready')\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save final report\n",
    "        report_path = self.rq4_dir / 'reports' / 'RQ4_Final_Report.json'\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(final_report, f, indent=2, default=str)\n",
    "        \n",
    "        # Create markdown summary\n",
    "        self.create_markdown_summary(final_report)\n",
    "        \n",
    "        logger.info(f\"âœ… Final report saved to {report_path}\")\n",
    "        return final_report\n",
    "    \n",
    "    def extract_key_results(self, statistical_report):\n",
    "        \"\"\"Extract key results for final report.\"\"\"\n",
    "        key_results = {}\n",
    "        \n",
    "        for metric, anova_result in statistical_report['anova_results'].items():\n",
    "            if anova_result:\n",
    "                key_results[metric] = {\n",
    "                    'significant': anova_result['significant'],\n",
    "                    'f_statistic': anova_result['f_statistic'],\n",
    "                    'p_value': anova_result['p_value'],\n",
    "                    'interpretation': self.interpret_metric_results(metric, anova_result)\n",
    "                }\n",
    "        \n",
    "        return key_results\n",
    "    \n",
    "    def interpret_metric_results(self, metric, anova_result):\n",
    "        \"\"\"Interpret metric results for clinical understanding.\"\"\"\n",
    "        if not anova_result['significant']:\n",
    "            return f\"No significant differences found in {metric} across model variants\"\n",
    "        \n",
    "        if metric == 'localization_accuracy':\n",
    "            return \"Grad-CAM significantly improves localization accuracy compared to baseline\"\n",
    "        elif metric == 'biological_relevance':\n",
    "            return \"Stain normalization significantly improves biological relevance of attention maps\"\n",
    "        elif metric == 'spatial_coherence':\n",
    "            return \"Significant differences found in spatial coherence across model variants\"\n",
    "        else:\n",
    "            return f\"Significant differences found in {metric} (F={anova_result['f_statistic']:.3f}, p={anova_result['p_value']:.3f})\"\n",
    "    \n",
    "    def generate_clinical_implications(self, statistical_report):\n",
    "        \"\"\"Generate clinical implications from results.\"\"\"\n",
    "        implications = {\n",
    "            'model_interpretability': [],\n",
    "            'clinical_validation': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Check localization accuracy\n",
    "        if 'localization_accuracy' in statistical_report['anova_results']:\n",
    "            la_result = statistical_report['anova_results']['localization_accuracy']\n",
    "            if la_result and la_result['significant']:\n",
    "                implications['model_interpretability'].append(\n",
    "                    \"Grad-CAM provides reliable attention maps that align with ground truth nuclei regions\"\n",
    "                )\n",
    "                implications['clinical_validation'].append(\n",
    "                    \"Attention maps can be used for clinical validation of model decisions\"\n",
    "                )\n",
    "        \n",
    "        # Check biological relevance\n",
    "        if 'biological_relevance' in statistical_report['anova_results']:\n",
    "            br_result = statistical_report['anova_results']['biological_relevance']\n",
    "            if br_result and br_result['significant']:\n",
    "                implications['model_interpretability'].append(\n",
    "                    \"Stain normalization improves biological relevance of attention maps\"\n",
    "                )\n",
    "                implications['recommendations'].append(\n",
    "                    \"Use stain normalization for better clinical interpretability\"\n",
    "                )\n",
    "        \n",
    "        return implications\n",
    "    \n",
    "    def create_markdown_summary(self, final_report):\n",
    "        \"\"\"Create markdown summary for easy reading.\"\"\"\n",
    "        md_content = f\"\"\"# RQ4: Grad-CAM Explainability Analysis - Results Summary\n",
    "\n",
    "## Research Question\n",
    "**Do lightweight explainability techniquesâ€”like Grad-CAMâ€”enhance interpretability of U-Net-based nuclei segmentation on PanNuke, and does stain normalization improve this further?**\n",
    "\n",
    "## Execution Information\n",
    "- **Date**: {final_report['execution_info']['timestamp']}\n",
    "- **Execution Time**: {final_report['execution_info']['execution_time_seconds']:.2f} seconds\n",
    "- **Device**: {final_report['execution_info']['device_used']}\n",
    "- **Total Samples**: {final_report['execution_info']['total_samples']}\n",
    "\n",
    "## Dataset Information\n",
    "- **Tissues Analyzed**: {', '.join(final_report['dataset_info']['tissues_analyzed'])}\n",
    "- **Total Images**: {final_report['dataset_info']['total_images']:,}\n",
    "- **Testing Mode**: {final_report['dataset_info']['testing_mode']}\n",
    "\n",
    "## Model Variants\n",
    "1. **Baseline**: U-Net without Grad-CAM\n",
    "2. **GradCAM Original**: U-Net with Grad-CAM (original images)\n",
    "3. **GradCAM Normalized**: U-Net with Grad-CAM (stain normalized images)\n",
    "\n",
    "## Key Results\n",
    "\"\"\"\n",
    "        \n",
    "        for metric, result in final_report['key_results'].items():\n",
    "            significance = \"âœ… SIGNIFICANT\" if result['significant'] else \"âŒ NOT SIGNIFICANT\"\n",
    "            md_content += f\"\\n### {metric.replace('_', ' ').title()}\\n\"\n",
    "            md_content += f\"- **Significance**: {significance}\\n\"\n",
    "            md_content += f\"- **F-statistic**: {result['f_statistic']:.4f}\\n\"\n",
    "            md_content += f\"- **p-value**: {result['p_value']:.4f}\\n\"\n",
    "            md_content += f\"- **Interpretation**: {result['interpretation']}\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "## Clinical Implications\n",
    "\n",
    "### Model Interpretability\n",
    "\"\"\"\n",
    "        for implication in final_report['clinical_implications']['model_interpretability']:\n",
    "            md_content += f\"- {implication}\\n\"\n",
    "        \n",
    "        md_content += \"\\n### Clinical Validation\\n\"\n",
    "        for implication in final_report['clinical_implications']['clinical_validation']:\n",
    "            md_content += f\"- {implication}\\n\"\n",
    "        \n",
    "        md_content += \"\\n### Recommendations\\n\"\n",
    "        for implication in final_report['clinical_implications']['recommendations']:\n",
    "            md_content += f\"- {implication}\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "## File Locations\n",
    "- **Raw Data**: `{final_report['file_locations']['raw_data']}`\n",
    "- **Processed Data**: `{final_report['file_locations']['processed_data']}`\n",
    "- **Statistical Analysis**: `{final_report['file_locations']['statistical_analysis']}`\n",
    "- **Visualizations**: `{final_report['file_locations']['visualizations']}`\n",
    "- **Attention Maps**: `{final_report['file_locations']['attention_maps']}`\n",
    "- **Publication Reports**: `{final_report['file_locations']['publication_reports']}`\n",
    "\n",
    "---\n",
    "*Generated by RQ4 Grad-CAM Explainability Analysis Pipeline*\n",
    "\"\"\"\n",
    "        \n",
    "        # Save markdown report\n",
    "        md_path = self.rq4_dir / 'reports' / 'RQ4_Summary_Report.md'\n",
    "        with open(md_path, 'w') as f:\n",
    "            f.write(md_content)\n",
    "        \n",
    "        logger.info(f\"âœ… Markdown summary saved to {md_path}\")\n",
    "\n",
    "# Initialize result manager\n",
    "result_manager = RQ4ResultManager(artifacts_dir)\n",
    "print(\"âœ… RQ4 result management system initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED MAIN EXECUTION PIPELINE WITH COMPREHENSIVE RESULT SAVING\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_rq4_analysis_enhanced():\n",
    "    \"\"\"\n",
    "    Execute the complete RQ4 Grad-CAM explainability analysis pipeline with enhanced result saving.\n",
    "    \"\"\"\n",
    "    logger.info(\"ðŸš€ Starting RQ4 Enhanced Analysis Pipeline\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Prepare datasets\n",
    "        logger.info(\"ðŸ“Š Step 1: Preparing datasets...\")\n",
    "        \n",
    "        # Create datasets for each tissue type\n",
    "        datasets = {}\n",
    "        for tissue in selected_tissues:\n",
    "            tissue_pairs = tissue_data[tissue]\n",
    "            \n",
    "            # Sample data if in testing mode\n",
    "            if TESTING_MODE and SAMPLE_SIZE_PER_TISSUE:\n",
    "                tissue_pairs = tissue_pairs[:SAMPLE_SIZE_PER_TISSUE]\n",
    "            \n",
    "            # Create original dataset\n",
    "            datasets[f'{tissue}_original'] = RQ4Dataset(\n",
    "                tissue_pairs, transform=transform, normalize=False, normalizer=None\n",
    "            )\n",
    "            \n",
    "            # Create normalized dataset\n",
    "            datasets[f'{tissue}_normalized'] = RQ4Dataset(\n",
    "                tissue_pairs, transform=transform, normalize=True, normalizer=normalizer\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"âœ… Created {len(datasets)} datasets\")\n",
    "        \n",
    "        # Step 2: Run evaluation on test data\n",
    "        logger.info(\"ðŸ”¬ Step 2: Running comprehensive evaluation...\")\n",
    "        \n",
    "        # Combine all test data\n",
    "        all_test_pairs = []\n",
    "        for tissue in selected_tissues:\n",
    "            tissue_pairs = tissue_data[tissue]\n",
    "            if TESTING_MODE and SAMPLE_SIZE_PER_TISSUE:\n",
    "                tissue_pairs = tissue_pairs[:SAMPLE_SIZE_PER_TISSUE]\n",
    "            all_test_pairs.extend(tissue_pairs)\n",
    "        \n",
    "        # Create evaluation dataset\n",
    "        eval_dataset = RQ4Dataset(all_test_pairs, transform=transform, normalize=False)\n",
    "        \n",
    "        # Run evaluation\n",
    "        evaluation_results = evaluator.evaluate_dataset(\n",
    "            eval_dataset, \n",
    "            max_samples=100 if TESTING_MODE else None\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"âœ… Evaluation completed. Processed {len(evaluation_results['baseline'])} samples\")\n",
    "        \n",
    "        # Step 3: Save evaluation results\n",
    "        logger.info(\"ðŸ’¾ Step 3: Saving evaluation results...\")\n",
    "        processed_data = result_manager.save_evaluation_results(evaluation_results, selected_tissues)\n",
    "        \n",
    "        # Step 4: Generate visualizations\n",
    "        logger.info(\"ðŸ“ˆ Step 4: Generating visualizations...\")\n",
    "        \n",
    "        # Create sample visualizations\n",
    "        sample_indices = [0, 1, 2] if len(evaluation_results['baseline']) >= 3 else [0]\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            if idx >= len(evaluation_results['baseline']):\n",
    "                break\n",
    "                \n",
    "            # Get sample data\n",
    "            sample = eval_dataset[idx]\n",
    "            image = sample['image']\n",
    "            mask = sample['mask']\n",
    "            \n",
    "            # Generate Grad-CAM visualizations\n",
    "            if 'gradcam_original' in models and len(evaluation_results['gradcam_original']) > idx:\n",
    "                try:\n",
    "                    # Original Grad-CAM\n",
    "                    original_cam = evaluation_results['gradcam_original'][idx]['attention_map']\n",
    "                    \n",
    "                    # Create comparison plot\n",
    "                    save_path = result_manager.rq4_dir / 'visualizations' / 'gradcam_maps' / f'sample_{i}_comparison.png'\n",
    "                    visualizer.plot_comparison_grid(\n",
    "                        [image], [mask], [original_cam], \n",
    "                        [f'Sample {i+1}'], save_path\n",
    "                    )\n",
    "                    \n",
    "                    # Create attention comparison if normalized available\n",
    "                    if 'gradcam_normalized' in evaluation_results and len(evaluation_results['gradcam_normalized']) > idx:\n",
    "                        normalized_cam = evaluation_results['gradcam_normalized'][idx]['attention_map']\n",
    "                        comparison_path = result_manager.rq4_dir / 'visualizations' / 'comparison_plots' / f'sample_{i}_attention_comparison.png'\n",
    "                        visualizer.plot_attention_comparison(\n",
    "                            original_cam, normalized_cam, image, comparison_path\n",
    "                        )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Visualization failed for sample {i}: {e}\")\n",
    "        \n",
    "        # Step 5: Statistical analysis\n",
    "        logger.info(\"ðŸ“Š Step 5: Performing statistical analysis...\")\n",
    "        \n",
    "        # Update statistical analyzer with results\n",
    "        statistical_analyzer.results = evaluation_results\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        statistical_report = statistical_analyzer.generate_comprehensive_report()\n",
    "        \n",
    "        # Save statistical analysis results\n",
    "        result_manager.save_statistical_analysis(statistical_report)\n",
    "        \n",
    "        # Step 6: Save attention maps\n",
    "        logger.info(\"ðŸ’¾ Step 6: Saving attention maps...\")\n",
    "        result_manager.save_attention_maps(evaluation_results, sample_indices)\n",
    "        \n",
    "        # Step 7: Generate summary visualizations\n",
    "        logger.info(\"ðŸ“Š Step 7: Generating summary visualizations...\")\n",
    "        \n",
    "        # Prepare data for visualization\n",
    "        df = statistical_analyzer.prepare_data_for_analysis()\n",
    "        \n",
    "        # Create metrics comparison plot\n",
    "        if len(df) > 0:\n",
    "            metrics_save_path = result_manager.rq4_dir / 'visualizations' / 'metrics_plots' / 'rq4_metrics_comparison.png'\n",
    "            visualizer.plot_metrics_comparison(df, metrics_save_path)\n",
    "        \n",
    "        # Step 8: Create final comprehensive report\n",
    "        logger.info(\"ðŸ“‹ Step 8: Creating final comprehensive report...\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Create final report\n",
    "        final_report = result_manager.create_final_report(\n",
    "            evaluation_results, statistical_report, execution_time\n",
    "        )\n",
    "        \n",
    "        # Print comprehensive results summary\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"ðŸŽ¯ RQ4 GRAD-CAM EXPLAINABILITY ANALYSIS - COMPREHENSIVE RESULTS\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "        print(f\"   â€¢ Total samples evaluated: {len(evaluation_results['baseline'])}\")\n",
    "        print(f\"   â€¢ Tissues analyzed: {', '.join(selected_tissues)}\")\n",
    "        print(f\"   â€¢ Model variants: {len([k for k in evaluation_results.keys() if evaluation_results[k]])}\")\n",
    "        \n",
    "        print(f\"\\nðŸ”¬ Statistical Analysis Results:\")\n",
    "        for metric, anova_result in statistical_report['anova_results'].items():\n",
    "            if anova_result:\n",
    "                significance = \"âœ… SIGNIFICANT\" if anova_result['significant'] else \"âŒ NOT SIGNIFICANT\"\n",
    "                print(f\"   â€¢ {metric}: F={anova_result['f_statistic']:.4f}, p={anova_result['p_value']:.4f} {significance}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Key Findings:\")\n",
    "        for metric, result in final_report['key_results'].items():\n",
    "            print(f\"   â€¢ {metric.replace('_', ' ').title()}: {result['interpretation']}\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Results Organization:\")\n",
    "        print(f\"   ðŸ“ Main Directory: {result_manager.rq4_dir}\")\n",
    "        print(f\"   ðŸ“Š Raw Data: {result_manager.rq4_dir / 'results' / 'raw_data'}\")\n",
    "        print(f\"   ðŸ“ˆ Processed Data: {result_manager.rq4_dir / 'results' / 'processed_data'}\")\n",
    "        print(f\"   ðŸ“Š Statistical Analysis: {result_manager.rq4_dir / 'results' / 'statistical_analysis'}\")\n",
    "        print(f\"   ðŸ–¼ï¸  Visualizations: {result_manager.rq4_dir / 'visualizations'}\")\n",
    "        print(f\"   ðŸ§  Attention Maps: {result_manager.rq4_dir / 'data' / 'attention_maps'}\")\n",
    "        print(f\"   ðŸ“‹ Reports: {result_manager.rq4_dir / 'reports'}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ Generated Files:\")\n",
    "        print(f\"   â€¢ RQ4_Final_Report.json - Complete analysis report\")\n",
    "        print(f\"   â€¢ RQ4_Summary_Report.md - Human-readable summary\")\n",
    "        print(f\"   â€¢ evaluation_metrics.csv - Processed metrics data\")\n",
    "        print(f\"   â€¢ anova_results.json - Statistical test results\")\n",
    "        print(f\"   â€¢ *_analysis.csv - Tissue-specific analysis files\")\n",
    "        print(f\"   â€¢ sample_*_comparison.png - Grad-CAM visualizations\")\n",
    "        \n",
    "        print(f\"\\nâ±ï¸  Total execution time: {execution_time:.2f} seconds\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        logger.info(f\"âœ… RQ4 enhanced analysis completed successfully in {execution_time:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'evaluation_results': evaluation_results,\n",
    "            'statistical_report': statistical_report,\n",
    "            'final_report': final_report,\n",
    "            'processed_data': processed_data,\n",
    "            'execution_time': execution_time,\n",
    "            'results_directory': str(result_manager.rq4_dir)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ RQ4 enhanced analysis failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "print(\"âœ… RQ4 enhanced execution pipeline ready with comprehensive result saving\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the enhanced RQ4 analysis with comprehensive result saving\n",
    "print(\"ðŸš€ Starting RQ4 Enhanced Grad-CAM Explainability Analysis...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“ Results will be saved in organized structure:\")\n",
    "print(f\"   Main Directory: {result_manager.rq4_dir}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the enhanced analysis\n",
    "results = run_complete_rq4_analysis_enhanced()\n",
    "\n",
    "print(\"\\nðŸŽ‰ RQ4 Enhanced Analysis Complete!\")\n",
    "print(f\"ðŸ“ All results saved to: {results['results_directory']}\")\n",
    "print(\"\\nðŸ“‹ Quick Access to Key Results:\")\n",
    "print(f\"   â€¢ Final Report: {results['results_directory']}/reports/RQ4_Final_Report.json\")\n",
    "print(f\"   â€¢ Summary Report: {results['results_directory']}/reports/RQ4_Summary_Report.md\")\n",
    "print(f\"   â€¢ Metrics Data: {results['results_directory']}/results/processed_data/evaluation_metrics.csv\")\n",
    "print(f\"   â€¢ Statistical Results: {results['results_directory']}/results/statistical_analysis/anova_results.json\")\n",
    "print(f\"   â€¢ Visualizations: {results['results_directory']}/visualizations/\")\n",
    "print(f\"   â€¢ Attention Maps: {results['results_directory']}/data/attention_maps/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
