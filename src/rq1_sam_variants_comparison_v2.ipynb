{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "# Note: torch/torchvision are environment-specific; install manually if missing.\n",
        "%pip install -q --upgrade pip\n",
        "%pip install -q statsmodels scikit-image seaborn matplotlib pandas scipy opencv-python-headless\n",
        "\n",
        "# Segment Anything\n",
        "try:\n",
        "    import segment_anything  # noqa: F401\n",
        "except Exception:\n",
        "    %pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "# TIAToolbox (official HoVer-Net)\n",
        "try:\n",
        "    import tiatoolbox  # noqa: F401\n",
        "except Exception:\n",
        "    %pip install -q tiatoolbox\n",
        "\n",
        "# TorchVision (used by lightweight HoverNet fallback)\n",
        "try:\n",
        "    import torchvision  # noqa: F401\n",
        "except Exception:\n",
        "    # If this fails, install torch/torchvision per your platform instructions\n",
        "    %pip install -q torchvision\n",
        "\n",
        "print(\"✅ Dependencies installation cell executed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dependency installation / verification (runs in Python, no magics)\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def ensure_pkg(module_name: str, pip_spec: str | None = None) -> None:\n",
        "    try:\n",
        "        importlib.import_module(module_name)\n",
        "        print(f\"{module_name} OK\")\n",
        "    except Exception:\n",
        "        spec = pip_spec or module_name\n",
        "        print(f\"Installing {spec} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", spec])\n",
        "        importlib.import_module(module_name)\n",
        "        print(f\"{module_name} OK\")\n",
        "\n",
        "# Core scientific & plotting\n",
        "for mod, spec in [\n",
        "    (\"statsmodels\", \"statsmodels\"),\n",
        "    (\"skimage\", \"scikit-image\"),\n",
        "    (\"seaborn\", \"seaborn\"),\n",
        "    (\"matplotlib\", \"matplotlib\"),\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"scipy\", \"scipy\"),\n",
        "    (\"PIL\", \"Pillow\"),\n",
        "    (\"cv2\", \"opencv-python-headless\"),\n",
        "]:\n",
        "    ensure_pkg(mod, spec)\n",
        "\n",
        "# Segment Anything\n",
        "try:\n",
        "    importlib.import_module(\"segment_anything\")\n",
        "    print(\"segment_anything OK\")\n",
        "except Exception:\n",
        "    print(\"Installing Segment Anything from GitHub ...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/facebookresearch/segment-anything.git\"])\n",
        "    importlib.import_module(\"segment_anything\")\n",
        "    print(\"segment_anything OK\")\n",
        "\n",
        "# TIAToolbox (official HoVer-Net)\n",
        "ensure_pkg(\"tiatoolbox\", \"tiatoolbox\")\n",
        "\n",
        "# TorchVision (fallback HoverNet uses it; attempt install if missing)\n",
        "try:\n",
        "    importlib.import_module(\"torchvision\")\n",
        "    print(\"torchvision OK\")\n",
        "except Exception:\n",
        "    try:\n",
        "        print(\"Installing torchvision ... (ensure it matches your torch version)\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torchvision\"])\n",
        "        importlib.import_module(\"torchvision\")\n",
        "        print(\"torchvision OK\")\n",
        "    except Exception:\n",
        "        print(\"Warning: torchvision install failed; if using TIAToolbox HoVer-Net this is optional.\")\n",
        "\n",
        "# Torch presence + CUDA check\n",
        "try:\n",
        "    import torch  # noqa: F401\n",
        "    import torch as _torch\n",
        "    print(\"torch OK, CUDA:\", _torch.cuda.is_available())\n",
        "except Exception:\n",
        "    print(\"Warning: torch not available. Install a platform-appropriate torch manually if needed.\")\n",
        "\n",
        "print(\"✅ Dependency check complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RQ1: SAM Variants vs Established Models on PanNuke\n",
        "\n",
        "**Research Question**: Do different variants of the Segment Anything Model (SAM), including domain-adapted PathoSAM, achieve competitive or superior nuclei instance segmentation performance on PanNuke compared to established models (HoVer-Net, CellViT, LKCell) and a U-Net baseline?\n",
        "\n",
        "- **H0 (Null)**: SAM variants do not significantly outperform established models in mPQ or detection F1.\n",
        "- **H1 (Alt.)**: At least one SAM variant significantly outperforms baselines in mPQ or detection F1.\n",
        "\n",
        "### What this notebook does\n",
        "- Loads PanNuke tiles via a reusable dataset\n",
        "- Runs inference for available models:\n",
        "  - SAM variants (if checkpoints available)\n",
        "  - U-Net baseline (checkpoint-gated)\n",
        "- Converts predictions to instance masks and computes: PQ, object F1, AJI, Dice\n",
        "- Performs paired statistics with multiple-comparison correction\n",
        "- Saves CSVs, figures, and an HTML report under `reports/rq1`\n",
        "\n",
        "Note: HoVer-Net, CellViT, and LKCell slots are scaffolded for future integration; this notebook focuses on SAM variants and a U-Net baseline to establish a robust, reproducible evaluation pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Project-local imports\n",
        "import sys\n",
        "if \"__file__\" in globals():\n",
        "    SRC_DIR = Path(__file__).resolve().parent\n",
        "else:\n",
        "    SRC_DIR = Path.cwd()\n",
        "sys.path.append(str(SRC_DIR))\n",
        "from datasets.pannuke_tissue_dataset import PanNukeTissueDataset\n",
        "from models.unet import UNet\n",
        "\n",
        "# Metrics (instance-aware)\n",
        "from metrics.seg_metrics import (\n",
        "    reconstruct_instances,\n",
        "    dice_coefficient,\n",
        "    aji_aggregated_jaccard,\n",
        "    pq_panoptic,\n",
        "    f1_object,\n",
        ")\n",
        "\n",
        "# Reproducibility & device\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = SRC_DIR.parent\n",
        "DATASET_TISSUES = PROJECT_ROOT / \"dataset_tissues\"\n",
        "REPORTS_DIR = PROJECT_ROOT / \"reports\" / \"rq1\"\n",
        "FIG_DIR = REPORTS_DIR / \"figures\"\n",
        "CSV_DIR = REPORTS_DIR / \"tables\"\n",
        "for d in [REPORTS_DIR, FIG_DIR, CSV_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"notebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset setup\n",
        "available_tissues = [p.name for p in DATASET_TISSUES.iterdir() if p.is_dir()]\n",
        "print(\"Tissues:\", len(available_tissues))\n",
        "\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "# Simple transforms via dataset defaults; they already resize/normalize if needed\n",
        "\n",
        "def make_loader(tissue: str, split: str = \"test\") -> DataLoader:\n",
        "    ds = PanNukeTissueDataset(\n",
        "        str(DATASET_TISSUES / tissue),\n",
        "        split=split,\n",
        "        image_transform=None,\n",
        "        target_transform=None,\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Small EDA count table\n",
        "records = []\n",
        "for t in sorted(available_tissues):\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        try:\n",
        "            ds = PanNukeTissueDataset(str(DATASET_TISSUES / t), split=split)\n",
        "            records.append({\"tissue\": t, \"split\": split, \"n\": len(ds)})\n",
        "        except Exception:\n",
        "            pass\n",
        "eda_df = pd.DataFrame(records).pivot(index=\"tissue\", columns=\"split\", values=\"n\").fillna(0).astype(int)\n",
        "eda_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# U-Net baseline loader (checkpoint optional)\n",
        "\n",
        "def load_unet_checkpoint(ckpt_path: Path, num_classes: int = 7) -> nn.Module:\n",
        "    model = UNet(in_channels=3, num_classes=num_classes)\n",
        "    if ckpt_path.exists():\n",
        "        state = torch.load(ckpt_path, map_location=device)\n",
        "        # allow raw state or dict\n",
        "        state_dict = state.get('model_state', state)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print(f\"Loaded U-Net weights from {ckpt_path}\")\n",
        "    else:\n",
        "        print(f\"U-Net checkpoint not found at {ckpt_path}; using randomly initialized model\")\n",
        "    model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "# SAM wrapper (automatic mask generation -> instance map)\n",
        "try:\n",
        "    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "    SAM_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    print(\"SAM not available:\", e)\n",
        "    SAM_AVAILABLE = False\n",
        "\n",
        "class SAMWrapper:\n",
        "    def __init__(self, model_type: str, checkpoint: str | None = None):\n",
        "        assert SAM_AVAILABLE, \"segment_anything not installed\"\n",
        "        if checkpoint and os.path.isfile(checkpoint):\n",
        "            self.sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "        else:\n",
        "            self.sam = sam_model_registry[model_type]()\n",
        "        self.mask_gen = SamAutomaticMaskGenerator(\n",
        "            model=self.sam,\n",
        "            points_per_side=32,\n",
        "            pred_iou_thresh=0.7,\n",
        "            stability_score_thresh=0.92,\n",
        "            crop_n_layers=1,\n",
        "            crop_n_points_downscale_factor=2,\n",
        "            min_mask_region_area=80,\n",
        "        )\n",
        "    @torch.no_grad()\n",
        "    def predict_instances(self, image_np: np.ndarray) -> np.ndarray:\n",
        "        # image_np: HxWx3 uint8\n",
        "        masks = self.mask_gen.generate(image_np)\n",
        "        if not masks:\n",
        "            return np.zeros(image_np.shape[:2], dtype=np.int32)\n",
        "        inst = np.zeros(image_np.shape[:2], dtype=np.int32)\n",
        "        for idx, m in enumerate(masks, start=1):\n",
        "            inst[m['segmentation'].astype(bool)] = idx\n",
        "        return inst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference helpers\n",
        "from PIL import Image\n",
        "\n",
        "@torch.no_grad()\n",
        "def unet_predict_instances(model: nn.Module, img_tensor: torch.Tensor) -> np.ndarray:\n",
        "    # img_tensor: 3xHxW (normalized)\n",
        "    model.eval()\n",
        "    logits = model(img_tensor.unsqueeze(0).to(device))\n",
        "    sem = torch.argmax(logits, dim=1).squeeze(0).detach().cpu().numpy().astype(np.uint8)\n",
        "    # derive boundary from semantic changes\n",
        "    pad = np.pad(sem, 1, mode='edge')\n",
        "    boundary = (sem != pad[1:-1, 1:-1]).astype(np.uint8)\n",
        "    inst = reconstruct_instances(sem, boundary)\n",
        "    return inst\n",
        "\n",
        "\n",
        "def tensor_to_uint8(rgb_tensor: torch.Tensor) -> np.ndarray:\n",
        "    # approximate inverse of default normalization for visualization/SAM\n",
        "    arr = rgb_tensor.permute(1,2,0).cpu().numpy()\n",
        "    arr = arr * np.array([0.229, 0.224, 0.225])[None,None,:] + np.array([0.485, 0.456, 0.406])[None,None,:]\n",
        "    arr = np.clip(arr, 0, 1)\n",
        "    return (arr * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "def evaluate_on_tissue(tissue: str, models: Dict[str, object], n_limit: int | None = None) -> List[Dict]:\n",
        "    loader = make_loader(tissue, split=\"test\")\n",
        "    results = []\n",
        "    seen = 0\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        images, targets = batch  # targets are semantic gt\n",
        "        for b in range(images.shape[0]):\n",
        "            if n_limit is not None and seen >= n_limit:\n",
        "                return results\n",
        "            img_t = images[b]\n",
        "            gt_sem = targets[b].numpy()\n",
        "            # GT instance reconstruction from sem + boundary\n",
        "            pad = np.pad(gt_sem, 1, mode='edge')\n",
        "            gt_boundary = (gt_sem != pad[1:-1, 1:-1]).astype(np.uint8)\n",
        "            gt_inst = reconstruct_instances(gt_sem, gt_boundary)\n",
        "            # Per-image id for pairing\n",
        "            image_id = f\"{tissue}/test/{batch_idx:05d}_{b}\"\n",
        "            # Evaluate each model (generic: if wrapper exposes predict_instances, use it)\n",
        "            for name, model in models.items():\n",
        "                if hasattr(model, 'predict_instances'):\n",
        "                    img_u8 = tensor_to_uint8(img_t)\n",
        "                    pred_inst = model.predict_instances(img_u8)\n",
        "                else:\n",
        "                    pred_inst = unet_predict_instances(model, img_t)\n",
        "                # Metrics\n",
        "                pq = pq_panoptic(gt_inst, pred_inst)\n",
        "                f1o = f1_object(gt_inst, pred_inst)\n",
        "                aji = aji_aggregated_jaccard(gt_inst, pred_inst)\n",
        "                dice = dice_coefficient(gt_sem, (pred_inst > 0).astype(np.uint8), num_classes=2, ignore_background=False)\n",
        "                results.append({\n",
        "                    \"tissue\": tissue,\n",
        "                    \"image_id\": image_id,\n",
        "                    \"model\": name,\n",
        "                    \"pq\": pq,\n",
        "                    \"f1_object\": f1o,\n",
        "                    \"aji\": aji,\n",
        "                    \"dice_bin\": dice,\n",
        "                })\n",
        "            seen += 1\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure models (gate by availability)\n",
        "MODELS: Dict[str, object] = {}\n",
        "\n",
        "# U-Net baseline checkpoint (update if you have a trained model)\n",
        "UNET_CKPT = PROJECT_ROOT / \"artifacts\" / \"rq3_enhanced\" / \"checkpoints\" / \"unet_original_enhanced_best.pth\"\n",
        "MODELS[\"unet_baseline\"] = load_unet_checkpoint(UNET_CKPT, num_classes=7)\n",
        "\n",
        "# SAM variants (require segment_anything + optional checkpoints)\n",
        "if SAM_AVAILABLE:\n",
        "    try:\n",
        "        MODELS[\"sam_vit_b\"] = SAMWrapper(\"vit_b\")\n",
        "    except Exception as e:\n",
        "        print(\"Skipping sam_vit_b:\", e)\n",
        "    try:\n",
        "        MODELS[\"sam_vit_l\"] = SAMWrapper(\"vit_l\")\n",
        "    except Exception as e:\n",
        "        print(\"Skipping sam_vit_l:\", e)\n",
        "    try:\n",
        "        MODELS[\"sam_vit_h\"] = SAMWrapper(\"vit_h\")\n",
        "    except Exception as e:\n",
        "        print(\"Skipping sam_vit_h:\", e)\n",
        "\n",
        "print(\"Models configured:\", list(MODELS.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TIAToolbox HoVer-Net integration (official)\n",
        "try:\n",
        "    from tiatoolbox.models.architecture.hovernet import HoVerNet\n",
        "    from tiatoolbox.models.engine.semantic_segmentor import SemanticSegmentor\n",
        "    from tiatoolbox.utils.transforms import imresize\n",
        "    TIA_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    print(\"TIAToolbox not available:\", e)\n",
        "    TIA_AVAILABLE = False\n",
        "\n",
        "class TIAHoverNetWrapper:\n",
        "    def __init__(self, weights_path: str | None = None, pretrained: str | None = 'hovernet_fast-pannuke'):\n",
        "        assert TIA_AVAILABLE, \"TIAToolbox is required for HoVer-Net integration\"\n",
        "        # SemanticSegmentor wraps HoVerNet with official postprocessing\n",
        "        self.segmentor = SemanticSegmentor(backbone='hovernet', pretrained=pretrained)\n",
        "        if weights_path and os.path.isfile(weights_path):\n",
        "            self.segmentor.model.load_state_dict(torch.load(weights_path, map_location='cpu'), strict=False)\n",
        "        self.segmentor.model.to(device)\n",
        "        self.segmentor.model.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_instances(self, image_np: np.ndarray) -> np.ndarray:\n",
        "        # TIAToolbox expects uint8 RGB; returns dict with inst_map for PanNuke models\n",
        "        # resize to 256 if needed to match training size\n",
        "        out = self.segmentor.predict(np.expand_dims(image_np, 0))\n",
        "        # out is list of dicts; prefer 'inst_map' when available\n",
        "        if isinstance(out, list) and len(out) > 0:\n",
        "            pred = out[0]\n",
        "            if isinstance(pred, dict) and 'inst_map' in pred:\n",
        "                return pred['inst_map'].astype(np.int32)\n",
        "        # Fallback: no instances\n",
        "        return np.zeros(image_np.shape[:2], dtype=np.int32)\n",
        "\n",
        "# Add TIAToolbox HoVer-Net (if available)\n",
        "try:\n",
        "    if TIA_AVAILABLE:\n",
        "        MODELS['hovernet_tia'] = TIAHoverNetWrapper(pretrained='hovernet_fast-pannuke')\n",
        "        print('Added TIAToolbox HoVer-Net (pretrained: hovernet_fast-pannuke)')\n",
        "except Exception as e:\n",
        "    print('Skipping TIAToolbox HoVer-Net:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HoVer-Net integration (inline lightweight implementation)\n",
        "class HoverNet(nn.Module):\n",
        "    def __init__(self, num_classes: int = 7, backbone: str = 'resnet34'):\n",
        "        super().__init__()\n",
        "        import torchvision.models as tvm\n",
        "        if backbone == 'resnet50':\n",
        "            m = tvm.resnet50(weights=tvm.ResNet50_Weights.DEFAULT)\n",
        "            enc_channels = 2048\n",
        "        else:\n",
        "            m = tvm.resnet34(weights=tvm.ResNet34_Weights.DEFAULT)\n",
        "            enc_channels = 512\n",
        "        self.encoder = nn.Sequential(*list(m.children())[:-2])\n",
        "        self.up1 = nn.ConvTranspose2d(enc_channels, 256, 2, 2)\n",
        "        self.dec1 = nn.Sequential(nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(True))\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
        "        self.dec2 = nn.Sequential(nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(True))\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
        "        self.dec3 = nn.Sequential(nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(True))\n",
        "        self.up4 = nn.ConvTranspose2d(64, 64, 2, 2)\n",
        "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        f = self.encoder(x)\n",
        "        x = self.up1(f)\n",
        "        x = self.dec1(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.dec2(x)\n",
        "        x = self.up3(x)\n",
        "        x = self.dec3(x)\n",
        "        x = self.up4(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def load_hovernet_checkpoint(ckpt_path: Path, num_classes: int = 7) -> nn.Module:\n",
        "    model = HoverNet(num_classes=num_classes)\n",
        "    if ckpt_path.exists():\n",
        "        state = torch.load(ckpt_path, map_location=device)\n",
        "        if isinstance(state, dict) and any(k.startswith('model') for k in state.keys()):\n",
        "            state = state.get('model_state', state.get('model_state_dict', state))\n",
        "        model.load_state_dict(state, strict=False)\n",
        "        print(f'Loaded HoVer-Net weights from {ckpt_path}')\n",
        "    else:\n",
        "        print(f'HoVer-Net checkpoint not found at {ckpt_path}; using randomly initialized model')\n",
        "    model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "# Add HoVer-Net\n",
        "HOVERNET_CKPT = PROJECT_ROOT / 'artifacts' / 'rq1' / 'checkpoints' / 'hovernet_best.pth'\n",
        "try:\n",
        "    MODELS['hovernet'] = load_hovernet_checkpoint(HOVERNET_CKPT, num_classes=7)\n",
        "except Exception as e:\n",
        "    print('Skipping HoVer-Net:', e)\n",
        "\n",
        "print('Models configured (after HoVer-Net attempt):', list(MODELS.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation across tissues (limit per-tissue for quick pass)\n",
        "ALL_ROWS: List[Dict] = []\n",
        "per_tissue_limit = 50  # increase for full evaluation\n",
        "\n",
        "for tissue in sorted(available_tissues):\n",
        "    print(f\"Evaluating {tissue} ...\")\n",
        "    rows = evaluate_on_tissue(tissue, MODELS, n_limit=per_tissue_limit)\n",
        "    ALL_ROWS.extend(rows)\n",
        "\n",
        "res_df = pd.DataFrame(ALL_ROWS)\n",
        "print(res_df.head())\n",
        "\n",
        "# Save per-image table\n",
        "csv_path = CSV_DIR / \"per_image_instance_metrics.csv\"\n",
        "res_df.to_csv(csv_path, index=False)\n",
        "print(\"Saved:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary stats per model\n",
        "summary = (\n",
        "    res_df.groupby(\"model\")[\"pq\", \"f1_object\", \"aji\", \"dice_bin\"].agg([\"mean\", \"std\", \"count\"]).round(4)\n",
        ")\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pairwise statistical analysis with BH correction\n",
        "from itertools import combinations\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from scipy.stats import ttest_rel, wilcoxon\n",
        "\n",
        "metrics = [\"pq\", \"f1_object\", \"aji\", \"dice_bin\"]\n",
        "models = res_df[\"model\"].unique().tolist()\n",
        "\n",
        "pairwise_rows = []\n",
        "for m1, m2 in combinations(models, 2):\n",
        "    df1 = res_df[res_df.model == m1].set_index([\"tissue\", \"image_id\"])  # align by image\n",
        "    df2 = res_df[res_df.model == m2].set_index([\"tissue\", \"image_id\"])  # same ids\n",
        "    common_idx = df1.index.intersection(df2.index)\n",
        "    if len(common_idx) < 5:\n",
        "        continue\n",
        "    for metric in metrics:\n",
        "        x = df1.loc[common_idx, metric].values\n",
        "        y = df2.loc[common_idx, metric].values\n",
        "        if len(x) != len(y) or len(x) < 5:\n",
        "            continue\n",
        "        # Paired tests\n",
        "        t_stat, t_p = ttest_rel(x, y, nan_policy='omit')\n",
        "        try:\n",
        "            w_stat, w_p = wilcoxon(x, y)\n",
        "        except Exception:\n",
        "            w_stat, w_p = np.nan, np.nan\n",
        "        diff = np.nanmean(x - y)\n",
        "        pairwise_rows.append({\n",
        "            \"model1\": m1,\n",
        "            \"model2\": m2,\n",
        "            \"metric\": metric,\n",
        "            \"n\": int(len(x)),\n",
        "            \"mean_diff\": float(diff),\n",
        "            \"t_p\": float(t_p) if np.isfinite(t_p) else 1.0,\n",
        "            \"w_p\": float(w_p) if np.isfinite(w_p) else 1.0,\n",
        "        })\n",
        "\n",
        "pairwise_df = pd.DataFrame(pairwise_rows)\n",
        "if not pairwise_df.empty:\n",
        "    # BH correction per metric separately\n",
        "    corrected = []\n",
        "    for metric, g in pairwise_df.groupby(\"metric\"):\n",
        "        for col in [\"t_p\", \"w_p\"]:\n",
        "            rej, p_bh, _, _ = multipletests(g[col].values, method='fdr_bh')\n",
        "            g[col+\"_bh\"] = p_bh\n",
        "            g[col+\"_sig_bh\"] = rej\n",
        "        corrected.append(g)\n",
        "    pairwise_df = pd.concat(corrected, ignore_index=True)\n",
        "\n",
        "pairwise_csv = CSV_DIR / \"pairwise_stats_bh.csv\"\n",
        "pairwise_df.to_csv(pairwise_csv, index=False)\n",
        "print(\"Saved:\", pairwise_csv)\n",
        "\n",
        "pairwise_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(data=res_df, x=\"model\", y=\"pq\")\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.title(\"PQ by model\")\n",
        "plt.tight_layout()\n",
        "fig_path1 = FIG_DIR / \"pq_by_model.png\"\n",
        "plt.savefig(fig_path1, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(data=res_df, x=\"model\", y=\"f1_object\")\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.title(\"Object F1 by model\")\n",
        "plt.tight_layout()\n",
        "fig_path2 = FIG_DIR / \"f1_by_model.png\"\n",
        "plt.savefig(fig_path2, dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved:\", fig_path1)\n",
        "print(\"Saved:\", fig_path2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-tissue paired Wilcoxon tests (BH corrected)\n",
        "from itertools import product\n",
        "\n",
        "metrics_primary = [\"pq\", \"f1_object\"]\n",
        "sam_models = [m for m in res_df.model.unique() if m.startswith(\"sam\")]\n",
        "established = [m for m in [\"hovernet\", \"cellvit\", \"lkcell\"] if m in res_df.model.unique()]\n",
        "\n",
        "rows = []\n",
        "for tissue in sorted(res_df.tissue.unique()):\n",
        "    df_t = res_df[res_df.tissue == tissue].set_index([\"tissue\", \"image_id\"])  # align pairs\n",
        "    for sam, est, metric in product(sam_models, established, metrics_primary):\n",
        "        a = df_t[df_t.model == sam][metric]\n",
        "        b = df_t[df_t.model == est][metric]\n",
        "        idx = a.index.intersection(b.index)\n",
        "        if len(idx) < 5:\n",
        "            continue\n",
        "        x, y = a.loc[idx].values, b.loc[idx].values\n",
        "        try:\n",
        "            stat, p = wilcoxon(x, y)\n",
        "        except Exception:\n",
        "            p = 1.0\n",
        "        rows.append({\n",
        "            \"tissue\": tissue,\n",
        "            \"sam\": sam,\n",
        "            \"established\": est,\n",
        "            \"metric\": metric,\n",
        "            \"n\": int(len(idx)),\n",
        "            \"mean_diff\": float(np.nanmean(x - y)),\n",
        "            \"wilcoxon_p\": float(p)\n",
        "        })\n",
        "\n",
        "tissue_df = pd.DataFrame(rows)\n",
        "if not tissue_df.empty:\n",
        "    outs = []\n",
        "    for metric, g in tissue_df.groupby(\"metric\"):\n",
        "        rej, p_bh, _, _ = multipletests(g[\"wilcoxon_p\"].values, method=\"fdr_bh\")\n",
        "        g = g.assign(wilcoxon_p_bh=p_bh, sig_bh=rej)\n",
        "        outs.append(g)\n",
        "    tissue_df_bh = pd.concat(outs, ignore_index=True)\n",
        "else:\n",
        "    tissue_df_bh = pd.DataFrame(columns=[\"tissue\",\"sam\",\"established\",\"metric\",\"n\",\"mean_diff\",\"wilcoxon_p\",\"wilcoxon_p_bh\",\"sig_bh\"])\n",
        "\n",
        "per_tissue_csv = CSV_DIR / \"per_tissue_wilcoxon_bh.csv\"\n",
        "tissue_df_bh.to_csv(per_tissue_csv, index=False)\n",
        "print(\"Saved:\", per_tissue_csv)\n",
        "\n",
        "tissue_df_bh.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HTML report (with per-tissue section)\n",
        "from datetime import datetime\n",
        "\n",
        "report_html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html><head><meta charset='utf-8'><title>RQ1 - SAM Variants vs Baselines</title></head>\n",
        "<body style='font-family:Segoe UI,Arial,sans-serif; margin:40px;'>\n",
        "<h1>RQ1: SAM Variants vs Established Models on PanNuke</h1>\n",
        "<p><em>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</em></p>\n",
        "<h2>Per-image metrics</h2>\n",
        "<p>Saved CSV: {csv_path.name}</p>\n",
        "<h2>Summary (by model)</h2>\n",
        "{summary.to_html()}\n",
        "<h2>Pairwise statistics (BH corrected)</h2>\n",
        "{pairwise_df.head(50).to_html(index=False) if 'pairwise_df' in globals() and not pairwise_df.empty else '<p>No pairwise results.</p>'}\n",
        "<h2>Per-tissue paired Wilcoxon (BH corrected)</h2>\n",
        "{tissue_df_bh.head(100).to_html(index=False) if 'tissue_df_bh' in globals() and not tissue_df_bh.empty else '<p>No per-tissue results.</p>'}\n",
        "<h2>Figures</h2>\n",
        "<ul>\n",
        "  <li>{fig_path1.name}</li>\n",
        "  <li>{fig_path2.name}</li>\n",
        "</ul>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "html_path = REPORTS_DIR / \"RQ1_SAM_Variants_Report.html\"\n",
        "html_path.write_text(report_html, encoding='utf-8')\n",
        "print(\"Saved:\", html_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HTML report\n",
        "from datetime import datetime\n",
        "\n",
        "report_html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html><head><meta charset='utf-8'><title>RQ1 - SAM Variants vs Baselines</title></head>\n",
        "<body style='font-family:Segoe UI,Arial,sans-serif; margin:40px;'>\n",
        "<h1>RQ1: SAM Variants vs Established Models on PanNuke</h1>\n",
        "<p><em>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</em></p>\n",
        "<h2>Per-image metrics</h2>\n",
        "<p>Saved CSV: {csv_path.name}</p>\n",
        "<h2>Summary (by model)</h2>\n",
        "{summary.to_html()}\n",
        "<h2>Pairwise statistics (BH corrected)</h2>\n",
        "{pairwise_df.head(50).to_html(index=False)}\n",
        "<h2>Figures</h2>\n",
        "<ul>\n",
        "  <li>{fig_path1.name}</li>\n",
        "  <li>{fig_path2.name}</li>\n",
        "</ul>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "html_path = REPORTS_DIR / \"RQ1_SAM_Variants_Report.html\"\n",
        "html_path.write_text(report_html, encoding='utf-8')\n",
        "print(\"Saved:\", html_path)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
