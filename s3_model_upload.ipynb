{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Backup and S3 Upload Notebook\n",
        "\n",
        "This notebook provides functionality to:\n",
        "1. Zip model artifacts while preserving directory structure\n",
        "2. Upload to AWS S3 bucket\n",
        "3. Maintain the same folder structure in S3\n",
        "4. Provide progress tracking and error handling\n",
        "\n",
        "## Prerequisites\n",
        "- AWS credentials configured (access key, secret key)\n",
        "- boto3 installed (`pip install boto3`)\n",
        "- Sufficient disk space for creating zip files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration Section\n",
        "\n",
        "**âš ï¸ Security Note**: Never commit AWS credentials to version control. Use environment variables or AWS credentials file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY_ID = \"your-access-key-here\"  # Replace with your access key\n",
        "AWS_SECRET_ACCESS_KEY = \"your-secret-key-here\"  # Replace with your secret key\n",
        "AWS_REGION = \"us-east-1\"  # Replace with your preferred region\n",
        "S3_BUCKET_NAME = \"your-bucket-name\"  # Replace with your S3 bucket name\n",
        "\n",
        "# Alternatively, use environment variables (recommended for security)\n",
        "# AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "# AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "# AWS_REGION = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
        "# S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\n",
        "\n",
        "# Project Configuration\n",
        "PROJECT_ROOT = \"/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch\"\n",
        "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, \"artifacts\")\n",
        "BACKUP_DIR = os.path.join(PROJECT_ROOT, \"backups\")\n",
        "\n",
        "# S3 Key Prefix (folder structure in S3)\n",
        "S3_KEY_PREFIX = f\"histopathology-research/{datetime.now().strftime('%Y-%m-%d')}\"\n",
        "\n",
        "print(f\"âœ… Configuration loaded\")\n",
        "print(f\"ğŸ“ Artifacts directory: {ARTIFACTS_DIR}\")\n",
        "print(f\"ğŸ’¾ Backup directory: {BACKUP_DIR}\")\n",
        "print(f\"ğŸª£ S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"ğŸ”‘ S3 Key Prefix: {S3_KEY_PREFIX}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import boto3\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "import hashlib\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ğŸ“š All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_directory_size(directory: str) -> int:\n",
        "    \"\"\"Calculate total size of directory in bytes.\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            if os.path.exists(filepath):\n",
        "                total_size += os.path.getsize(filepath)\n",
        "    return total_size\n",
        "\n",
        "def format_bytes(bytes_size: int) -> str:\n",
        "    \"\"\"Convert bytes to human readable format.\"\"\"\n",
        "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if bytes_size < 1024.0:\n",
        "            return f\"{bytes_size:.2f} {unit}\"\n",
        "        bytes_size /= 1024.0\n",
        "    return f\"{bytes_size:.2f} PB\"\n",
        "\n",
        "def calculate_file_hash(filepath: str) -> str:\n",
        "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "def create_backup_directory():\n",
        "    \"\"\"Create backup directory if it doesn't exist.\"\"\"\n",
        "    os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "    print(f\"ğŸ“ Backup directory ready: {BACKUP_DIR}\")\n",
        "\n",
        "print(\"ğŸ”§ Utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Directory Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_artifacts_directory():\n",
        "    \"\"\"Analyze the artifacts directory structure and contents.\"\"\"\n",
        "    print(\"ğŸ” Analyzing artifacts directory...\\n\")\n",
        "    \n",
        "    analysis = {\n",
        "        'total_size': 0,\n",
        "        'directories': {},\n",
        "        'file_count': 0\n",
        "    }\n",
        "    \n",
        "    if not os.path.exists(ARTIFACTS_DIR):\n",
        "        print(f\"âŒ Artifacts directory not found: {ARTIFACTS_DIR}\")\n",
        "        return analysis\n",
        "    \n",
        "    # Analyze each research question directory\n",
        "    for rq_dir in ['rq2', 'rq3']:\n",
        "        rq_path = os.path.join(ARTIFACTS_DIR, rq_dir)\n",
        "        if os.path.exists(rq_path):\n",
        "            size = get_directory_size(rq_path)\n",
        "            file_count = sum(len(files) for _, _, files in os.walk(rq_path))\n",
        "            \n",
        "            analysis['directories'][rq_dir] = {\n",
        "                'path': rq_path,\n",
        "                'size': size,\n",
        "                'file_count': file_count,\n",
        "                'formatted_size': format_bytes(size)\n",
        "            }\n",
        "            \n",
        "            analysis['total_size'] += size\n",
        "            analysis['file_count'] += file_count\n",
        "            \n",
        "            print(f\"ğŸ“Š {rq_dir.upper()}:\")\n",
        "            print(f\"   ğŸ“ Path: {rq_path}\")\n",
        "            print(f\"   ğŸ“ Size: {format_bytes(size)}\")\n",
        "            print(f\"   ğŸ“„ Files: {file_count}\")\n",
        "            \n",
        "            # List subdirectories\n",
        "            subdirs = [d for d in os.listdir(rq_path) if os.path.isdir(os.path.join(rq_path, d))]\n",
        "            if subdirs:\n",
        "                print(f\"   ğŸ“‚ Subdirectories: {', '.join(subdirs)}\")\n",
        "            print()\n",
        "    \n",
        "    print(f\"ğŸ“ˆ TOTAL ANALYSIS:\")\n",
        "    print(f\"   ğŸ“ Total size: {format_bytes(analysis['total_size'])}\")\n",
        "    print(f\"   ğŸ“„ Total files: {analysis['file_count']}\")\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Run analysis\n",
        "artifacts_analysis = analyze_artifacts_directory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Zip Creation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_zip_archive(source_dir: str, zip_path: str, exclude_patterns: List[str] = None) -> Dict:\n",
        "    \"\"\"Create a zip archive while preserving directory structure.\"\"\"\n",
        "    if exclude_patterns is None:\n",
        "        exclude_patterns = ['.DS_Store', '__pycache__', '.git']\n",
        "    \n",
        "    zip_info = {\n",
        "        'source_dir': source_dir,\n",
        "        'zip_path': zip_path,\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'files_added': 0,\n",
        "        'original_size': 0,\n",
        "        'compressed_size': 0,\n",
        "        'compression_ratio': 0.0,\n",
        "        'files_list': []\n",
        "    }\n",
        "    \n",
        "    print(f\"ğŸ—œï¸  Creating zip archive...\")\n",
        "    print(f\"   ğŸ“ Source: {source_dir}\")\n",
        "    print(f\"   ğŸ“¦ Destination: {zip_path}\")\n",
        "    \n",
        "    # Get total number of files for progress bar\n",
        "    total_files = sum(len(files) for _, _, files in os.walk(source_dir))\n",
        "    \n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n",
        "        with tqdm(total=total_files, desc=\"Adding files\") as pbar:\n",
        "            for root, dirs, files in os.walk(source_dir):\n",
        "                # Remove excluded directories\n",
        "                dirs[:] = [d for d in dirs if not any(pattern in d for pattern in exclude_patterns)]\n",
        "                \n",
        "                for file in files:\n",
        "                    # Skip excluded files\n",
        "                    if any(pattern in file for pattern in exclude_patterns):\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "                    \n",
        "                    file_path = os.path.join(root, file)\n",
        "                    \n",
        "                    # Calculate relative path to preserve structure\n",
        "                    arcname = os.path.relpath(file_path, source_dir)\n",
        "                    \n",
        "                    try:\n",
        "                        # Add file to zip\n",
        "                        zipf.write(file_path, arcname)\n",
        "                        \n",
        "                        # Update statistics\n",
        "                        file_size = os.path.getsize(file_path)\n",
        "                        zip_info['files_added'] += 1\n",
        "                        zip_info['original_size'] += file_size\n",
        "                        zip_info['files_list'].append({\n",
        "                            'path': arcname,\n",
        "                            'size': file_size,\n",
        "                            'hash': calculate_file_hash(file_path)\n",
        "                        })\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"âš ï¸  Warning: Could not add {file_path}: {e}\")\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "    \n",
        "    # Calculate final statistics\n",
        "    if os.path.exists(zip_path):\n",
        "        zip_info['compressed_size'] = os.path.getsize(zip_path)\n",
        "        if zip_info['original_size'] > 0:\n",
        "            zip_info['compression_ratio'] = (\n",
        "                1 - zip_info['compressed_size'] / zip_info['original_size']\n",
        "            ) * 100\n",
        "    \n",
        "    print(f\"âœ… Zip archive created successfully!\")\n",
        "    print(f\"   ğŸ“„ Files added: {zip_info['files_added']}\")\n",
        "    print(f\"   ğŸ“ Original size: {format_bytes(zip_info['original_size'])}\")\n",
        "    print(f\"   ğŸ—œï¸  Compressed size: {format_bytes(zip_info['compressed_size'])}\")\n",
        "    print(f\"   ğŸ“Š Compression ratio: {zip_info['compression_ratio']:.1f}%\")\n",
        "    \n",
        "    return zip_info\n",
        "\n",
        "print(\"ğŸ—œï¸  Zip creation functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. AWS S3 Upload Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_s3_client():\n",
        "    \"\"\"Initialize and test S3 client.\"\"\"\n",
        "    try:\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "            region_name=AWS_REGION\n",
        "        )\n",
        "        \n",
        "        # Test connection by listing buckets\n",
        "        response = s3_client.list_buckets()\n",
        "        \n",
        "        # Check if our target bucket exists\n",
        "        bucket_names = [bucket['Name'] for bucket in response['Buckets']]\n",
        "        \n",
        "        if S3_BUCKET_NAME in bucket_names:\n",
        "            print(f\"âœ… S3 client initialized successfully\")\n",
        "            print(f\"ğŸª£ Target bucket '{S3_BUCKET_NAME}' found\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  Warning: Bucket '{S3_BUCKET_NAME}' not found in your account\")\n",
        "            print(f\"Available buckets: {bucket_names}\")\n",
        "            \n",
        "        return s3_client\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to initialize S3 client: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_file_to_s3(s3_client, local_path: str, s3_key: str) -> Dict:\n",
        "    \"\"\"Upload a single file to S3 with progress tracking.\"\"\"\n",
        "    upload_info = {\n",
        "        'local_path': local_path,\n",
        "        's3_key': s3_key,\n",
        "        'bucket': S3_BUCKET_NAME,\n",
        "        'success': False,\n",
        "        'upload_time': None,\n",
        "        'file_size': 0,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        file_size = os.path.getsize(local_path)\n",
        "        upload_info['file_size'] = file_size\n",
        "        \n",
        "        print(f\"â¬†ï¸  Uploading: {os.path.basename(local_path)} ({format_bytes(file_size)})\")\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Upload file\n",
        "        s3_client.upload_file(local_path, S3_BUCKET_NAME, s3_key)\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        upload_info['upload_time'] = (end_time - start_time).total_seconds()\n",
        "        upload_info['success'] = True\n",
        "        \n",
        "        print(f\"âœ… Upload completed in {upload_info['upload_time']:.2f} seconds\")\n",
        "        print(f\"   ğŸ”— S3 Location: s3://{S3_BUCKET_NAME}/{s3_key}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        upload_info['error'] = str(e)\n",
        "        print(f\"âŒ Upload failed: {e}\")\n",
        "    \n",
        "    return upload_info\n",
        "\n",
        "print(\"â˜ï¸  S3 upload functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Main Execution: Create Zip Archives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create backup directory\n",
        "create_backup_directory()\n",
        "\n",
        "# Create timestamp for this backup session\n",
        "backup_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Dictionary to store all zip information\n",
        "zip_archives = {}\n",
        "\n",
        "print(f\"ğŸš€ Starting backup process at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"ğŸ“… Backup session ID: {backup_timestamp}\\n\")\n",
        "\n",
        "# Create zip archives for each research question\n",
        "for rq in ['rq2', 'rq3']:\n",
        "    rq_dir = os.path.join(ARTIFACTS_DIR, rq)\n",
        "    \n",
        "    if os.path.exists(rq_dir) and os.listdir(rq_dir):  # Check if directory exists and is not empty\n",
        "        print(f\"\\nğŸ“¦ Processing {rq.upper()}...\")\n",
        "        \n",
        "        # Create zip filename\n",
        "        zip_filename = f\"artifacts_{rq}_{backup_timestamp}.zip\"\n",
        "        zip_path = os.path.join(BACKUP_DIR, zip_filename)\n",
        "        \n",
        "        # Create zip archive\n",
        "        zip_info = create_zip_archive(rq_dir, zip_path)\n",
        "        zip_archives[rq] = zip_info\n",
        "        \n",
        "        print(f\"âœ… {rq.upper()} archive created: {zip_filename}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Skipping {rq.upper()}: Directory empty or doesn't exist\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Zip creation completed!\")\n",
        "print(f\"ğŸ“ All archives saved to: {BACKUP_DIR}\")\n",
        "\n",
        "# Display summary\n",
        "total_original_size = sum(info['original_size'] for info in zip_archives.values())\n",
        "total_compressed_size = sum(info['compressed_size'] for info in zip_archives.values())\n",
        "total_files = sum(info['files_added'] for info in zip_archives.values())\n",
        "\n",
        "print(f\"\\nğŸ“Š BACKUP SUMMARY:\")\n",
        "print(f\"   ğŸ“¦ Archives created: {len(zip_archives)}\")\n",
        "print(f\"   ğŸ“„ Total files: {total_files}\")\n",
        "print(f\"   ğŸ“ Original size: {format_bytes(total_original_size)}\")\n",
        "print(f\"   ğŸ—œï¸  Compressed size: {format_bytes(total_compressed_size)}\")\n",
        "if total_original_size > 0:\n",
        "    overall_compression = (1 - total_compressed_size / total_original_size) * 100\n",
        "    print(f\"   ğŸ“Š Overall compression: {overall_compression:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Initialize S3 Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3 client\n",
        "print(\"ğŸ” Initializing AWS S3 connection...\")\n",
        "s3_client = initialize_s3_client()\n",
        "\n",
        "if s3_client is None:\n",
        "    print(\"\\nâŒ Cannot proceed with S3 upload. Please check your AWS credentials.\")\n",
        "    print(\"\\nğŸ”§ Troubleshooting steps:\")\n",
        "    print(\"   1. Verify AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\")\n",
        "    print(\"   2. Check if the specified region is correct\")\n",
        "    print(\"   3. Ensure your AWS account has S3 permissions\")\n",
        "    print(\"   4. Verify the bucket name exists and you have access\")\n",
        "else:\n",
        "    print(\"\\nğŸ¯ Ready to upload to S3!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Upload to S3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if s3_client and zip_archives:\n",
        "    print(f\"â˜ï¸  Starting S3 upload process...\")\n",
        "    print(f\"ğŸª£ Target bucket: {S3_BUCKET_NAME}\")\n",
        "    print(f\"ğŸ“‚ S3 prefix: {S3_KEY_PREFIX}\\n\")\n",
        "    \n",
        "    upload_results = []\n",
        "    \n",
        "    for rq, zip_info in zip_archives.items():\n",
        "        print(f\"\\nâ¬†ï¸  Uploading {rq.upper()} archive...\")\n",
        "        \n",
        "        # Create S3 key maintaining directory structure\n",
        "        zip_filename = os.path.basename(zip_info['zip_path'])\n",
        "        s3_key = f\"{S3_KEY_PREFIX}/artifacts/{rq}/{zip_filename}\"\n",
        "        \n",
        "        # Upload file\n",
        "        upload_result = upload_file_to_s3(s3_client, zip_info['zip_path'], s3_key)\n",
        "        upload_results.append(upload_result)\n",
        "    \n",
        "    # Create and save upload manifest\n",
        "    manifest = {\n",
        "        'upload_session': {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'project': 'HistoPathology Research',\n",
        "            's3_bucket': S3_BUCKET_NAME,\n",
        "            's3_prefix': S3_KEY_PREFIX\n",
        "        },\n",
        "        'summary': {\n",
        "            'total_files': len(upload_results),\n",
        "            'successful_uploads': sum(1 for r in upload_results if r['success']),\n",
        "            'failed_uploads': sum(1 for r in upload_results if not r['success']),\n",
        "            'total_size': sum(r['file_size'] for r in upload_results),\n",
        "            'total_upload_time': sum(r.get('upload_time', 0) for r in upload_results)\n",
        "        },\n",
        "        'uploads': upload_results\n",
        "    }\n",
        "    \n",
        "    manifest_path = os.path.join(BACKUP_DIR, f\"upload_manifest_{backup_timestamp}.json\")\n",
        "    \n",
        "    with open(manifest_path, 'w') as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    \n",
        "    # Upload manifest to S3\n",
        "    manifest_s3_key = f\"{S3_KEY_PREFIX}/manifests/upload_manifest_{backup_timestamp}.json\"\n",
        "    manifest_upload = upload_file_to_s3(s3_client, manifest_path, manifest_s3_key)\n",
        "    \n",
        "    print(f\"\\nğŸ‰ S3 Upload Process Completed!\")\n",
        "    print(f\"\\nğŸ“Š UPLOAD SUMMARY:\")\n",
        "    print(f\"   âœ… Successful uploads: {manifest['summary']['successful_uploads']}\")\n",
        "    print(f\"   âŒ Failed uploads: {manifest['summary']['failed_uploads']}\")\n",
        "    print(f\"   ğŸ“ Total uploaded: {format_bytes(manifest['summary']['total_size'])}\")\n",
        "    print(f\"   â±ï¸  Total time: {manifest['summary']['total_upload_time']:.2f} seconds\")\n",
        "    print(f\"   ğŸ“„ Manifest saved: {manifest_path}\")\n",
        "    \n",
        "    # Display S3 locations\n",
        "    print(f\"\\nğŸ”— S3 LOCATIONS:\")\n",
        "    for result in upload_results:\n",
        "        if result['success']:\n",
        "            print(f\"   ğŸ“¦ s3://{result['bucket']}/{result['s3_key']}\")\n",
        "    \n",
        "    if manifest_upload['success']:\n",
        "        print(f\"   ğŸ“„ s3://{S3_BUCKET_NAME}/{manifest_s3_key}\")\n",
        "    \n",
        "else:\n",
        "    if not s3_client:\n",
        "        print(\"âŒ S3 client not initialized. Cannot upload.\")\n",
        "    if not zip_archives:\n",
        "        print(\"âŒ No zip archives created. Nothing to upload.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ MODEL BACKUP AND S3 UPLOAD COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nğŸ“… Session: {backup_timestamp}\")\n",
        "print(f\"ğŸ“ Local backups: {BACKUP_DIR}\")\n",
        "print(f\"â˜ï¸  S3 bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"ğŸ“‚ S3 prefix: {S3_KEY_PREFIX}\")\n",
        "\n",
        "if 'zip_archives' in locals() and zip_archives:\n",
        "    print(f\"\\nğŸ“¦ ARCHIVES CREATED:\")\n",
        "    for rq, info in zip_archives.items():\n",
        "        print(f\"   {rq.upper()}: {os.path.basename(info['zip_path'])} ({format_bytes(info['compressed_size'])})\")\n",
        "\n",
        "if 'upload_results' in locals():\n",
        "    successful_uploads = sum(1 for r in upload_results if r['success'])\n",
        "    print(f\"\\nâ˜ï¸  UPLOADS: {successful_uploads}/{len(upload_results)} successful\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ TIP: Keep the manifest file for future reference!\")\n",
        "print(f\"ğŸ“„ Manifest location: {BACKUP_DIR}/upload_manifest_{backup_timestamp}.json\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
