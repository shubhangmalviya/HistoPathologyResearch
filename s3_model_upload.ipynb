{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Backup and S3 Upload Notebook\n",
        "\n",
        "This notebook provides functionality to:\n",
        "1. Zip model artifacts while preserving directory structure\n",
        "2. Upload to AWS S3 bucket\n",
        "3. Maintain the same folder structure in S3\n",
        "4. Provide progress tracking and error handling\n",
        "\n",
        "## Prerequisites\n",
        "- AWS credentials configured (access key, secret key)\n",
        "- boto3 installed (`pip install boto3`)\n",
        "- Sufficient disk space for creating zip files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration Section\n",
        "\n",
        "**‚ö†Ô∏è Security Note**: Never commit AWS credentials to version control. Use environment variables or AWS credentials file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY_ID = \"your-access-key-here\"  # Replace with your access key\n",
        "AWS_SECRET_ACCESS_KEY = \"your-secret-key-here\"  # Replace with your secret key\n",
        "AWS_REGION = \"us-east-1\"  # Replace with your preferred region\n",
        "S3_BUCKET_NAME = \"your-bucket-name\"  # Replace with your S3 bucket name\n",
        "\n",
        "# Alternatively, use environment variables (recommended for security)\n",
        "# AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "# AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "# AWS_REGION = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
        "# S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\n",
        "\n",
        "# Project Configuration\n",
        "PROJECT_ROOT = \"/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch\"\n",
        "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, \"artifacts\")\n",
        "BACKUP_DIR = os.path.join(PROJECT_ROOT, \"backups\")\n",
        "\n",
        "# S3 Key Prefix (folder structure in S3)\n",
        "S3_KEY_PREFIX = f\"histopathology-research/{datetime.now().strftime('%Y-%m-%d')}\"\n",
        "\n",
        "print(f\"‚úÖ Configuration loaded\")\n",
        "print(f\"üìÅ Artifacts directory: {ARTIFACTS_DIR}\")\n",
        "print(f\"üíæ Backup directory: {BACKUP_DIR}\")\n",
        "print(f\"ü™£ S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"üîë S3 Key Prefix: {S3_KEY_PREFIX}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import boto3\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "import hashlib\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üìö All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_directory_size(directory: str) -> int:\n",
        "    \"\"\"Calculate total size of directory in bytes.\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            if os.path.exists(filepath):\n",
        "                total_size += os.path.getsize(filepath)\n",
        "    return total_size\n",
        "\n",
        "def format_bytes(bytes_size: int) -> str:\n",
        "    \"\"\"Convert bytes to human readable format.\"\"\"\n",
        "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if bytes_size < 1024.0:\n",
        "            return f\"{bytes_size:.2f} {unit}\"\n",
        "        bytes_size /= 1024.0\n",
        "    return f\"{bytes_size:.2f} PB\"\n",
        "\n",
        "def calculate_file_hash(filepath: str) -> str:\n",
        "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "def create_backup_directory():\n",
        "    \"\"\"Create backup directory if it doesn't exist.\"\"\"\n",
        "    os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "    print(f\"üìÅ Backup directory ready: {BACKUP_DIR}\")\n",
        "\n",
        "print(\"üîß Utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Directory Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_artifacts_directory():\n",
        "    \"\"\"Analyze the artifacts directory structure and contents.\"\"\"\n",
        "    print(\"üîç Analyzing artifacts directory...\\n\")\n",
        "    \n",
        "    analysis = {\n",
        "        'total_size': 0,\n",
        "        'directories': {},\n",
        "        'file_count': 0\n",
        "    }\n",
        "    \n",
        "    if not os.path.exists(ARTIFACTS_DIR):\n",
        "        print(f\"‚ùå Artifacts directory not found: {ARTIFACTS_DIR}\")\n",
        "        return analysis\n",
        "    \n",
        "    # Analyze each research question directory\n",
        "    for rq_dir in ['rq2', 'rq3']:\n",
        "        rq_path = os.path.join(ARTIFACTS_DIR, rq_dir)\n",
        "        if os.path.exists(rq_path):\n",
        "            size = get_directory_size(rq_path)\n",
        "            file_count = sum(len(files) for _, _, files in os.walk(rq_path))\n",
        "            \n",
        "            analysis['directories'][rq_dir] = {\n",
        "                'path': rq_path,\n",
        "                'size': size,\n",
        "                'file_count': file_count,\n",
        "                'formatted_size': format_bytes(size)\n",
        "            }\n",
        "            \n",
        "            analysis['total_size'] += size\n",
        "            analysis['file_count'] += file_count\n",
        "            \n",
        "            print(f\"üìä {rq_dir.upper()}:\")\n",
        "            print(f\"   üìÅ Path: {rq_path}\")\n",
        "            print(f\"   üìè Size: {format_bytes(size)}\")\n",
        "            print(f\"   üìÑ Files: {file_count}\")\n",
        "            \n",
        "            # List subdirectories\n",
        "            subdirs = [d for d in os.listdir(rq_path) if os.path.isdir(os.path.join(rq_path, d))]\n",
        "            if subdirs:\n",
        "                print(f\"   üìÇ Subdirectories: {', '.join(subdirs)}\")\n",
        "            print()\n",
        "    \n",
        "    print(f\"üìà TOTAL ANALYSIS:\")\n",
        "    print(f\"   üìè Total size: {format_bytes(analysis['total_size'])}\")\n",
        "    print(f\"   üìÑ Total files: {analysis['file_count']}\")\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Run analysis\n",
        "artifacts_analysis = analyze_artifacts_directory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Zip Creation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_zip_archive(source_dir: str, zip_path: str, exclude_patterns: List[str] = None) -> Dict:\n",
        "    \"\"\"Create a zip archive while preserving directory structure.\"\"\"\n",
        "    if exclude_patterns is None:\n",
        "        exclude_patterns = ['.DS_Store', '__pycache__', '.git']\n",
        "    \n",
        "    zip_info = {\n",
        "        'source_dir': source_dir,\n",
        "        'zip_path': zip_path,\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'files_added': 0,\n",
        "        'original_size': 0,\n",
        "        'compressed_size': 0,\n",
        "        'compression_ratio': 0.0,\n",
        "        'files_list': []\n",
        "    }\n",
        "    \n",
        "    print(f\"üóúÔ∏è  Creating zip archive...\")\n",
        "    print(f\"   üìÅ Source: {source_dir}\")\n",
        "    print(f\"   üì¶ Destination: {zip_path}\")\n",
        "    \n",
        "    # Get total number of files for progress bar\n",
        "    total_files = sum(len(files) for _, _, files in os.walk(source_dir))\n",
        "    \n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n",
        "        with tqdm(total=total_files, desc=\"Adding files\") as pbar:\n",
        "            for root, dirs, files in os.walk(source_dir):\n",
        "                # Remove excluded directories\n",
        "                dirs[:] = [d for d in dirs if not any(pattern in d for pattern in exclude_patterns)]\n",
        "                \n",
        "                for file in files:\n",
        "                    # Skip excluded files\n",
        "                    if any(pattern in file for pattern in exclude_patterns):\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "                    \n",
        "                    file_path = os.path.join(root, file)\n",
        "                    \n",
        "                    # Calculate relative path to preserve structure\n",
        "                    arcname = os.path.relpath(file_path, source_dir)\n",
        "                    \n",
        "                    try:\n",
        "                        # Add file to zip\n",
        "                        zipf.write(file_path, arcname)\n",
        "                        \n",
        "                        # Update statistics\n",
        "                        file_size = os.path.getsize(file_path)\n",
        "                        zip_info['files_added'] += 1\n",
        "                        zip_info['original_size'] += file_size\n",
        "                        zip_info['files_list'].append({\n",
        "                            'path': arcname,\n",
        "                            'size': file_size,\n",
        "                            'hash': calculate_file_hash(file_path)\n",
        "                        })\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è  Warning: Could not add {file_path}: {e}\")\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "    \n",
        "    # Calculate final statistics\n",
        "    if os.path.exists(zip_path):\n",
        "        zip_info['compressed_size'] = os.path.getsize(zip_path)\n",
        "        if zip_info['original_size'] > 0:\n",
        "            zip_info['compression_ratio'] = (\n",
        "                1 - zip_info['compressed_size'] / zip_info['original_size']\n",
        "            ) * 100\n",
        "    \n",
        "    print(f\"‚úÖ Zip archive created successfully!\")\n",
        "    print(f\"   üìÑ Files added: {zip_info['files_added']}\")\n",
        "    print(f\"   üìè Original size: {format_bytes(zip_info['original_size'])}\")\n",
        "    print(f\"   üóúÔ∏è  Compressed size: {format_bytes(zip_info['compressed_size'])}\")\n",
        "    print(f\"   üìä Compression ratio: {zip_info['compression_ratio']:.1f}%\")\n",
        "    \n",
        "    return zip_info\n",
        "\n",
        "print(\"üóúÔ∏è  Zip creation functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. AWS S3 Upload Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_s3_client():\n",
        "    \"\"\"Initialize and test S3 client.\"\"\"\n",
        "    try:\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "            region_name=AWS_REGION\n",
        "        )\n",
        "        \n",
        "        # Test connection by listing buckets\n",
        "        response = s3_client.list_buckets()\n",
        "        \n",
        "        # Check if our target bucket exists\n",
        "        bucket_names = [bucket['Name'] for bucket in response['Buckets']]\n",
        "        \n",
        "        if S3_BUCKET_NAME in bucket_names:\n",
        "            print(f\"‚úÖ S3 client initialized successfully\")\n",
        "            print(f\"ü™£ Target bucket '{S3_BUCKET_NAME}' found\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Bucket '{S3_BUCKET_NAME}' not found in your account\")\n",
        "            print(f\"Available buckets: {bucket_names}\")\n",
        "            \n",
        "        return s3_client\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to initialize S3 client: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_file_to_s3(s3_client, local_path: str, s3_key: str) -> Dict:\n",
        "    \"\"\"Upload a single file to S3 with progress tracking.\"\"\"\n",
        "    upload_info = {\n",
        "        'local_path': local_path,\n",
        "        's3_key': s3_key,\n",
        "        'bucket': S3_BUCKET_NAME,\n",
        "        'success': False,\n",
        "        'upload_time': None,\n",
        "        'file_size': 0,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        file_size = os.path.getsize(local_path)\n",
        "        upload_info['file_size'] = file_size\n",
        "        \n",
        "        print(f\"‚¨ÜÔ∏è  Uploading: {os.path.basename(local_path)} ({format_bytes(file_size)})\")\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Upload file\n",
        "        s3_client.upload_file(local_path, S3_BUCKET_NAME, s3_key)\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        upload_info['upload_time'] = (end_time - start_time).total_seconds()\n",
        "        upload_info['success'] = True\n",
        "        \n",
        "        print(f\"‚úÖ Upload completed in {upload_info['upload_time']:.2f} seconds\")\n",
        "        print(f\"   üîó S3 Location: s3://{S3_BUCKET_NAME}/{s3_key}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        upload_info['error'] = str(e)\n",
        "        print(f\"‚ùå Upload failed: {e}\")\n",
        "    \n",
        "    return upload_info\n",
        "\n",
        "print(\"‚òÅÔ∏è  S3 upload functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Main Execution: Create Zip Archives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create backup directory\n",
        "create_backup_directory()\n",
        "\n",
        "# Create timestamp for this backup session\n",
        "backup_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Dictionary to store all zip information\n",
        "zip_archives = {}\n",
        "\n",
        "print(f\"üöÄ Starting backup process at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üìÖ Backup session ID: {backup_timestamp}\\n\")\n",
        "\n",
        "# Create zip archives for each research question\n",
        "for rq in ['rq2', 'rq3']:\n",
        "    rq_dir = os.path.join(ARTIFACTS_DIR, rq)\n",
        "    \n",
        "    if os.path.exists(rq_dir) and os.listdir(rq_dir):  # Check if directory exists and is not empty\n",
        "        print(f\"\\nüì¶ Processing {rq.upper()}...\")\n",
        "        \n",
        "        # Create zip filename\n",
        "        zip_filename = f\"artifacts_{rq}_{backup_timestamp}.zip\"\n",
        "        zip_path = os.path.join(BACKUP_DIR, zip_filename)\n",
        "        \n",
        "        # Create zip archive\n",
        "        zip_info = create_zip_archive(rq_dir, zip_path)\n",
        "        zip_archives[rq] = zip_info\n",
        "        \n",
        "        print(f\"‚úÖ {rq.upper()} archive created: {zip_filename}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Skipping {rq.upper()}: Directory empty or doesn't exist\")\n",
        "\n",
        "print(f\"\\nüéâ Zip creation completed!\")\n",
        "print(f\"üìÅ All archives saved to: {BACKUP_DIR}\")\n",
        "\n",
        "# Display summary\n",
        "total_original_size = sum(info['original_size'] for info in zip_archives.values())\n",
        "total_compressed_size = sum(info['compressed_size'] for info in zip_archives.values())\n",
        "total_files = sum(info['files_added'] for info in zip_archives.values())\n",
        "\n",
        "print(f\"\\nüìä BACKUP SUMMARY:\")\n",
        "print(f\"   üì¶ Archives created: {len(zip_archives)}\")\n",
        "print(f\"   üìÑ Total files: {total_files}\")\n",
        "print(f\"   üìè Original size: {format_bytes(total_original_size)}\")\n",
        "print(f\"   üóúÔ∏è  Compressed size: {format_bytes(total_compressed_size)}\")\n",
        "if total_original_size > 0:\n",
        "    overall_compression = (1 - total_compressed_size / total_original_size) * 100\n",
        "    print(f\"   üìä Overall compression: {overall_compression:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Initialize S3 Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3 client\n",
        "print(\"üîê Initializing AWS S3 connection...\")\n",
        "s3_client = initialize_s3_client()\n",
        "\n",
        "if s3_client is None:\n",
        "    print(\"\\n‚ùå Cannot proceed with S3 upload. Please check your AWS credentials.\")\n",
        "    print(\"\\nüîß Troubleshooting steps:\")\n",
        "    print(\"   1. Verify AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\")\n",
        "    print(\"   2. Check if the specified region is correct\")\n",
        "    print(\"   3. Ensure your AWS account has S3 permissions\")\n",
        "    print(\"   4. Verify the bucket name exists and you have access\")\n",
        "else:\n",
        "    print(\"\\nüéØ Ready to upload to S3!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Upload to S3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if s3_client and zip_archives:\n",
        "    print(f\"‚òÅÔ∏è  Starting S3 upload process...\")\n",
        "    print(f\"ü™£ Target bucket: {S3_BUCKET_NAME}\")\n",
        "    print(f\"üìÇ S3 prefix: {S3_KEY_PREFIX}\\n\")\n",
        "    \n",
        "    upload_results = []\n",
        "    \n",
        "    for rq, zip_info in zip_archives.items():\n",
        "        print(f\"\\n‚¨ÜÔ∏è  Uploading {rq.upper()} archive...\")\n",
        "        \n",
        "        # Create S3 key maintaining directory structure\n",
        "        zip_filename = os.path.basename(zip_info['zip_path'])\n",
        "        s3_key = f\"{S3_KEY_PREFIX}/artifacts/{rq}/{zip_filename}\"\n",
        "        \n",
        "        # Upload file\n",
        "        upload_result = upload_file_to_s3(s3_client, zip_info['zip_path'], s3_key)\n",
        "        upload_results.append(upload_result)\n",
        "    \n",
        "    # Create and save upload manifest\n",
        "    manifest = {\n",
        "        'upload_session': {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'project': 'HistoPathology Research',\n",
        "            's3_bucket': S3_BUCKET_NAME,\n",
        "            's3_prefix': S3_KEY_PREFIX\n",
        "        },\n",
        "        'summary': {\n",
        "            'total_files': len(upload_results),\n",
        "            'successful_uploads': sum(1 for r in upload_results if r['success']),\n",
        "            'failed_uploads': sum(1 for r in upload_results if not r['success']),\n",
        "            'total_size': sum(r['file_size'] for r in upload_results),\n",
        "            'total_upload_time': sum(r.get('upload_time', 0) for r in upload_results)\n",
        "        },\n",
        "        'uploads': upload_results\n",
        "    }\n",
        "    \n",
        "    manifest_path = os.path.join(BACKUP_DIR, f\"upload_manifest_{backup_timestamp}.json\")\n",
        "    \n",
        "    with open(manifest_path, 'w') as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    \n",
        "    # Upload manifest to S3\n",
        "    manifest_s3_key = f\"{S3_KEY_PREFIX}/manifests/upload_manifest_{backup_timestamp}.json\"\n",
        "    manifest_upload = upload_file_to_s3(s3_client, manifest_path, manifest_s3_key)\n",
        "    \n",
        "    print(f\"\\nüéâ S3 Upload Process Completed!\")\n",
        "    print(f\"\\nüìä UPLOAD SUMMARY:\")\n",
        "    print(f\"   ‚úÖ Successful uploads: {manifest['summary']['successful_uploads']}\")\n",
        "    print(f\"   ‚ùå Failed uploads: {manifest['summary']['failed_uploads']}\")\n",
        "    print(f\"   üìè Total uploaded: {format_bytes(manifest['summary']['total_size'])}\")\n",
        "    print(f\"   ‚è±Ô∏è  Total time: {manifest['summary']['total_upload_time']:.2f} seconds\")\n",
        "    print(f\"   üìÑ Manifest saved: {manifest_path}\")\n",
        "    \n",
        "    # Display S3 locations\n",
        "    print(f\"\\nüîó S3 LOCATIONS:\")\n",
        "    for result in upload_results:\n",
        "        if result['success']:\n",
        "            print(f\"   üì¶ s3://{result['bucket']}/{result['s3_key']}\")\n",
        "    \n",
        "    if manifest_upload['success']:\n",
        "        print(f\"   üìÑ s3://{S3_BUCKET_NAME}/{manifest_s3_key}\")\n",
        "    \n",
        "else:\n",
        "    if not s3_client:\n",
        "        print(\"‚ùå S3 client not initialized. Cannot upload.\")\n",
        "    if not zip_archives:\n",
        "        print(\"‚ùå No zip archives created. Nothing to upload.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ MODEL BACKUP AND S3 UPLOAD COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìÖ Session: {backup_timestamp}\")\n",
        "print(f\"üìÅ Local backups: {BACKUP_DIR}\")\n",
        "print(f\"‚òÅÔ∏è  S3 bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"üìÇ S3 prefix: {S3_KEY_PREFIX}\")\n",
        "\n",
        "if 'zip_archives' in locals() and zip_archives:\n",
        "    print(f\"\\nüì¶ ARCHIVES CREATED:\")\n",
        "    for rq, info in zip_archives.items():\n",
        "        print(f\"   {rq.upper()}: {os.path.basename(info['zip_path'])} ({format_bytes(info['compressed_size'])})\")\n",
        "\n",
        "if 'upload_results' in locals():\n",
        "    successful_uploads = sum(1 for r in upload_results if r['success'])\n",
        "    print(f\"\\n‚òÅÔ∏è  UPLOADS: {successful_uploads}/{len(upload_results)} successful\")\n",
        "\n",
        "print(f\"\\nüí° TIP: Keep the manifest file for future reference!\")\n",
        "print(f\"üìÑ Manifest location: {BACKUP_DIR}/upload_manifest_{backup_timestamp}.json\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
