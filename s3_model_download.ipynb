{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Download and Restore from S3 Notebook\n",
        "\n",
        "This notebook provides functionality to:\n",
        "1. List and discover model archives in S3 bucket\n",
        "2. Download zip files from AWS S3 bucket\n",
        "3. Restore the original directory structure\n",
        "4. Verify file integrity and completeness\n",
        "5. Provide progress tracking and error handling\n",
        "\n",
        "## Prerequisites\n",
        "- AWS credentials configured (access key, secret key)\n",
        "- boto3 installed (`pip install boto3`)\n",
        "- Sufficient disk space for downloading and extracting files\n",
        "- Original upload manifest files (recommended for verification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration Section\n",
        "\n",
        "**‚ö†Ô∏è Security Note**: Never commit AWS credentials to version control. Use environment variables or AWS credentials file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY_ID = \"your-access-key-here\"  # Replace with your access key\n",
        "AWS_SECRET_ACCESS_KEY = \"your-secret-key-here\"  # Replace with your secret key\n",
        "AWS_REGION = \"us-east-1\"  # Replace with your preferred region\n",
        "S3_BUCKET_NAME = \"your-bucket-name\"  # Replace with your S3 bucket name\n",
        "\n",
        "# Alternatively, use environment variables (recommended for security)\n",
        "# AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "# AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "# AWS_REGION = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
        "# S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\n",
        "\n",
        "# Project Configuration\n",
        "PROJECT_ROOT = \"/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch\"\n",
        "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, \"artifacts\")\n",
        "DOWNLOAD_DIR = os.path.join(PROJECT_ROOT, \"downloads\")\n",
        "RESTORE_DIR = os.path.join(PROJECT_ROOT, \"restored_artifacts\")\n",
        "\n",
        "# S3 Configuration - you can specify a specific date or leave empty to list all\n",
        "TARGET_DATE = \"\"  # Format: \"2024-01-15\" or leave empty to show all dates\n",
        "S3_KEY_PREFIX = f\"histopathology-research/{TARGET_DATE}\" if TARGET_DATE else \"histopathology-research\"\n",
        "\n",
        "# Download Options\n",
        "OVERWRITE_EXISTING = False  # Set to True to overwrite existing files\n",
        "VERIFY_DOWNLOADS = True     # Set to True to verify file integrity after download\n",
        "\n",
        "print(f\"‚úÖ Configuration loaded\")\n",
        "print(f\"üìÅ Project root: {PROJECT_ROOT}\")\n",
        "print(f\"üíæ Download directory: {DOWNLOAD_DIR}\")\n",
        "print(f\"üìÇ Restore directory: {RESTORE_DIR}\")\n",
        "print(f\"ü™£ S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"üîç S3 Search prefix: {S3_KEY_PREFIX}\")\n",
        "print(f\"üîÑ Overwrite existing: {OVERWRITE_EXISTING}\")\n",
        "print(f\"‚úÖ Verify downloads: {VERIFY_DOWNLOADS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import boto3\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import hashlib\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "import shutil\n",
        "from botocore.exceptions import NoCredentialsError, ClientError\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üìö All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_bytes(bytes_size: int) -> str:\n",
        "    \"\"\"Convert bytes to human readable format.\"\"\"\n",
        "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if bytes_size < 1024.0:\n",
        "            return f\"{bytes_size:.2f} {unit}\"\n",
        "        bytes_size /= 1024.0\n",
        "    return f\"{bytes_size:.2f} PB\"\n",
        "\n",
        "def calculate_file_hash(filepath: str) -> str:\n",
        "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"Create necessary directories if they don't exist.\"\"\"\n",
        "    for directory in [DOWNLOAD_DIR, RESTORE_DIR]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"üìÅ Download directory ready: {DOWNLOAD_DIR}\")\n",
        "    print(f\"üìÅ Restore directory ready: {RESTORE_DIR}\")\n",
        "\n",
        "def parse_s3_key(s3_key: str) -> Dict:\n",
        "    \"\"\"Parse S3 key to extract useful information.\"\"\"\n",
        "    parts = s3_key.split('/')\n",
        "    info = {\n",
        "        'full_key': s3_key,\n",
        "        'filename': parts[-1],\n",
        "        'project': parts[0] if len(parts) > 0 else '',\n",
        "        'date': parts[1] if len(parts) > 1 else '',\n",
        "        'category': parts[2] if len(parts) > 2 else '',\n",
        "        'research_question': parts[3] if len(parts) > 3 else ''\n",
        "    }\n",
        "    return info\n",
        "\n",
        "def format_timestamp(timestamp_str: str) -> str:\n",
        "    \"\"\"Format timestamp string to human readable format.\"\"\"\n",
        "    try:\n",
        "        # Parse timestamp from filename (format: YYYYMMDD_HHMMSS)\n",
        "        if '_' in timestamp_str:\n",
        "            date_part, time_part = timestamp_str.split('_')\n",
        "            formatted_date = f\"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]}\"\n",
        "            formatted_time = f\"{time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}\"\n",
        "            return f\"{formatted_date} {formatted_time}\"\n",
        "        return timestamp_str\n",
        "    except:\n",
        "        return timestamp_str\n",
        "\n",
        "print(\"üîß Utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. AWS S3 Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_s3_client():\n",
        "    \"\"\"Initialize and test S3 client.\"\"\"\n",
        "    try:\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "            region_name=AWS_REGION\n",
        "        )\n",
        "        \n",
        "        # Test connection by listing buckets\n",
        "        response = s3_client.list_buckets()\n",
        "        \n",
        "        # Check if our target bucket exists\n",
        "        bucket_names = [bucket['Name'] for bucket in response['Buckets']]\n",
        "        \n",
        "        if S3_BUCKET_NAME in bucket_names:\n",
        "            print(f\"‚úÖ S3 client initialized successfully\")\n",
        "            print(f\"ü™£ Target bucket '{S3_BUCKET_NAME}' found\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Bucket '{S3_BUCKET_NAME}' not found in your account\")\n",
        "            print(f\"Available buckets: {bucket_names}\")\n",
        "            \n",
        "        return s3_client\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to initialize S3 client: {e}\")\n",
        "        return None\n",
        "\n",
        "def list_s3_objects(s3_client, prefix: str = \"\") -> List[Dict]:\n",
        "    \"\"\"List all objects in S3 bucket with given prefix.\"\"\"\n",
        "    objects = []\n",
        "    \n",
        "    try:\n",
        "        paginator = s3_client.get_paginator('list_objects_v2')\n",
        "        page_iterator = paginator.paginate(\n",
        "            Bucket=S3_BUCKET_NAME,\n",
        "            Prefix=prefix\n",
        "        )\n",
        "        \n",
        "        for page in page_iterator:\n",
        "            if 'Contents' in page:\n",
        "                for obj in page['Contents']:\n",
        "                    obj_info = {\n",
        "                        'key': obj['Key'],\n",
        "                        'size': obj['Size'],\n",
        "                        'last_modified': obj['LastModified'],\n",
        "                        'formatted_size': format_bytes(obj['Size']),\n",
        "                        'parsed': parse_s3_key(obj['Key'])\n",
        "                    }\n",
        "                    objects.append(obj_info)\n",
        "        \n",
        "        return objects\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to list S3 objects: {e}\")\n",
        "        return []\n",
        "\n",
        "def download_file_from_s3(s3_client, s3_key: str, local_path: str) -> Dict:\n",
        "    \"\"\"Download a single file from S3 with progress tracking.\"\"\"\n",
        "    download_info = {\n",
        "        's3_key': s3_key,\n",
        "        'local_path': local_path,\n",
        "        'bucket': S3_BUCKET_NAME,\n",
        "        'success': False,\n",
        "        'download_time': None,\n",
        "        'file_size': 0,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Get object info first\n",
        "        response = s3_client.head_object(Bucket=S3_BUCKET_NAME, Key=s3_key)\n",
        "        file_size = response['ContentLength']\n",
        "        download_info['file_size'] = file_size\n",
        "        \n",
        "        print(f\"‚¨áÔ∏è  Downloading: {os.path.basename(s3_key)} ({format_bytes(file_size)})\")\n",
        "        \n",
        "        # Check if file already exists\n",
        "        if os.path.exists(local_path) and not OVERWRITE_EXISTING:\n",
        "            print(f\"‚ö†Ô∏è  File already exists, skipping: {local_path}\")\n",
        "            download_info['success'] = True\n",
        "            download_info['download_time'] = 0\n",
        "            return download_info\n",
        "        \n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Download with progress tracking\n",
        "        def progress_callback(bytes_transferred):\n",
        "            percentage = (bytes_transferred / file_size) * 100 if file_size > 0 else 0\n",
        "            print(f\"\\r   Progress: {percentage:.1f}% ({format_bytes(bytes_transferred)}/{format_bytes(file_size)})\", end=\"\")\n",
        "        \n",
        "        s3_client.download_file(\n",
        "            S3_BUCKET_NAME, \n",
        "            s3_key, \n",
        "            local_path,\n",
        "            Callback=progress_callback\n",
        "        )\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        download_info['download_time'] = (end_time - start_time).total_seconds()\n",
        "        download_info['success'] = True\n",
        "        \n",
        "        print(f\"\\n‚úÖ Download completed in {download_info['download_time']:.2f} seconds\")\n",
        "        print(f\"   üíæ Local path: {local_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        download_info['error'] = str(e)\n",
        "        print(f\"\\n‚ùå Download failed: {e}\")\n",
        "    \n",
        "    return download_info\n",
        "\n",
        "print(\"‚òÅÔ∏è  S3 functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Archive Extraction Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_zip_archive(zip_path: str, extract_to: str, preserve_structure: bool = True) -> Dict:\n",
        "    \"\"\"Extract zip archive while preserving or restoring directory structure.\"\"\"\n",
        "    extract_info = {\n",
        "        'zip_path': zip_path,\n",
        "        'extract_to': extract_to,\n",
        "        'extracted_at': datetime.now().isoformat(),\n",
        "        'files_extracted': 0,\n",
        "        'total_size': 0,\n",
        "        'success': False,\n",
        "        'files_list': [],\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print(f\"üì¶ Extracting archive: {os.path.basename(zip_path)}\")\n",
        "        print(f\"   üìÅ Destination: {extract_to}\")\n",
        "        \n",
        "        with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
        "            # Get list of files in archive\n",
        "            file_list = zipf.namelist()\n",
        "            total_files = len(file_list)\n",
        "            \n",
        "            print(f\"   üìÑ Files in archive: {total_files}\")\n",
        "            \n",
        "            # Create destination directory\n",
        "            os.makedirs(extract_to, exist_ok=True)\n",
        "            \n",
        "            # Extract with progress bar\n",
        "            with tqdm(total=total_files, desc=\"Extracting files\") as pbar:\n",
        "                for file_info in zipf.filelist:\n",
        "                    try:\n",
        "                        # Extract file\n",
        "                        zipf.extract(file_info, extract_to)\n",
        "                        \n",
        "                        # Update statistics\n",
        "                        extract_info['files_extracted'] += 1\n",
        "                        extract_info['total_size'] += file_info.file_size\n",
        "                        extract_info['files_list'].append({\n",
        "                            'filename': file_info.filename,\n",
        "                            'size': file_info.file_size,\n",
        "                            'compressed_size': file_info.compress_size\n",
        "                        })\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è  Warning: Could not extract {file_info.filename}: {e}\")\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "        \n",
        "        extract_info['success'] = True\n",
        "        print(f\"‚úÖ Extraction completed successfully!\")\n",
        "        print(f\"   üìÑ Files extracted: {extract_info['files_extracted']}\")\n",
        "        print(f\"   üìè Total size: {format_bytes(extract_info['total_size'])}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        extract_info['error'] = str(e)\n",
        "        print(f\"‚ùå Extraction failed: {e}\")\n",
        "    \n",
        "    return extract_info\n",
        "\n",
        "def restore_to_original_structure(extracted_path: str, target_artifacts_dir: str, research_question: str) -> Dict:\n",
        "    \"\"\"Restore extracted files to original artifacts directory structure.\"\"\"\n",
        "    restore_info = {\n",
        "        'source_path': extracted_path,\n",
        "        'target_path': target_artifacts_dir,\n",
        "        'research_question': research_question,\n",
        "        'files_moved': 0,\n",
        "        'success': False,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print(f\"üîÑ Restoring {research_question.upper()} to original structure...\")\n",
        "        \n",
        "        # Create target directory structure\n",
        "        target_rq_dir = os.path.join(target_artifacts_dir, research_question)\n",
        "        os.makedirs(target_rq_dir, exist_ok=True)\n",
        "        \n",
        "        # Move files while preserving structure\n",
        "        if os.path.exists(extracted_path):\n",
        "            for root, dirs, files in os.walk(extracted_path):\n",
        "                for file in files:\n",
        "                    source_file = os.path.join(root, file)\n",
        "                    \n",
        "                    # Calculate relative path\n",
        "                    rel_path = os.path.relpath(source_file, extracted_path)\n",
        "                    target_file = os.path.join(target_rq_dir, rel_path)\n",
        "                    \n",
        "                    # Create target directory if needed\n",
        "                    os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
        "                    \n",
        "                    # Copy or move file\n",
        "                    if OVERWRITE_EXISTING or not os.path.exists(target_file):\n",
        "                        shutil.copy2(source_file, target_file)\n",
        "                        restore_info['files_moved'] += 1\n",
        "                    else:\n",
        "                        print(f\"‚ö†Ô∏è  Skipping existing file: {target_file}\")\n",
        "        \n",
        "        restore_info['success'] = True\n",
        "        print(f\"‚úÖ Restoration completed!\")\n",
        "        print(f\"   üìÑ Files restored: {restore_info['files_moved']}\")\n",
        "        print(f\"   üìÅ Target directory: {target_rq_dir}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        restore_info['error'] = str(e)\n",
        "        print(f\"‚ùå Restoration failed: {e}\")\n",
        "    \n",
        "    return restore_info\n",
        "\n",
        "print(\"üì¶ Archive extraction functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize and Discover S3 Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "create_directories()\n",
        "\n",
        "# Initialize S3 client\n",
        "print(\"\\nüîê Initializing AWS S3 connection...\")\n",
        "s3_client = initialize_s3_client()\n",
        "\n",
        "if s3_client is None:\n",
        "    print(\"\\n‚ùå Cannot proceed with S3 operations. Please check your AWS credentials.\")\n",
        "    print(\"\\nüîß Troubleshooting steps:\")\n",
        "    print(\"   1. Verify AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\")\n",
        "    print(\"   2. Check if the specified region is correct\")\n",
        "    print(\"   3. Ensure your AWS account has S3 permissions\")\n",
        "    print(\"   4. Verify the bucket name exists and you have access\")\n",
        "else:\n",
        "    print(\"\\nüîç Discovering available files in S3...\")\n",
        "    \n",
        "    # List all objects with the prefix\n",
        "    s3_objects = list_s3_objects(s3_client, S3_KEY_PREFIX)\n",
        "    \n",
        "    if not s3_objects:\n",
        "        print(f\"‚ùå No objects found with prefix: {S3_KEY_PREFIX}\")\n",
        "        print(\"üí° Try adjusting the TARGET_DATE in configuration or check if files were uploaded\")\n",
        "    else:\n",
        "        print(f\"\\nüìä DISCOVERY RESULTS:\")\n",
        "        print(f\"   üìÑ Total objects found: {len(s3_objects)}\")\n",
        "        \n",
        "        # Categorize objects\n",
        "        archives = [obj for obj in s3_objects if obj['key'].endswith('.zip') and 'artifacts_' in obj['key']]\n",
        "        manifests = [obj for obj in s3_objects if obj['key'].endswith('.json') and 'manifest' in obj['key']]\n",
        "        \n",
        "        print(f\"   üì¶ Archive files: {len(archives)}\")\n",
        "        print(f\"   üìÑ Manifest files: {len(manifests)}\")\n",
        "        \n",
        "        # Display available archives by date and research question\n",
        "        if archives:\n",
        "            print(f\"\\nüì¶ AVAILABLE ARCHIVES:\")\n",
        "            for archive in archives:\n",
        "                parsed = archive['parsed']\n",
        "                timestamp_part = archive['parsed']['filename'].split('_')[-1].replace('.zip', '')\n",
        "                formatted_time = format_timestamp(timestamp_part)\n",
        "                \n",
        "                print(f\"   üóÇÔ∏è  {parsed['research_question'].upper()}: {archive['formatted_size']} - {formatted_time}\")\n",
        "                print(f\"      üìç S3 Key: {archive['key']}\")\n",
        "                print(f\"      üìÖ Last Modified: {archive['last_modified'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                print()\n",
        "        \n",
        "        # Display available manifests\n",
        "        if manifests:\n",
        "            print(f\"üìÑ AVAILABLE MANIFESTS:\")\n",
        "            for manifest in manifests:\n",
        "                parsed = manifest['parsed']\n",
        "                timestamp_part = manifest['parsed']['filename'].split('_')[-1].replace('.json', '')\n",
        "                formatted_time = format_timestamp(timestamp_part)\n",
        "                \n",
        "                print(f\"   üìã {manifest['formatted_size']} - {formatted_time}\")\n",
        "                print(f\"      üìç S3 Key: {manifest['key']}\")\n",
        "                print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Download Archives from S3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if s3_client and 'archives' in locals() and archives:\n",
        "    print(\"‚¨áÔ∏è  Starting download process...\")\n",
        "    \n",
        "    download_session_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    download_results = []\n",
        "    \n",
        "    print(f\"üìÖ Download session ID: {download_session_id}\\n\")\n",
        "    \n",
        "    for archive in archives:\n",
        "        print(f\"\\nüì¶ Processing {archive['parsed']['research_question'].upper()} archive...\")\n",
        "        \n",
        "        # Create local path maintaining structure\n",
        "        local_filename = os.path.basename(archive['key'])\n",
        "        local_path = os.path.join(DOWNLOAD_DIR, local_filename)\n",
        "        \n",
        "        # Download archive\n",
        "        download_result = download_file_from_s3(s3_client, archive['key'], local_path)\n",
        "        download_results.append(download_result)\n",
        "        \n",
        "        # Verify download if enabled\n",
        "        if VERIFY_DOWNLOADS and download_result['success']:\n",
        "            print(f\"üîç Verifying download...\")\n",
        "            if os.path.exists(local_path):\n",
        "                local_size = os.path.getsize(local_path)\n",
        "                if local_size == download_result['file_size']:\n",
        "                    print(f\"‚úÖ Verification passed: File size matches ({format_bytes(local_size)})\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Verification failed: Size mismatch (Expected: {format_bytes(download_result['file_size'])}, Got: {format_bytes(local_size)})\")\n",
        "            else:\n",
        "                print(f\"‚ùå Verification failed: Downloaded file not found\")\n",
        "    \n",
        "    # Download manifests if available\n",
        "    if 'manifests' in locals() and manifests:\n",
        "        print(f\"\\nüìÑ Downloading manifest files...\")\n",
        "        for manifest in manifests:\n",
        "            local_filename = os.path.basename(manifest['key'])\n",
        "            local_path = os.path.join(DOWNLOAD_DIR, local_filename)\n",
        "            \n",
        "            manifest_result = download_file_from_s3(s3_client, manifest['key'], local_path)\n",
        "            download_results.append(manifest_result)\n",
        "    \n",
        "    # Create download summary\n",
        "    successful_downloads = [r for r in download_results if r['success']]\n",
        "    failed_downloads = [r for r in download_results if not r['success']]\n",
        "    \n",
        "    print(f\"\\nüéâ DOWNLOAD SUMMARY:\")\n",
        "    print(f\"   ‚úÖ Successful downloads: {len(successful_downloads)}\")\n",
        "    print(f\"   ‚ùå Failed downloads: {len(failed_downloads)}\")\n",
        "    print(f\"   üìè Total downloaded: {format_bytes(sum(r['file_size'] for r in successful_downloads))}\")\n",
        "    print(f\"   ‚è±Ô∏è  Total time: {sum(r.get('download_time', 0) for r in successful_downloads):.2f} seconds\")\n",
        "    \n",
        "    if failed_downloads:\n",
        "        print(f\"\\n‚ùå FAILED DOWNLOADS:\")\n",
        "        for failed in failed_downloads:\n",
        "            print(f\"   üìÑ {os.path.basename(failed['s3_key'])}: {failed.get('error', 'Unknown error')}\")\n",
        "\n",
        "else:\n",
        "    if not s3_client:\n",
        "        print(\"‚ùå S3 client not initialized. Cannot download.\")\n",
        "    elif 'archives' not in locals() or not archives:\n",
        "        print(\"‚ùå No archives found to download.\")\n",
        "    else:\n",
        "        print(\"‚ùå Unknown error occurred.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Extract and Restore Archives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'successful_downloads' in locals() and successful_downloads:\n",
        "    print(\"üì¶ Starting extraction and restoration process...\")\n",
        "    \n",
        "    extraction_results = []\n",
        "    restoration_results = []\n",
        "    \n",
        "    # Filter for archive files only (not manifests)\n",
        "    archive_downloads = [d for d in successful_downloads if d['s3_key'].endswith('.zip') and 'artifacts_' in d['s3_key']]\n",
        "    \n",
        "    for download in archive_downloads:\n",
        "        zip_path = download['local_path']\n",
        "        \n",
        "        if not os.path.exists(zip_path):\n",
        "            print(f\"‚ö†Ô∏è  Skipping missing file: {zip_path}\")\n",
        "            continue\n",
        "        \n",
        "        # Determine research question from filename\n",
        "        filename = os.path.basename(zip_path)\n",
        "        if 'artifacts_rq2_' in filename:\n",
        "            research_question = 'rq2'\n",
        "        elif 'artifacts_rq3_' in filename:\n",
        "            research_question = 'rq3'\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Cannot determine research question from filename: {filename}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nüóÇÔ∏è  Processing {research_question.upper()} archive: {filename}\")\n",
        "        \n",
        "        # Create temporary extraction directory\n",
        "        temp_extract_dir = os.path.join(DOWNLOAD_DIR, f\"temp_extract_{research_question}\")\n",
        "        \n",
        "        # Extract archive\n",
        "        extract_info = extract_zip_archive(zip_path, temp_extract_dir)\n",
        "        extraction_results.append(extract_info)\n",
        "        \n",
        "        if extract_info['success']:\n",
        "            # Restore to original structure\n",
        "            restore_info = restore_to_original_structure(\n",
        "                temp_extract_dir, \n",
        "                RESTORE_DIR, \n",
        "                research_question\n",
        "            )\n",
        "            restoration_results.append(restore_info)\n",
        "            \n",
        "            # Clean up temporary extraction directory\n",
        "            try:\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "                print(f\"üßπ Cleaned up temporary directory: {temp_extract_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Could not clean up temporary directory: {e}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Skipping restoration due to extraction failure\")\n",
        "    \n",
        "    # Summary of extraction and restoration\n",
        "    successful_extractions = [r for r in extraction_results if r['success']]\n",
        "    successful_restorations = [r for r in restoration_results if r['success']]\n",
        "    \n",
        "    print(f\"\\nüéâ EXTRACTION & RESTORATION SUMMARY:\")\n",
        "    print(f\"   üì¶ Archives processed: {len(archive_downloads)}\")\n",
        "    print(f\"   ‚úÖ Successful extractions: {len(successful_extractions)}\")\n",
        "    print(f\"   ‚úÖ Successful restorations: {len(successful_restorations)}\")\n",
        "    \n",
        "    if successful_restorations:\n",
        "        total_files_restored = sum(r['files_moved'] for r in successful_restorations)\n",
        "        print(f\"   üìÑ Total files restored: {total_files_restored}\")\n",
        "        print(f\"   üìÅ Restoration directory: {RESTORE_DIR}\")\n",
        "        \n",
        "        print(f\"\\nüìÇ RESTORED STRUCTURE:\")\n",
        "        for restore in successful_restorations:\n",
        "            rq_dir = os.path.join(RESTORE_DIR, restore['research_question'])\n",
        "            if os.path.exists(rq_dir):\n",
        "                subdirs = [d for d in os.listdir(rq_dir) if os.path.isdir(os.path.join(rq_dir, d))]\n",
        "                print(f\"   {restore['research_question'].upper()}: {restore['files_moved']} files\")\n",
        "                if subdirs:\n",
        "                    print(f\"      üìÇ Subdirectories: {', '.join(subdirs)}\")\n",
        "    \n",
        "    # Check if we should copy to original artifacts directory\n",
        "    copy_to_original = input(f\"\\n‚ùì Copy restored files to original artifacts directory ({ARTIFACTS_DIR})? [y/N]: \").lower().strip()\n",
        "    \n",
        "    if copy_to_original == 'y':\n",
        "        print(f\"\\nüîÑ Copying to original artifacts directory...\")\n",
        "        \n",
        "        for restore in successful_restorations:\n",
        "            source_dir = os.path.join(RESTORE_DIR, restore['research_question'])\n",
        "            target_dir = os.path.join(ARTIFACTS_DIR, restore['research_question'])\n",
        "            \n",
        "            if os.path.exists(source_dir):\n",
        "                try:\n",
        "                    # Create target directory\n",
        "                    os.makedirs(target_dir, exist_ok=True)\n",
        "                    \n",
        "                    # Copy files\n",
        "                    for root, dirs, files in os.walk(source_dir):\n",
        "                        for file in files:\n",
        "                            source_file = os.path.join(root, file)\n",
        "                            rel_path = os.path.relpath(source_file, source_dir)\n",
        "                            target_file = os.path.join(target_dir, rel_path)\n",
        "                            \n",
        "                            # Create target directory if needed\n",
        "                            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
        "                            \n",
        "                            if OVERWRITE_EXISTING or not os.path.exists(target_file):\n",
        "                                shutil.copy2(source_file, target_file)\n",
        "                                print(f\"üìÑ Copied: {rel_path}\")\n",
        "                            else:\n",
        "                                print(f\"‚ö†Ô∏è  Skipped existing: {rel_path}\")\n",
        "                    \n",
        "                    print(f\"‚úÖ {restore['research_question'].upper()} copied to {target_dir}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Failed to copy {restore['research_question'].upper()}: {e}\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Copy operation completed!\")\n",
        "    else:\n",
        "        print(f\"\\nüí° Files remain in restoration directory: {RESTORE_DIR}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No successful downloads found to extract and restore.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Verification and Integrity Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_restored_structure(restore_dir: str) -> Dict:\n",
        "    \"\"\"Verify the restored directory structure and files.\"\"\"\n",
        "    verification_results = {\n",
        "        'total_files': 0,\n",
        "        'total_size': 0,\n",
        "        'research_questions': {},\n",
        "        'verification_time': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    print(\"üîç Verifying restored structure...\")\n",
        "    \n",
        "    if not os.path.exists(restore_dir):\n",
        "        print(f\"‚ùå Restore directory not found: {restore_dir}\")\n",
        "        return verification_results\n",
        "    \n",
        "    # Check each research question directory\n",
        "    for rq in ['rq2', 'rq3']:\n",
        "        rq_path = os.path.join(restore_dir, rq)\n",
        "        \n",
        "        if os.path.exists(rq_path):\n",
        "            rq_info = {\n",
        "                'exists': True,\n",
        "                'file_count': 0,\n",
        "                'total_size': 0,\n",
        "                'subdirectories': [],\n",
        "                'sample_files': []\n",
        "            }\n",
        "            \n",
        "            # Walk through directory\n",
        "            for root, dirs, files in os.walk(rq_path):\n",
        "                rq_info['file_count'] += len(files)\n",
        "                \n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_size = os.path.getsize(file_path)\n",
        "                    rq_info['total_size'] += file_size\n",
        "                    verification_results['total_size'] += file_size\n",
        "                    \n",
        "                    # Keep sample of files for verification\n",
        "                    if len(rq_info['sample_files']) < 5:\n",
        "                        rel_path = os.path.relpath(file_path, rq_path)\n",
        "                        rq_info['sample_files'].append({\n",
        "                            'path': rel_path,\n",
        "                            'size': file_size,\n",
        "                            'hash': calculate_file_hash(file_path) if file_size < 100*1024*1024 else 'skipped_large_file'  # Skip hash for files > 100MB\n",
        "                        })\n",
        "            \n",
        "            # Get subdirectories\n",
        "            if os.path.exists(rq_path):\n",
        "                rq_info['subdirectories'] = [d for d in os.listdir(rq_path) if os.path.isdir(os.path.join(rq_path, d))]\n",
        "            \n",
        "            verification_results['research_questions'][rq] = rq_info\n",
        "            verification_results['total_files'] += rq_info['file_count']\n",
        "            \n",
        "            print(f\"‚úÖ {rq.upper()}: {rq_info['file_count']} files, {format_bytes(rq_info['total_size'])}\")\n",
        "            if rq_info['subdirectories']:\n",
        "                print(f\"   üìÇ Subdirectories: {', '.join(rq_info['subdirectories'])}\")\n",
        "        else:\n",
        "            verification_results['research_questions'][rq] = {'exists': False}\n",
        "            print(f\"‚ùå {rq.upper()}: Directory not found\")\n",
        "    \n",
        "    print(f\"\\nüìä VERIFICATION SUMMARY:\")\n",
        "    print(f\"   üìÑ Total files: {verification_results['total_files']}\")\n",
        "    print(f\"   üìè Total size: {format_bytes(verification_results['total_size'])}\")\n",
        "    \n",
        "    return verification_results\n",
        "\n",
        "# Run verification if we have restored files\n",
        "if 'successful_restorations' in locals() and successful_restorations:\n",
        "    verification = verify_restored_structure(RESTORE_DIR)\n",
        "    \n",
        "    # Save verification report\n",
        "    verification_report_path = os.path.join(DOWNLOAD_DIR, f\"verification_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "    \n",
        "    with open(verification_report_path, 'w') as f:\n",
        "        json.dump(verification, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nüìÑ Verification report saved: {verification_report_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No restored files to verify.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup Options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional cleanup of downloaded zip files\n",
        "CLEANUP_DOWNLOADS = False  # Set to True to delete downloaded zip files after successful extraction\n",
        "\n",
        "if CLEANUP_DOWNLOADS and 'successful_downloads' in locals():\n",
        "    print(\"üßπ Cleaning up downloaded files...\")\n",
        "    \n",
        "    # Only clean up zip files that were successfully extracted\n",
        "    if 'successful_extractions' in locals():\n",
        "        extracted_files = [r['zip_path'] for r in successful_extractions if r['success']]\n",
        "        \n",
        "        cleaned_count = 0\n",
        "        for zip_path in extracted_files:\n",
        "            if os.path.exists(zip_path):\n",
        "                try:\n",
        "                    os.remove(zip_path)\n",
        "                    print(f\"üóëÔ∏è  Deleted: {os.path.basename(zip_path)}\")\n",
        "                    cleaned_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Could not delete {zip_path}: {e}\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Cleanup completed. Removed {cleaned_count} zip files.\")\n",
        "        print(f\"üíæ Verification reports and manifests retained.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No successful extractions found. Keeping all downloaded files.\")\n",
        "else:\n",
        "    if 'successful_downloads' in locals():\n",
        "        print(\"üìÅ Downloaded files retained in download directory.\")\n",
        "        print(f\"   üìÇ Location: {DOWNLOAD_DIR}\")\n",
        "        \n",
        "        # List what we're keeping\n",
        "        zip_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.zip')]\n",
        "        json_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.json')]\n",
        "        \n",
        "        if zip_files:\n",
        "            print(f\"   üì¶ Zip files: {len(zip_files)}\")\n",
        "        if json_files:\n",
        "            print(f\"   üìÑ JSON files: {len(json_files)}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No downloads to clean up.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ MODEL DOWNLOAD AND RESTORATION COMPLETED\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "session_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "print(f\"\\nüìÖ Session: {session_id}\")\n",
        "print(f\"ü™£ S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"üîç Search Prefix: {S3_KEY_PREFIX}\")\n",
        "\n",
        "# Summary of operations\n",
        "if 's3_objects' in locals():\n",
        "    print(f\"\\nüîç DISCOVERY:\")\n",
        "    print(f\"   üìÑ Objects found: {len(s3_objects)}\")\n",
        "    if 'archives' in locals():\n",
        "        print(f\"   üì¶ Archives: {len(archives)}\")\n",
        "    if 'manifests' in locals():\n",
        "        print(f\"   üìã Manifests: {len(manifests)}\")\n",
        "\n",
        "if 'download_results' in locals():\n",
        "    successful_downloads = [r for r in download_results if r['success']]\n",
        "    print(f\"\\n‚¨áÔ∏è  DOWNLOADS:\")\n",
        "    print(f\"   ‚úÖ Successful: {len(successful_downloads)}\")\n",
        "    print(f\"   üìè Total size: {format_bytes(sum(r['file_size'] for r in successful_downloads))}\")\n",
        "\n",
        "if 'extraction_results' in locals():\n",
        "    successful_extractions = [r for r in extraction_results if r['success']]\n",
        "    print(f\"\\nüì¶ EXTRACTIONS:\")\n",
        "    print(f\"   ‚úÖ Successful: {len(successful_extractions)}\")\n",
        "    if successful_extractions:\n",
        "        total_extracted_files = sum(r['files_extracted'] for r in successful_extractions)\n",
        "        print(f\"   üìÑ Files extracted: {total_extracted_files}\")\n",
        "\n",
        "if 'restoration_results' in locals():\n",
        "    successful_restorations = [r for r in restoration_results if r['success']]\n",
        "    print(f\"\\nüîÑ RESTORATIONS:\")\n",
        "    print(f\"   ‚úÖ Successful: {len(successful_restorations)}\")\n",
        "    if successful_restorations:\n",
        "        total_restored_files = sum(r['files_moved'] for r in successful_restorations)\n",
        "        print(f\"   üìÑ Files restored: {total_restored_files}\")\n",
        "\n",
        "# Directory locations\n",
        "print(f\"\\nüìÅ DIRECTORIES:\")\n",
        "print(f\"   üíæ Downloads: {DOWNLOAD_DIR}\")\n",
        "print(f\"   üìÇ Restored: {RESTORE_DIR}\")\n",
        "print(f\"   üè† Original: {ARTIFACTS_DIR}\")\n",
        "\n",
        "# Next steps\n",
        "print(f\"\\nüí° NEXT STEPS:\")\n",
        "print(f\"   1. Verify your models are working correctly\")\n",
        "print(f\"   2. Check file integrity using verification reports\")\n",
        "print(f\"   3. Remove download files if no longer needed\")\n",
        "print(f\"   4. Update any paths in your code if necessary\")\n",
        "\n",
        "# Important files to keep\n",
        "print(f\"\\nüìÑ IMPORTANT FILES:\")\n",
        "if 'verification_report_path' in locals():\n",
        "    print(f\"   üîç Verification report: {verification_report_path}\")\n",
        "\n",
        "manifest_files = []\n",
        "if 'download_results' in locals():\n",
        "    manifest_files = [r['local_path'] for r in download_results if r['s3_key'].endswith('.json') and r['success']]\n",
        "\n",
        "if manifest_files:\n",
        "    print(f\"   üìã Manifest files:\")\n",
        "    for manifest_file in manifest_files:\n",
        "        print(f\"      - {manifest_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ All operations completed successfully!\")\n",
        "print(\"üîí Your model artifacts have been restored from S3 backup\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
