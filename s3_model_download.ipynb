{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Download and Restore from S3 Notebook\n",
        "\n",
        "This notebook provides functionality to:\n",
        "1. List and discover model archives in S3 bucket\n",
        "2. Download zip files from AWS S3 bucket\n",
        "3. Restore the original directory structure\n",
        "4. Verify file integrity and completeness\n",
        "5. Provide progress tracking and error handling\n",
        "\n",
        "## Prerequisites\n",
        "- AWS credentials configured (access key, secret key)\n",
        "- boto3 installed (`pip install boto3`)\n",
        "- Sufficient disk space for downloading and extracting files\n",
        "- Original upload manifest files (recommended for verification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration Section\n",
        "\n",
        "**⚠️ Security Note**: Never commit AWS credentials to version control. Use environment variables or AWS credentials file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY_ID = \"your-access-key-here\"  # Replace with your access key\n",
        "AWS_SECRET_ACCESS_KEY = \"your-secret-key-here\"  # Replace with your secret key\n",
        "AWS_REGION = \"us-east-1\"  # Replace with your preferred region\n",
        "S3_BUCKET_NAME = \"your-bucket-name\"  # Replace with your S3 bucket name\n",
        "\n",
        "# Alternatively, use environment variables (recommended for security)\n",
        "# AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "# AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "# AWS_REGION = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
        "# S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\n",
        "\n",
        "# Project Configuration\n",
        "PROJECT_ROOT = \"/Users/shubhangmalviya/Documents/Projects/Walsh College/HistoPathologyResearch\"\n",
        "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, \"artifacts\")\n",
        "DOWNLOAD_DIR = os.path.join(PROJECT_ROOT, \"downloads\")\n",
        "RESTORE_DIR = os.path.join(PROJECT_ROOT, \"restored_artifacts\")\n",
        "\n",
        "# S3 Configuration - you can specify a specific date or leave empty to list all\n",
        "TARGET_DATE = \"\"  # Format: \"2024-01-15\" or leave empty to show all dates\n",
        "S3_KEY_PREFIX = f\"histopathology-research/{TARGET_DATE}\" if TARGET_DATE else \"histopathology-research\"\n",
        "\n",
        "# Download Options\n",
        "OVERWRITE_EXISTING = False  # Set to True to overwrite existing files\n",
        "VERIFY_DOWNLOADS = True     # Set to True to verify file integrity after download\n",
        "\n",
        "print(f\"✅ Configuration loaded\")\n",
        "print(f\"📁 Project root: {PROJECT_ROOT}\")\n",
        "print(f\"💾 Download directory: {DOWNLOAD_DIR}\")\n",
        "print(f\"📂 Restore directory: {RESTORE_DIR}\")\n",
        "print(f\"🪣 S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"🔍 S3 Search prefix: {S3_KEY_PREFIX}\")\n",
        "print(f\"🔄 Overwrite existing: {OVERWRITE_EXISTING}\")\n",
        "print(f\"✅ Verify downloads: {VERIFY_DOWNLOADS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import boto3\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import hashlib\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "import shutil\n",
        "from botocore.exceptions import NoCredentialsError, ClientError\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"📚 All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_bytes(bytes_size: int) -> str:\n",
        "    \"\"\"Convert bytes to human readable format.\"\"\"\n",
        "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if bytes_size < 1024.0:\n",
        "            return f\"{bytes_size:.2f} {unit}\"\n",
        "        bytes_size /= 1024.0\n",
        "    return f\"{bytes_size:.2f} PB\"\n",
        "\n",
        "def calculate_file_hash(filepath: str) -> str:\n",
        "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"Create necessary directories if they don't exist.\"\"\"\n",
        "    for directory in [DOWNLOAD_DIR, RESTORE_DIR]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"📁 Download directory ready: {DOWNLOAD_DIR}\")\n",
        "    print(f\"📁 Restore directory ready: {RESTORE_DIR}\")\n",
        "\n",
        "def parse_s3_key(s3_key: str) -> Dict:\n",
        "    \"\"\"Parse S3 key to extract useful information.\"\"\"\n",
        "    parts = s3_key.split('/')\n",
        "    info = {\n",
        "        'full_key': s3_key,\n",
        "        'filename': parts[-1],\n",
        "        'project': parts[0] if len(parts) > 0 else '',\n",
        "        'date': parts[1] if len(parts) > 1 else '',\n",
        "        'category': parts[2] if len(parts) > 2 else '',\n",
        "        'research_question': parts[3] if len(parts) > 3 else ''\n",
        "    }\n",
        "    return info\n",
        "\n",
        "def format_timestamp(timestamp_str: str) -> str:\n",
        "    \"\"\"Format timestamp string to human readable format.\"\"\"\n",
        "    try:\n",
        "        # Parse timestamp from filename (format: YYYYMMDD_HHMMSS)\n",
        "        if '_' in timestamp_str:\n",
        "            date_part, time_part = timestamp_str.split('_')\n",
        "            formatted_date = f\"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]}\"\n",
        "            formatted_time = f\"{time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}\"\n",
        "            return f\"{formatted_date} {formatted_time}\"\n",
        "        return timestamp_str\n",
        "    except:\n",
        "        return timestamp_str\n",
        "\n",
        "print(\"🔧 Utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. AWS S3 Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_s3_client():\n",
        "    \"\"\"Initialize and test S3 client.\"\"\"\n",
        "    try:\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "            region_name=AWS_REGION\n",
        "        )\n",
        "        \n",
        "        # Test connection by listing buckets\n",
        "        response = s3_client.list_buckets()\n",
        "        \n",
        "        # Check if our target bucket exists\n",
        "        bucket_names = [bucket['Name'] for bucket in response['Buckets']]\n",
        "        \n",
        "        if S3_BUCKET_NAME in bucket_names:\n",
        "            print(f\"✅ S3 client initialized successfully\")\n",
        "            print(f\"🪣 Target bucket '{S3_BUCKET_NAME}' found\")\n",
        "        else:\n",
        "            print(f\"⚠️  Warning: Bucket '{S3_BUCKET_NAME}' not found in your account\")\n",
        "            print(f\"Available buckets: {bucket_names}\")\n",
        "            \n",
        "        return s3_client\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to initialize S3 client: {e}\")\n",
        "        return None\n",
        "\n",
        "def list_s3_objects(s3_client, prefix: str = \"\") -> List[Dict]:\n",
        "    \"\"\"List all objects in S3 bucket with given prefix.\"\"\"\n",
        "    objects = []\n",
        "    \n",
        "    try:\n",
        "        paginator = s3_client.get_paginator('list_objects_v2')\n",
        "        page_iterator = paginator.paginate(\n",
        "            Bucket=S3_BUCKET_NAME,\n",
        "            Prefix=prefix\n",
        "        )\n",
        "        \n",
        "        for page in page_iterator:\n",
        "            if 'Contents' in page:\n",
        "                for obj in page['Contents']:\n",
        "                    obj_info = {\n",
        "                        'key': obj['Key'],\n",
        "                        'size': obj['Size'],\n",
        "                        'last_modified': obj['LastModified'],\n",
        "                        'formatted_size': format_bytes(obj['Size']),\n",
        "                        'parsed': parse_s3_key(obj['Key'])\n",
        "                    }\n",
        "                    objects.append(obj_info)\n",
        "        \n",
        "        return objects\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to list S3 objects: {e}\")\n",
        "        return []\n",
        "\n",
        "def download_file_from_s3(s3_client, s3_key: str, local_path: str) -> Dict:\n",
        "    \"\"\"Download a single file from S3 with progress tracking.\"\"\"\n",
        "    download_info = {\n",
        "        's3_key': s3_key,\n",
        "        'local_path': local_path,\n",
        "        'bucket': S3_BUCKET_NAME,\n",
        "        'success': False,\n",
        "        'download_time': None,\n",
        "        'file_size': 0,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Get object info first\n",
        "        response = s3_client.head_object(Bucket=S3_BUCKET_NAME, Key=s3_key)\n",
        "        file_size = response['ContentLength']\n",
        "        download_info['file_size'] = file_size\n",
        "        \n",
        "        print(f\"⬇️  Downloading: {os.path.basename(s3_key)} ({format_bytes(file_size)})\")\n",
        "        \n",
        "        # Check if file already exists\n",
        "        if os.path.exists(local_path) and not OVERWRITE_EXISTING:\n",
        "            print(f\"⚠️  File already exists, skipping: {local_path}\")\n",
        "            download_info['success'] = True\n",
        "            download_info['download_time'] = 0\n",
        "            return download_info\n",
        "        \n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Download with progress tracking\n",
        "        def progress_callback(bytes_transferred):\n",
        "            percentage = (bytes_transferred / file_size) * 100 if file_size > 0 else 0\n",
        "            print(f\"\\r   Progress: {percentage:.1f}% ({format_bytes(bytes_transferred)}/{format_bytes(file_size)})\", end=\"\")\n",
        "        \n",
        "        s3_client.download_file(\n",
        "            S3_BUCKET_NAME, \n",
        "            s3_key, \n",
        "            local_path,\n",
        "            Callback=progress_callback\n",
        "        )\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        download_info['download_time'] = (end_time - start_time).total_seconds()\n",
        "        download_info['success'] = True\n",
        "        \n",
        "        print(f\"\\n✅ Download completed in {download_info['download_time']:.2f} seconds\")\n",
        "        print(f\"   💾 Local path: {local_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        download_info['error'] = str(e)\n",
        "        print(f\"\\n❌ Download failed: {e}\")\n",
        "    \n",
        "    return download_info\n",
        "\n",
        "print(\"☁️  S3 functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Archive Extraction Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_zip_archive(zip_path: str, extract_to: str, preserve_structure: bool = True) -> Dict:\n",
        "    \"\"\"Extract zip archive while preserving or restoring directory structure.\"\"\"\n",
        "    extract_info = {\n",
        "        'zip_path': zip_path,\n",
        "        'extract_to': extract_to,\n",
        "        'extracted_at': datetime.now().isoformat(),\n",
        "        'files_extracted': 0,\n",
        "        'total_size': 0,\n",
        "        'success': False,\n",
        "        'files_list': [],\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print(f\"📦 Extracting archive: {os.path.basename(zip_path)}\")\n",
        "        print(f\"   📁 Destination: {extract_to}\")\n",
        "        \n",
        "        with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
        "            # Get list of files in archive\n",
        "            file_list = zipf.namelist()\n",
        "            total_files = len(file_list)\n",
        "            \n",
        "            print(f\"   📄 Files in archive: {total_files}\")\n",
        "            \n",
        "            # Create destination directory\n",
        "            os.makedirs(extract_to, exist_ok=True)\n",
        "            \n",
        "            # Extract with progress bar\n",
        "            with tqdm(total=total_files, desc=\"Extracting files\") as pbar:\n",
        "                for file_info in zipf.filelist:\n",
        "                    try:\n",
        "                        # Extract file\n",
        "                        zipf.extract(file_info, extract_to)\n",
        "                        \n",
        "                        # Update statistics\n",
        "                        extract_info['files_extracted'] += 1\n",
        "                        extract_info['total_size'] += file_info.file_size\n",
        "                        extract_info['files_list'].append({\n",
        "                            'filename': file_info.filename,\n",
        "                            'size': file_info.file_size,\n",
        "                            'compressed_size': file_info.compress_size\n",
        "                        })\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️  Warning: Could not extract {file_info.filename}: {e}\")\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "        \n",
        "        extract_info['success'] = True\n",
        "        print(f\"✅ Extraction completed successfully!\")\n",
        "        print(f\"   📄 Files extracted: {extract_info['files_extracted']}\")\n",
        "        print(f\"   📏 Total size: {format_bytes(extract_info['total_size'])}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        extract_info['error'] = str(e)\n",
        "        print(f\"❌ Extraction failed: {e}\")\n",
        "    \n",
        "    return extract_info\n",
        "\n",
        "def restore_to_original_structure(extracted_path: str, target_artifacts_dir: str, research_question: str) -> Dict:\n",
        "    \"\"\"Restore extracted files to original artifacts directory structure.\"\"\"\n",
        "    restore_info = {\n",
        "        'source_path': extracted_path,\n",
        "        'target_path': target_artifacts_dir,\n",
        "        'research_question': research_question,\n",
        "        'files_moved': 0,\n",
        "        'success': False,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print(f\"🔄 Restoring {research_question.upper()} to original structure...\")\n",
        "        \n",
        "        # Create target directory structure\n",
        "        target_rq_dir = os.path.join(target_artifacts_dir, research_question)\n",
        "        os.makedirs(target_rq_dir, exist_ok=True)\n",
        "        \n",
        "        # Move files while preserving structure\n",
        "        if os.path.exists(extracted_path):\n",
        "            for root, dirs, files in os.walk(extracted_path):\n",
        "                for file in files:\n",
        "                    source_file = os.path.join(root, file)\n",
        "                    \n",
        "                    # Calculate relative path\n",
        "                    rel_path = os.path.relpath(source_file, extracted_path)\n",
        "                    target_file = os.path.join(target_rq_dir, rel_path)\n",
        "                    \n",
        "                    # Create target directory if needed\n",
        "                    os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
        "                    \n",
        "                    # Copy or move file\n",
        "                    if OVERWRITE_EXISTING or not os.path.exists(target_file):\n",
        "                        shutil.copy2(source_file, target_file)\n",
        "                        restore_info['files_moved'] += 1\n",
        "                    else:\n",
        "                        print(f\"⚠️  Skipping existing file: {target_file}\")\n",
        "        \n",
        "        restore_info['success'] = True\n",
        "        print(f\"✅ Restoration completed!\")\n",
        "        print(f\"   📄 Files restored: {restore_info['files_moved']}\")\n",
        "        print(f\"   📁 Target directory: {target_rq_dir}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        restore_info['error'] = str(e)\n",
        "        print(f\"❌ Restoration failed: {e}\")\n",
        "    \n",
        "    return restore_info\n",
        "\n",
        "print(\"📦 Archive extraction functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize and Discover S3 Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "create_directories()\n",
        "\n",
        "# Initialize S3 client\n",
        "print(\"\\n🔐 Initializing AWS S3 connection...\")\n",
        "s3_client = initialize_s3_client()\n",
        "\n",
        "if s3_client is None:\n",
        "    print(\"\\n❌ Cannot proceed with S3 operations. Please check your AWS credentials.\")\n",
        "    print(\"\\n🔧 Troubleshooting steps:\")\n",
        "    print(\"   1. Verify AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\")\n",
        "    print(\"   2. Check if the specified region is correct\")\n",
        "    print(\"   3. Ensure your AWS account has S3 permissions\")\n",
        "    print(\"   4. Verify the bucket name exists and you have access\")\n",
        "else:\n",
        "    print(\"\\n🔍 Discovering available files in S3...\")\n",
        "    \n",
        "    # List all objects with the prefix\n",
        "    s3_objects = list_s3_objects(s3_client, S3_KEY_PREFIX)\n",
        "    \n",
        "    if not s3_objects:\n",
        "        print(f\"❌ No objects found with prefix: {S3_KEY_PREFIX}\")\n",
        "        print(\"💡 Try adjusting the TARGET_DATE in configuration or check if files were uploaded\")\n",
        "    else:\n",
        "        print(f\"\\n📊 DISCOVERY RESULTS:\")\n",
        "        print(f\"   📄 Total objects found: {len(s3_objects)}\")\n",
        "        \n",
        "        # Categorize objects\n",
        "        archives = [obj for obj in s3_objects if obj['key'].endswith('.zip') and 'artifacts_' in obj['key']]\n",
        "        manifests = [obj for obj in s3_objects if obj['key'].endswith('.json') and 'manifest' in obj['key']]\n",
        "        \n",
        "        print(f\"   📦 Archive files: {len(archives)}\")\n",
        "        print(f\"   📄 Manifest files: {len(manifests)}\")\n",
        "        \n",
        "        # Display available archives by date and research question\n",
        "        if archives:\n",
        "            print(f\"\\n📦 AVAILABLE ARCHIVES:\")\n",
        "            for archive in archives:\n",
        "                parsed = archive['parsed']\n",
        "                timestamp_part = archive['parsed']['filename'].split('_')[-1].replace('.zip', '')\n",
        "                formatted_time = format_timestamp(timestamp_part)\n",
        "                \n",
        "                print(f\"   🗂️  {parsed['research_question'].upper()}: {archive['formatted_size']} - {formatted_time}\")\n",
        "                print(f\"      📍 S3 Key: {archive['key']}\")\n",
        "                print(f\"      📅 Last Modified: {archive['last_modified'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                print()\n",
        "        \n",
        "        # Display available manifests\n",
        "        if manifests:\n",
        "            print(f\"📄 AVAILABLE MANIFESTS:\")\n",
        "            for manifest in manifests:\n",
        "                parsed = manifest['parsed']\n",
        "                timestamp_part = manifest['parsed']['filename'].split('_')[-1].replace('.json', '')\n",
        "                formatted_time = format_timestamp(timestamp_part)\n",
        "                \n",
        "                print(f\"   📋 {manifest['formatted_size']} - {formatted_time}\")\n",
        "                print(f\"      📍 S3 Key: {manifest['key']}\")\n",
        "                print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Download Archives from S3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if s3_client and 'archives' in locals() and archives:\n",
        "    print(\"⬇️  Starting download process...\")\n",
        "    \n",
        "    download_session_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    download_results = []\n",
        "    \n",
        "    print(f\"📅 Download session ID: {download_session_id}\\n\")\n",
        "    \n",
        "    for archive in archives:\n",
        "        print(f\"\\n📦 Processing {archive['parsed']['research_question'].upper()} archive...\")\n",
        "        \n",
        "        # Create local path maintaining structure\n",
        "        local_filename = os.path.basename(archive['key'])\n",
        "        local_path = os.path.join(DOWNLOAD_DIR, local_filename)\n",
        "        \n",
        "        # Download archive\n",
        "        download_result = download_file_from_s3(s3_client, archive['key'], local_path)\n",
        "        download_results.append(download_result)\n",
        "        \n",
        "        # Verify download if enabled\n",
        "        if VERIFY_DOWNLOADS and download_result['success']:\n",
        "            print(f\"🔍 Verifying download...\")\n",
        "            if os.path.exists(local_path):\n",
        "                local_size = os.path.getsize(local_path)\n",
        "                if local_size == download_result['file_size']:\n",
        "                    print(f\"✅ Verification passed: File size matches ({format_bytes(local_size)})\")\n",
        "                else:\n",
        "                    print(f\"❌ Verification failed: Size mismatch (Expected: {format_bytes(download_result['file_size'])}, Got: {format_bytes(local_size)})\")\n",
        "            else:\n",
        "                print(f\"❌ Verification failed: Downloaded file not found\")\n",
        "    \n",
        "    # Download manifests if available\n",
        "    if 'manifests' in locals() and manifests:\n",
        "        print(f\"\\n📄 Downloading manifest files...\")\n",
        "        for manifest in manifests:\n",
        "            local_filename = os.path.basename(manifest['key'])\n",
        "            local_path = os.path.join(DOWNLOAD_DIR, local_filename)\n",
        "            \n",
        "            manifest_result = download_file_from_s3(s3_client, manifest['key'], local_path)\n",
        "            download_results.append(manifest_result)\n",
        "    \n",
        "    # Create download summary\n",
        "    successful_downloads = [r for r in download_results if r['success']]\n",
        "    failed_downloads = [r for r in download_results if not r['success']]\n",
        "    \n",
        "    print(f\"\\n🎉 DOWNLOAD SUMMARY:\")\n",
        "    print(f\"   ✅ Successful downloads: {len(successful_downloads)}\")\n",
        "    print(f\"   ❌ Failed downloads: {len(failed_downloads)}\")\n",
        "    print(f\"   📏 Total downloaded: {format_bytes(sum(r['file_size'] for r in successful_downloads))}\")\n",
        "    print(f\"   ⏱️  Total time: {sum(r.get('download_time', 0) for r in successful_downloads):.2f} seconds\")\n",
        "    \n",
        "    if failed_downloads:\n",
        "        print(f\"\\n❌ FAILED DOWNLOADS:\")\n",
        "        for failed in failed_downloads:\n",
        "            print(f\"   📄 {os.path.basename(failed['s3_key'])}: {failed.get('error', 'Unknown error')}\")\n",
        "\n",
        "else:\n",
        "    if not s3_client:\n",
        "        print(\"❌ S3 client not initialized. Cannot download.\")\n",
        "    elif 'archives' not in locals() or not archives:\n",
        "        print(\"❌ No archives found to download.\")\n",
        "    else:\n",
        "        print(\"❌ Unknown error occurred.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Extract and Restore Archives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'successful_downloads' in locals() and successful_downloads:\n",
        "    print(\"📦 Starting extraction and restoration process...\")\n",
        "    \n",
        "    extraction_results = []\n",
        "    restoration_results = []\n",
        "    \n",
        "    # Filter for archive files only (not manifests)\n",
        "    archive_downloads = [d for d in successful_downloads if d['s3_key'].endswith('.zip') and 'artifacts_' in d['s3_key']]\n",
        "    \n",
        "    for download in archive_downloads:\n",
        "        zip_path = download['local_path']\n",
        "        \n",
        "        if not os.path.exists(zip_path):\n",
        "            print(f\"⚠️  Skipping missing file: {zip_path}\")\n",
        "            continue\n",
        "        \n",
        "        # Determine research question from filename\n",
        "        filename = os.path.basename(zip_path)\n",
        "        if 'artifacts_rq2_' in filename:\n",
        "            research_question = 'rq2'\n",
        "        elif 'artifacts_rq3_' in filename:\n",
        "            research_question = 'rq3'\n",
        "        else:\n",
        "            print(f\"⚠️  Cannot determine research question from filename: {filename}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n🗂️  Processing {research_question.upper()} archive: {filename}\")\n",
        "        \n",
        "        # Create temporary extraction directory\n",
        "        temp_extract_dir = os.path.join(DOWNLOAD_DIR, f\"temp_extract_{research_question}\")\n",
        "        \n",
        "        # Extract archive\n",
        "        extract_info = extract_zip_archive(zip_path, temp_extract_dir)\n",
        "        extraction_results.append(extract_info)\n",
        "        \n",
        "        if extract_info['success']:\n",
        "            # Restore to original structure\n",
        "            restore_info = restore_to_original_structure(\n",
        "                temp_extract_dir, \n",
        "                RESTORE_DIR, \n",
        "                research_question\n",
        "            )\n",
        "            restoration_results.append(restore_info)\n",
        "            \n",
        "            # Clean up temporary extraction directory\n",
        "            try:\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "                print(f\"🧹 Cleaned up temporary directory: {temp_extract_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Could not clean up temporary directory: {e}\")\n",
        "        else:\n",
        "            print(f\"❌ Skipping restoration due to extraction failure\")\n",
        "    \n",
        "    # Summary of extraction and restoration\n",
        "    successful_extractions = [r for r in extraction_results if r['success']]\n",
        "    successful_restorations = [r for r in restoration_results if r['success']]\n",
        "    \n",
        "    print(f\"\\n🎉 EXTRACTION & RESTORATION SUMMARY:\")\n",
        "    print(f\"   📦 Archives processed: {len(archive_downloads)}\")\n",
        "    print(f\"   ✅ Successful extractions: {len(successful_extractions)}\")\n",
        "    print(f\"   ✅ Successful restorations: {len(successful_restorations)}\")\n",
        "    \n",
        "    if successful_restorations:\n",
        "        total_files_restored = sum(r['files_moved'] for r in successful_restorations)\n",
        "        print(f\"   📄 Total files restored: {total_files_restored}\")\n",
        "        print(f\"   📁 Restoration directory: {RESTORE_DIR}\")\n",
        "        \n",
        "        print(f\"\\n📂 RESTORED STRUCTURE:\")\n",
        "        for restore in successful_restorations:\n",
        "            rq_dir = os.path.join(RESTORE_DIR, restore['research_question'])\n",
        "            if os.path.exists(rq_dir):\n",
        "                subdirs = [d for d in os.listdir(rq_dir) if os.path.isdir(os.path.join(rq_dir, d))]\n",
        "                print(f\"   {restore['research_question'].upper()}: {restore['files_moved']} files\")\n",
        "                if subdirs:\n",
        "                    print(f\"      📂 Subdirectories: {', '.join(subdirs)}\")\n",
        "    \n",
        "    # Check if we should copy to original artifacts directory\n",
        "    copy_to_original = input(f\"\\n❓ Copy restored files to original artifacts directory ({ARTIFACTS_DIR})? [y/N]: \").lower().strip()\n",
        "    \n",
        "    if copy_to_original == 'y':\n",
        "        print(f\"\\n🔄 Copying to original artifacts directory...\")\n",
        "        \n",
        "        for restore in successful_restorations:\n",
        "            source_dir = os.path.join(RESTORE_DIR, restore['research_question'])\n",
        "            target_dir = os.path.join(ARTIFACTS_DIR, restore['research_question'])\n",
        "            \n",
        "            if os.path.exists(source_dir):\n",
        "                try:\n",
        "                    # Create target directory\n",
        "                    os.makedirs(target_dir, exist_ok=True)\n",
        "                    \n",
        "                    # Copy files\n",
        "                    for root, dirs, files in os.walk(source_dir):\n",
        "                        for file in files:\n",
        "                            source_file = os.path.join(root, file)\n",
        "                            rel_path = os.path.relpath(source_file, source_dir)\n",
        "                            target_file = os.path.join(target_dir, rel_path)\n",
        "                            \n",
        "                            # Create target directory if needed\n",
        "                            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
        "                            \n",
        "                            if OVERWRITE_EXISTING or not os.path.exists(target_file):\n",
        "                                shutil.copy2(source_file, target_file)\n",
        "                                print(f\"📄 Copied: {rel_path}\")\n",
        "                            else:\n",
        "                                print(f\"⚠️  Skipped existing: {rel_path}\")\n",
        "                    \n",
        "                    print(f\"✅ {restore['research_question'].upper()} copied to {target_dir}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Failed to copy {restore['research_question'].upper()}: {e}\")\n",
        "        \n",
        "        print(f\"\\n✅ Copy operation completed!\")\n",
        "    else:\n",
        "        print(f\"\\n💡 Files remain in restoration directory: {RESTORE_DIR}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No successful downloads found to extract and restore.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Verification and Integrity Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_restored_structure(restore_dir: str) -> Dict:\n",
        "    \"\"\"Verify the restored directory structure and files.\"\"\"\n",
        "    verification_results = {\n",
        "        'total_files': 0,\n",
        "        'total_size': 0,\n",
        "        'research_questions': {},\n",
        "        'verification_time': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    print(\"🔍 Verifying restored structure...\")\n",
        "    \n",
        "    if not os.path.exists(restore_dir):\n",
        "        print(f\"❌ Restore directory not found: {restore_dir}\")\n",
        "        return verification_results\n",
        "    \n",
        "    # Check each research question directory\n",
        "    for rq in ['rq2', 'rq3']:\n",
        "        rq_path = os.path.join(restore_dir, rq)\n",
        "        \n",
        "        if os.path.exists(rq_path):\n",
        "            rq_info = {\n",
        "                'exists': True,\n",
        "                'file_count': 0,\n",
        "                'total_size': 0,\n",
        "                'subdirectories': [],\n",
        "                'sample_files': []\n",
        "            }\n",
        "            \n",
        "            # Walk through directory\n",
        "            for root, dirs, files in os.walk(rq_path):\n",
        "                rq_info['file_count'] += len(files)\n",
        "                \n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_size = os.path.getsize(file_path)\n",
        "                    rq_info['total_size'] += file_size\n",
        "                    verification_results['total_size'] += file_size\n",
        "                    \n",
        "                    # Keep sample of files for verification\n",
        "                    if len(rq_info['sample_files']) < 5:\n",
        "                        rel_path = os.path.relpath(file_path, rq_path)\n",
        "                        rq_info['sample_files'].append({\n",
        "                            'path': rel_path,\n",
        "                            'size': file_size,\n",
        "                            'hash': calculate_file_hash(file_path) if file_size < 100*1024*1024 else 'skipped_large_file'  # Skip hash for files > 100MB\n",
        "                        })\n",
        "            \n",
        "            # Get subdirectories\n",
        "            if os.path.exists(rq_path):\n",
        "                rq_info['subdirectories'] = [d for d in os.listdir(rq_path) if os.path.isdir(os.path.join(rq_path, d))]\n",
        "            \n",
        "            verification_results['research_questions'][rq] = rq_info\n",
        "            verification_results['total_files'] += rq_info['file_count']\n",
        "            \n",
        "            print(f\"✅ {rq.upper()}: {rq_info['file_count']} files, {format_bytes(rq_info['total_size'])}\")\n",
        "            if rq_info['subdirectories']:\n",
        "                print(f\"   📂 Subdirectories: {', '.join(rq_info['subdirectories'])}\")\n",
        "        else:\n",
        "            verification_results['research_questions'][rq] = {'exists': False}\n",
        "            print(f\"❌ {rq.upper()}: Directory not found\")\n",
        "    \n",
        "    print(f\"\\n📊 VERIFICATION SUMMARY:\")\n",
        "    print(f\"   📄 Total files: {verification_results['total_files']}\")\n",
        "    print(f\"   📏 Total size: {format_bytes(verification_results['total_size'])}\")\n",
        "    \n",
        "    return verification_results\n",
        "\n",
        "# Run verification if we have restored files\n",
        "if 'successful_restorations' in locals() and successful_restorations:\n",
        "    verification = verify_restored_structure(RESTORE_DIR)\n",
        "    \n",
        "    # Save verification report\n",
        "    verification_report_path = os.path.join(DOWNLOAD_DIR, f\"verification_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "    \n",
        "    with open(verification_report_path, 'w') as f:\n",
        "        json.dump(verification, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n📄 Verification report saved: {verification_report_path}\")\n",
        "else:\n",
        "    print(\"⚠️  No restored files to verify.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup Options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional cleanup of downloaded zip files\n",
        "CLEANUP_DOWNLOADS = False  # Set to True to delete downloaded zip files after successful extraction\n",
        "\n",
        "if CLEANUP_DOWNLOADS and 'successful_downloads' in locals():\n",
        "    print(\"🧹 Cleaning up downloaded files...\")\n",
        "    \n",
        "    # Only clean up zip files that were successfully extracted\n",
        "    if 'successful_extractions' in locals():\n",
        "        extracted_files = [r['zip_path'] for r in successful_extractions if r['success']]\n",
        "        \n",
        "        cleaned_count = 0\n",
        "        for zip_path in extracted_files:\n",
        "            if os.path.exists(zip_path):\n",
        "                try:\n",
        "                    os.remove(zip_path)\n",
        "                    print(f\"🗑️  Deleted: {os.path.basename(zip_path)}\")\n",
        "                    cleaned_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️  Could not delete {zip_path}: {e}\")\n",
        "        \n",
        "        print(f\"\\n✅ Cleanup completed. Removed {cleaned_count} zip files.\")\n",
        "        print(f\"💾 Verification reports and manifests retained.\")\n",
        "    else:\n",
        "        print(\"⚠️  No successful extractions found. Keeping all downloaded files.\")\n",
        "else:\n",
        "    if 'successful_downloads' in locals():\n",
        "        print(\"📁 Downloaded files retained in download directory.\")\n",
        "        print(f\"   📂 Location: {DOWNLOAD_DIR}\")\n",
        "        \n",
        "        # List what we're keeping\n",
        "        zip_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.zip')]\n",
        "        json_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.json')]\n",
        "        \n",
        "        if zip_files:\n",
        "            print(f\"   📦 Zip files: {len(zip_files)}\")\n",
        "        if json_files:\n",
        "            print(f\"   📄 JSON files: {len(json_files)}\")\n",
        "    else:\n",
        "        print(\"⚠️  No downloads to clean up.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎉 MODEL DOWNLOAD AND RESTORATION COMPLETED\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "session_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "print(f\"\\n📅 Session: {session_id}\")\n",
        "print(f\"🪣 S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"🔍 Search Prefix: {S3_KEY_PREFIX}\")\n",
        "\n",
        "# Summary of operations\n",
        "if 's3_objects' in locals():\n",
        "    print(f\"\\n🔍 DISCOVERY:\")\n",
        "    print(f\"   📄 Objects found: {len(s3_objects)}\")\n",
        "    if 'archives' in locals():\n",
        "        print(f\"   📦 Archives: {len(archives)}\")\n",
        "    if 'manifests' in locals():\n",
        "        print(f\"   📋 Manifests: {len(manifests)}\")\n",
        "\n",
        "if 'download_results' in locals():\n",
        "    successful_downloads = [r for r in download_results if r['success']]\n",
        "    print(f\"\\n⬇️  DOWNLOADS:\")\n",
        "    print(f\"   ✅ Successful: {len(successful_downloads)}\")\n",
        "    print(f\"   📏 Total size: {format_bytes(sum(r['file_size'] for r in successful_downloads))}\")\n",
        "\n",
        "if 'extraction_results' in locals():\n",
        "    successful_extractions = [r for r in extraction_results if r['success']]\n",
        "    print(f\"\\n📦 EXTRACTIONS:\")\n",
        "    print(f\"   ✅ Successful: {len(successful_extractions)}\")\n",
        "    if successful_extractions:\n",
        "        total_extracted_files = sum(r['files_extracted'] for r in successful_extractions)\n",
        "        print(f\"   📄 Files extracted: {total_extracted_files}\")\n",
        "\n",
        "if 'restoration_results' in locals():\n",
        "    successful_restorations = [r for r in restoration_results if r['success']]\n",
        "    print(f\"\\n🔄 RESTORATIONS:\")\n",
        "    print(f\"   ✅ Successful: {len(successful_restorations)}\")\n",
        "    if successful_restorations:\n",
        "        total_restored_files = sum(r['files_moved'] for r in successful_restorations)\n",
        "        print(f\"   📄 Files restored: {total_restored_files}\")\n",
        "\n",
        "# Directory locations\n",
        "print(f\"\\n📁 DIRECTORIES:\")\n",
        "print(f\"   💾 Downloads: {DOWNLOAD_DIR}\")\n",
        "print(f\"   📂 Restored: {RESTORE_DIR}\")\n",
        "print(f\"   🏠 Original: {ARTIFACTS_DIR}\")\n",
        "\n",
        "# Next steps\n",
        "print(f\"\\n💡 NEXT STEPS:\")\n",
        "print(f\"   1. Verify your models are working correctly\")\n",
        "print(f\"   2. Check file integrity using verification reports\")\n",
        "print(f\"   3. Remove download files if no longer needed\")\n",
        "print(f\"   4. Update any paths in your code if necessary\")\n",
        "\n",
        "# Important files to keep\n",
        "print(f\"\\n📄 IMPORTANT FILES:\")\n",
        "if 'verification_report_path' in locals():\n",
        "    print(f\"   🔍 Verification report: {verification_report_path}\")\n",
        "\n",
        "manifest_files = []\n",
        "if 'download_results' in locals():\n",
        "    manifest_files = [r['local_path'] for r in download_results if r['s3_key'].endswith('.json') and r['success']]\n",
        "\n",
        "if manifest_files:\n",
        "    print(f\"   📋 Manifest files:\")\n",
        "    for manifest_file in manifest_files:\n",
        "        print(f\"      - {manifest_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ All operations completed successfully!\")\n",
        "print(\"🔒 Your model artifacts have been restored from S3 backup\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
